{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 2: SIFT Local Feature Matching\n",
    "(adapted from the work developed by James Hays, Cusuh Ham, John Lambert, Vijay Upadhya, and Samarth Brahmbhatt for GaTech.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "<font size=\"4\">The goal of this assignment is to create a local feature matching algorithm using techniques described in Szeliski chapter 7.1. The pipeline we suggest is a simplified version of the famous SIFT pipeline. The matching pipeline is intended to work for instance-level matching – multiple views of the same physical scene. </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='meta_data/local_matches.png' height='1200'/>\n",
    "<font size=\"4\">In the figure above, the top 100 most confident local feature matches from a baseline implementation are shown. In this case, 89 were correct (lines shown in green), and 11 were incorrect (lines shown in red).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details\n",
    "\n",
    "<font size=\"4\">For this assignment, you need to implement the three major steps of a local feature matching algorithm (detecting interest points, creating local feature descriptors, and matching feature vectors). You will implement two versions of the local feature descriptor, and the code is organized as follows:</font>\n",
    " * <font size=\"4\">Interest point detection (see Szeliski 7.1.1)</font>\n",
    " * <font size=\"4\">Local feature description with a simple normalized patch feature (see Szeliski 7.1.2)</font>\n",
    " * <font size=\"4\">Feature matching (see Szeliski 7.1.3)</font>\n",
    " * <font size=\"4\">Local feature description with the SIFT feature (see Szeliski 7.1.2) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips, tricks, and common problems\n",
    "* <font size='4'>Make sure you’re not swapping x and y coordinates at some point. If your interest points aren’t showing up where you expect, or if you’re getting out of bound errors, you might be swapping x and y coordinates. Remember, images expressed as NumPy arrays are accessed image[y, x].\n",
    "* <font size='4'>Make sure your features aren’t somehow degenerate. you can visualize features with plt.imshow( image1_features), although you may need to normalize them first. If the features are mostly zero or mostly identical, you may have made a mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission format\n",
    "* <font size='4'>`<your_nu_username>.ipynb` (finished this file)\n",
    "* <font size='4'>`<your_nu_username>.pdf` (your project report)\n",
    "    \n",
    "<font size='4'>**Note**: Do not install any additional packages inside the conda environment. The TAs will use the same environment as defined in the config files we provide you, so anything that’s not in there by default will probably cause your code to break during grading. Failure to follow any of these instructions will lead to point deductions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from typing import Any, Callable, List, Tuple\n",
    "\n",
    "from pa2_code.utils import load_image, PIL_resize, rgb2gray, normalize_img, verify, save_image\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# Notre Dame\n",
    "image1 = load_image('../data/1a_notredame.jpg')\n",
    "image2 = load_image('../data/1b_notredame.jpg')\n",
    "eval_file = '../ground_truth/notredame.pkl'\n",
    "\n",
    "# # Mount Rushmore -- this pair is relatively easy (still harder than Notre Dame, though)\n",
    "# image1 = load_image('../data/2a_rushmore.jpg')\n",
    "# image2 = load_image('../data/2b_rushmore.jpg')\n",
    "# eval_file = '../ground_truth/rushmore.pkl'\n",
    "\n",
    "# # Episcopal Gaudi -- This pair is relatively difficult\n",
    "# image1 = load_image('../data/3a_gaudi.jpg')\n",
    "# image2 = load_image('../data/3b_gaudi.jpg')\n",
    "# eval_file = '../ground_truth/gaudi.pkl'\n",
    "\n",
    "scale_factor = 0.5\n",
    "image1 = PIL_resize(image1, (int(image1.shape[1]*scale_factor), int(image1.shape[0]*scale_factor)))\n",
    "image2 = PIL_resize(image2, (int(image2.shape[1]*scale_factor), int(image2.shape[0]*scale_factor)))\n",
    "\n",
    "image1_bw = rgb2gray(image1)\n",
    "image2_bw = rgb2gray(image2)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image1)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(image2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming assignment starts here (100 points in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potentially useful NumPy and OpenCV functions\n",
    "<font size='4'>From Numpy: `np.argsort(), np.arctan2(), np.concatenate(), np.fliplr(), np.flipud(), np.histogram(), np.hypot(), np.linalg.norm(), np.linspace(), np.newaxis, np.reshape(), np.sort()`.</font>\n",
    "\n",
    "<font size='4'>Please use `cv2.filter2D` for image filtering/convolution. Its documentation can be found [here](https://vovkos.github.io/doxyrest-showcase/opencv/sphinx_rtd_theme/group_imgproc_filter.html#doxid-d4-d86-group-imgproc-filter-1ga27c049795ce870216ddfb366086b5a04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forbidden functions\n",
    "<font size='4'>You can use these OpenCV, Sci-kit Image, and SciPy functions for testing, but not in your final code). `cv2. getGaussianKernel(),np.gradient(), cv2.Sobel(), cv2.SIFT(), cv2.SURF(), cv2.BFMatcher(), cv2.BFMatcher ().match(), cv2.BFMatcher().knnMatch(), cv2.FlannBasedMatcher().knnMatch(), cv2.HOGDescriptor(), cv2. cornerHarris(), cv2.FastFeatureDetector(), cv2.ORB(), skimage.feature, skimage.feature.hog(), skimage. feature.daisy, skimage.feature.corner_harris(), skimage.feature.corner_shi_tomasi(), skimage.feature .match_descriptors(), skimage.feature.ORB(), cv.filter2D(), scipy.signal.convolve()`.\n",
    "    \n",
    "<font size='4'>We haven’t enumerated all possible forbidden functions here, but using anyone else’s code that performs interest point detection, feature computation, or feature matching for you is forbidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Interest point detection (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='meta_data/interest_point_detection.png' height='1200'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4'>You do not need to worry about scale invariance or keypoint orientation estimation for your baseline Harris corner detector. The original paper by Chris Harris and Mike Stephens describing their corner detector can be found [here](http://www.bmva.org/bmvc/1988/avc-88-023.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 1.1: Compute image gradients using the Sobel filter (2 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOBEL_X_KERNEL = np.array(\n",
    "    [\n",
    "        [-1, 0, 1],\n",
    "        [-2, 0, 2],\n",
    "        [-1, 0, 1]\n",
    "    ]).astype(np.float32)\n",
    "\n",
    "SOBEL_Y_KERNEL = np.array(\n",
    "    [\n",
    "        [-1, -2, -1],\n",
    "        [0, 0, 0],\n",
    "        [1, 2, 1]\n",
    "    ]).astype(np.float32)\n",
    "\n",
    "\n",
    "def compute_image_gradients(image_bw: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Use convolution with Sobel filters to compute the image gradient at each\n",
    "    pixel.\n",
    "\n",
    "    Args:\n",
    "        image_bw: A numpy array of shape (M,N) containing the grayscale image\n",
    "\n",
    "    Returns:\n",
    "        Ix: Array of shape (M,N) representing partial derivatives of image\n",
    "            w.r.t. x-direction\n",
    "        Iy: Array of shape (M,N) representing partial derivative of image\n",
    "            w.r.t. y-direction\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError('`compute_image_gradients` function needs to be implemented')\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return Ix, Iy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "from pa2_unit_tests.test_part1_harris_corner import test_compute_image_gradients\n",
    "\n",
    "print('compute_image_gradients(): ', verify(test_compute_image_gradients(compute_image_gradients)))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.axis('off')\n",
    "\n",
    "Ix, Iy = compute_image_gradients(image1_bw)\n",
    "gradient_magnitudes = np.sqrt(Ix**2 + Iy**2)\n",
    "gradient_magnitudes = normalize_img(gradient_magnitudes)\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(r'$\\sqrt{I_x^2 + I_y^2}$')\n",
    "plt.imshow( (gradient_magnitudes*255).astype(np.uint8))\n",
    "\n",
    "Ix, Iy = compute_image_gradients(image2_bw)\n",
    "gradient_magnitudes = np.sqrt(Ix**2 + Iy**2)\n",
    "gradient_magnitudes = normalize_img(gradient_magnitudes)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(r'$\\sqrt{I_x^2 + I_y^2}$')\n",
    "plt.imshow( (gradient_magnitudes*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 1.2: Create a 2D Gaussian kernel (2 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaussian_kernel_2D(ksize: int, sigma: float) -> np.ndarray:\n",
    "    \"\"\"Create a numpy matrix representing a 2d Gaussian kernel\n",
    "\n",
    "    Args:\n",
    "        ksize: dimension of square kernel\n",
    "        sigma: standard deviation of Gaussian\n",
    "\n",
    "    Returns:\n",
    "        kernel: Array of shape (ksize,ksize) representing a 2d Gaussian kernel\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "    \n",
    "    raise NotImplementedError('`get_gaussian_kernel_2D` function needs to be implemented')\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "from pa2_unit_tests.test_part1_harris_corner import (\n",
    "    test_get_gaussian_kernel_2D_peak,\n",
    "    test_get_gaussian_kernel_2D_sumsto1,\n",
    "    test_get_gaussian_kernel_2D,\n",
    "    test_second_moments\n",
    ")\n",
    "\n",
    "print(\n",
    "    'get_gaussian_kernel_2D_peak():', \n",
    "    verify(test_get_gaussian_kernel_2D_peak(get_gaussian_kernel_2D))\n",
    ")\n",
    "print(\n",
    "    'get_gaussian_kernel_2D_sumsto1():', \n",
    "    verify(test_get_gaussian_kernel_2D_sumsto1(get_gaussian_kernel_2D))\n",
    ")\n",
    "print(\n",
    "    'get_gaussian_kernel_2D():', \n",
    "    verify(test_get_gaussian_kernel_2D(get_gaussian_kernel_2D))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 1.3: Compute the second moments of the input image. You will need to use your\n",
    "`get_gaussian_kernel_2D()` method (3 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_moments(\n",
    "    image_bw: np.ndarray,\n",
    "    ksize: int = 7,\n",
    "    sigma: float = 10\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\" Compute second moments from image.\n",
    "\n",
    "    Compute image gradients Ix and Iy at each pixel, the mixed derivatives,\n",
    "    then the second moments (sx2, sxsy, sy2) at each pixel, using convolution\n",
    "    with a Gaussian filter.\n",
    "\n",
    "    Args:\n",
    "        image_bw: array of shape (M,N) containing the grayscale image\n",
    "        ksize: size of 2d Gaussian filter\n",
    "        sigma: standard deviation of Gaussian filter\n",
    "\n",
    "    Returns:\n",
    "        sx2: array of shape (M,N) containing the second moment in x direction\n",
    "        sy2: array of shape (M,N) containing the second moment in y direction\n",
    "        sxsy: array of dim (M,N) containing the second moment in the x then the\n",
    "            y direction\n",
    "    \"\"\"\n",
    "\n",
    "    sx2, sy2, sxsy = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR SECOND MOMENTS CODE HERE                                     #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError('`second_moments` function needs to be implemented')\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return sx2, sy2, sxsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "from pa2_code.utils import normalize_img\n",
    "\n",
    "print('second_moments():', verify(test_second_moments(second_moments)))\n",
    "\n",
    "sx2, sy2, sxsy = second_moments(image1_bw, ksize = 7, sigma = 10)\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "Ix, Iy = compute_image_gradients(image1_bw)\n",
    "plt.subplot(2,3,1); plt.title(r'$I_x$')\n",
    "plt.imshow( (normalize_img(np.abs(Ix))*255).astype(np.uint8))\n",
    "plt.subplot(2,3,2); plt.title(r'$I_y$')\n",
    "plt.imshow( (normalize_img(np.abs(Iy))*255).astype(np.uint8))\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.title(r'$s_x^2$')\n",
    "plt.imshow( (normalize_img(np.abs(sx2))*255).astype(np.uint8))\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.title(r'$s_y^2$')\n",
    "plt.imshow( (normalize_img(np.abs(sy2))*255).astype(np.uint8))\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.title(r'$s_xs_y$')\n",
    "plt.imshow( (normalize_img(np.abs(sxsy))*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 1.4: Get the raw corner responses over the entire image (the previously\n",
    "implemented methods may be helpful) (4 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_harris_response_map(\n",
    "    image_bw: np.ndarray,\n",
    "    ksize: int = 7,\n",
    "    sigma: float = 5,\n",
    "    alpha: float = 0.05\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute the Harris cornerness score at each pixel (See Szeliski 7.1.1)\n",
    "\n",
    "    Recall that R = det(M) - alpha * (trace(M))^2\n",
    "    where M = [S_xx S_xy;\n",
    "               S_xy  S_yy],\n",
    "          S_xx = Gk * I_xx\n",
    "          S_yy = Gk * I_yy\n",
    "          S_xy  = Gk * I_xy,\n",
    "    and * is a convolutional operation over a Gaussian kernel of size (k, k).\n",
    "    (You can verify that this is equivalent to taking a (Gaussian) weighted sum\n",
    "    over the window of size (k, k), see how convolutional operation works here:\n",
    "        http://cs231n.github.io/convolutional-networks/)\n",
    "\n",
    "    Ix, Iy are simply image derivatives in x and y directions, respectively.\n",
    "\n",
    "    Args:\n",
    "        image_bw: array of shape (M,N) containing the grayscale image\n",
    "            ksize: size of 2d Gaussian filter\n",
    "        sigma: standard deviation of gaussian filter\n",
    "        alpha: scalar term in Harris response score\n",
    "\n",
    "    Returns:\n",
    "        R: array of shape (M,N), indicating the corner score of each pixel.\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError('`compute_harris_response_map` function needs to be implemented')\n",
    "\n",
    "    ###########################################################################\n",
    "    #                           END OF YOUR CODE                              #\n",
    "    ###########################################################################\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "from pa2_unit_tests.test_part1_harris_corner import test_compute_harris_response_map\n",
    "\n",
    "print(\n",
    "    'compute_harris_response_map(): ', \n",
    "    verify(test_compute_harris_response_map(compute_harris_response_map)))\n",
    "\n",
    "R = compute_harris_response_map(image1_bw)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image1_bw, cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(r'$R$')\n",
    "plt.imshow(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 1.5: Performs the maxpooling operation using just NumPy (5 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_maxpool_numpy(R: np.ndarray, ksize: int, k: int) -> np.ndarray:\n",
    "    \"\"\" Get top k interest points that are local maxima over (ksize,ksize)\n",
    "    neighborhood.\n",
    "\n",
    "    Args:\n",
    "        R: score response map of shape (M,N)\n",
    "        ksize: kernel size of max-pooling operator\n",
    "        k: number of interest points (take top k by confidence)\n",
    "\n",
    "    Returns:\n",
    "        x: array of shape (k,) containing x-coordinates of interest points\n",
    "        y: array of shape (k,) containing y-coordinates of interest points\n",
    "        c: array of shape (k,) containing confidences of interest points\n",
    "        \n",
    "    HINTS:\n",
    "    - We encourage you to try implementing this naively first, just be aware\n",
    "      that it may take an absurdly long time to run. You will need to get a\n",
    "      function that takes a reasonable amount of time to run so that the TAs\n",
    "      can verify your code works.\n",
    "    - If you need to apply padding to the image, only use the zero-padding\n",
    "      method. You need to compute how much padding is required, if any.\n",
    "    - \"Stride\" should be set to 1 in your implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    x, y, confidences = None, None, None\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "    \n",
    "    raise NotImplementedError('`maxpool_numpy` function needs to be implemented')\n",
    "\n",
    "    ###########################################################################\n",
    "    #                           END OF YOUR CODE                              #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return x, y, confidences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "from pa2_unit_tests.test_part1_harris_corner import test_nms_maxpool_numpy\n",
    "from pa2_code.utils import verify\n",
    "\n",
    "print('nms_maxpool_numpy(): ', verify(test_nms_maxpool_numpy(nms_maxpool_numpy)))\n",
    "\n",
    "toy_response_map = np.array(\n",
    "[\n",
    "    [1,2,2,1,2],\n",
    "    [1,6,2,1,1],\n",
    "    [2,2,1,1,1],\n",
    "    [1,1,1,7,1],\n",
    "    [1,1,1,1,1]\n",
    "]).astype(np.float32)\n",
    "plt.figure(figsize=(6,3))\n",
    "# plt.subplot(1,2,1)\n",
    "plt.imshow(toy_response_map.astype(np.uint8))\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# maxpooled_image = nms_maxpool_numpy(toy_response_map, ksize=3)\n",
    "# plt.imshow(maxpooled_image.astype(np.uint8))\n",
    "\n",
    "x_coords, y_coords, confidences = nms_maxpool_numpy(toy_response_map, k=2, ksize=3)\n",
    "print('Coordinates of local maxima:')\n",
    "for x, y, c in zip(x_coords, y_coords, confidences):\n",
    "    print(f'\\tAt {x},{y}, local maximum w/ confidence={c:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 1.6: Remove values close to the border that we can’t create a useful SIFT window around (2 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_border_vals(\n",
    "    img: np.ndarray,\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    c: np.ndarray\n",
    ") -> Tuple[np.ndarray,np.ndarray,np.ndarray]:\n",
    "    \"\"\"\n",
    "    Remove interest points that are too close to a border to allow SIFT feature\n",
    "    extraction. Make sure you remove all points where a 16x16 window around\n",
    "    that point cannot be formed.\n",
    "\n",
    "    Args:\n",
    "        img: array of shape (M,N) containing the grayscale image\n",
    "        x: array of shape (k,) representing x coord of interest points\n",
    "        y: array of shape (k,) representing y coord of interest points\n",
    "        c: array of shape (k,) representing confidences of interest points\n",
    "\n",
    "    Returns:\n",
    "        x: array of shape (p,), where p <= k (less than or equal after pruning)\n",
    "        y: array of shape (p,)\n",
    "        c: array of shape (p,)\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError('`remove_border_vals` function needs to be implemented')\n",
    "\n",
    "    ###########################################################################\n",
    "    #                           END OF YOUR CODE                              #\n",
    "    ###########################################################################\n",
    "\n",
    "    return x, y, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "from pa2_unit_tests.test_part1_harris_corner import test_get_harris_interest_points, test_remove_border_vals\n",
    "\n",
    "print(\n",
    "    'test_remove_border_vals(): ', \n",
    "    verify(test_remove_border_vals(remove_border_vals))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 1.7: Get interests points from the entire image (the previously imple- mented methods may be helpful) (7 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harris_interest_points(\n",
    "    image_bw: np.ndarray,\n",
    "    k: int = 2500\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Implement the Harris Corner detector. You will find\n",
    "    compute_harris_response_map(), nms_maxpool_numpy(), and\n",
    "    remove_border_vals() useful. Make sure to sort the interest points in\n",
    "    order of confidence!\n",
    "\n",
    "    Args:\n",
    "        image_bw: array of shape (M,N) containing the grayscale image\n",
    "        k: maximum number of interest points to retrieve\n",
    "\n",
    "    Returns:\n",
    "        x: array of shape (p,) containing x-coordinates of interest points\n",
    "        y: array of shape (p,) containing y-coordinates of interest points\n",
    "        c: array of dim (p,) containing the strength(confidence) of each\n",
    "            interest point where p <= k.\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError('`get_harris_interest_points` function needs to be implemented')\n",
    "\n",
    "    ###########################################################################\n",
    "    #                           END OF YOUR CODE                              #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return x, y, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "print(\n",
    "    'get_harris_interest_points()', \n",
    "    verify(test_get_harris_interest_points(get_harris_interest_points))\n",
    ")\n",
    "\n",
    "import copy\n",
    "from pa2_code.utils import show_interest_points\n",
    "\n",
    "num_interest_points = 2500\n",
    "X1, Y1, _ = get_harris_interest_points( copy.deepcopy(image1_bw), num_interest_points)\n",
    "X2, Y2, _ = get_harris_interest_points( copy.deepcopy(image2_bw), num_interest_points)\n",
    "\n",
    "num_pts_to_visualize = 300\n",
    "# Visualize the interest points\n",
    "rendered_img1 = show_interest_points(image1, X1[:num_pts_to_visualize], Y1[:num_pts_to_visualize])\n",
    "rendered_img2 = show_interest_points(image2, X2[:num_pts_to_visualize], Y2[:num_pts_to_visualize])\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1); plt.imshow(rendered_img1)\n",
    "plt.subplot(1,2,2); plt.imshow(rendered_img2)\n",
    "print(f'{len(X1)} corners in image 1, {len(X2)} corners in image 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Par 2: Local feature descriptors (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4'>To get your matching pipeline working quickly, you will implement a bare-bones feature descriptor using normalized, grayscale image intensity patches as your local feature. See Szeliski 7.1.2 for more details.\n",
    "    \n",
    "<font size='4'>Choose the top-left option of the 4 possible choices for center of a square window, as shown in the Figure below.\n",
    "\n",
    "<img src='meta_data/window.png' height='1200'/>\n",
    "<font size='4'>For this example of a 6 × 6 window, the yellow cells could all be considered the center. Please choose the top left (marked “C”) as the center throughout this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 2: Implement a bare-bones feature descriptor using normalized, grayscale image intensity patches as your local feature (10 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalized_patch_descriptors(\n",
    "    image_bw: np.ndarray, X: float, Y: float, feature_width: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Create local features using normalized patches.\n",
    "\n",
    "    Normalize image intensities in a local window centered at keypoint to a\n",
    "    feature vector with unit norm. This local feature is simple to code and\n",
    "    works OK.\n",
    "\n",
    "    Choose the top-left option of the 4 possible choices for center of a square\n",
    "    window.\n",
    "\n",
    "    Args:\n",
    "        image_bw: array of shape (M,N) representing grayscale image\n",
    "        X: array of shape (K,) representing x-coordinate of keypoints\n",
    "        Y: array of shape (K,) representing y-coordinate of keypoints\n",
    "        feature_width: size of the square window\n",
    "\n",
    "    Returns:\n",
    "        fvs: array of shape (K,D) representing feature descriptors\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError('`compute_normalized_patch_descriptors` needs to be implemented')\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return fvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "from pa2_unit_tests.test_part2_patch_descriptor import test_compute_normalized_patch_descriptors\n",
    "\n",
    "print(\n",
    "    'compute_normalized_patch_descriptors:', \n",
    "    verify(test_compute_normalized_patch_descriptors(compute_normalized_patch_descriptors))\n",
    ")\n",
    "\n",
    "image1_features = compute_normalized_patch_descriptors(image1_bw, X1, Y1, feature_width=16)\n",
    "image2_features = compute_normalized_patch_descriptors(image2_bw, X2, Y2, feature_width=16)\n",
    "\n",
    "# Visualize what the first 300 feature vectors for image 1 look like (they should not be identical or all black)\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1); plt.imshow(image1_features[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Feature matching (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4'>You will implement the “ratio test” (also known as the “nearest neighbor distance ratio test”) method of matching local features as described in the lecture materials and Szeliski 7.1.3 (page 421). See equation 7.18 in particular. The potential matches that pass the ratio test the easiest should have a greater tendency to be correct matches – think about why this is. \n",
    "    \n",
    "In this part, you will have to code `compute_feature_distances()` to get pairwise feature distances, and `match_features_ratio_test()` to perform the ratio test to get matches from a pair of feature lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 3.1: Get pairwise feature distances (5 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_distances(\n",
    "    features1: np.ndarray,\n",
    "    features2: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function computes a list of distances from every feature in one array\n",
    "    to every feature in another.\n",
    "\n",
    "    Using Numpy broadcasting is required to keep memory requirements low.\n",
    "\n",
    "    Note: Using a double for-loop is going to be too slow. One for-loop is the\n",
    "    maximum possible. Vectorization is needed.\n",
    "    See numpy broadcasting details here:\n",
    "        https://cs231n.github.io/python-numpy-tutorial/#broadcasting\n",
    "\n",
    "    Args:\n",
    "        features1: A numpy array of shape (n1,feat_dim) representing one set of\n",
    "            features, where feat_dim denotes the feature dimensionality\n",
    "        features2: A numpy array of shape (n2,feat_dim) representing a second\n",
    "            set of features (n1 not necessarily equal to n2)\n",
    "\n",
    "    Returns:\n",
    "        dists: A numpy array of shape (n1,n2) which holds the distances (in\n",
    "            feature space) from each feature in features1 to each feature in\n",
    "            features2\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError('`compute_feature_distances` function needs to be implemented')\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "from pa2_unit_tests.test_part3_feature_matching import (\n",
    "    test_match_features_ratio_test,\n",
    "    test_compute_feature_distances_2d,\n",
    "    test_compute_feature_distances_10d\n",
    ")\n",
    "print(\n",
    "    'compute_feature_distances (2d):', \n",
    "    verify(test_compute_feature_distances_2d(compute_feature_distances))\n",
    ")\n",
    "\n",
    "print(\n",
    "    'compute_feature_distances (10d):', \n",
    "    verify(test_compute_feature_distances_10d(compute_feature_distances))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 3.2: Perform the ratio test to get matches from a pair of feature lists (5 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features_ratio_test(\n",
    "    features1: np.ndarray,\n",
    "    features2: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\" Nearest-neighbor distance ratio feature matching.\n",
    "\n",
    "    This function does not need to be symmetric (e.g. it can produce different\n",
    "    numbers of matches depending on the order of the arguments).\n",
    "\n",
    "    To start with, simply implement the \"ratio test\", equation 7.18 in section\n",
    "    7.1.3 of Szeliski. There are a lot of repetitive features in these images,\n",
    "    and all of their descriptors will look similar. The ratio test helps us\n",
    "    resolve this issue (also see Figure 11 of David Lowe's IJCV paper).\n",
    "\n",
    "    You should call `compute_feature_distances()` in this function, and then\n",
    "    process the output.\n",
    "\n",
    "    Args:\n",
    "        features1: A numpy array of shape (n1,feat_dim) representing one set of\n",
    "            features, where feat_dim denotes the feature dimensionality\n",
    "        features2: A numpy array of shape (n2,feat_dim) representing a second\n",
    "            set of features (n1 not necessarily equal to n2)\n",
    "\n",
    "    Returns:\n",
    "        matches: A numpy array of shape (k,2), where k is the number of matches.\n",
    "            The first column is an index in features1, and the second column is\n",
    "            an index in features2\n",
    "        confidences: A numpy array of shape (k,) with the real valued confidence\n",
    "            for every match\n",
    "\n",
    "    'matches' and 'confidences' can be empty, e.g., (0x2) and (0x1)\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError('`match_features_ratio_test` function needs to be implemented')\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return matches, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "print(\n",
    "    'match_features_ratio_test:', \n",
    "    verify(test_match_features_ratio_test(match_features_ratio_test))\n",
    ")\n",
    "\n",
    "matches, confidences = match_features_ratio_test(image1_features, image2_features)\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(X1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: SIFT Descriptor (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4'>You will implement a SIFT-like local feature as described in the lecture materials and Szeliski 7.1.2. We’ll use a simple one-line modification (“Square-Root SIFT”) from a 2012 [CVPR paper](https://www.robots.ox.ac.uk/~vgg/publications/2012/Arandjelovic12/arandjelovic12.pdf) to get a free boost in performance. \n",
    "    \n",
    "<font size='4'>Regarding Histograms SIFT relies upon histograms. An unweighted 1D histogram with 3 bins could have bin edges of $[0,2,4,6]$. If $x = [0.0,0.1,2.5,5.8,5.9]$, and the bins are defined over half-open intervals $[e_{left},e_{right})$ with edges $e$, then the histogram $h = [2,1,2]$.\n",
    "    \n",
    "<font size='4'>A weighted 1D histogram with the same 3 bins and bin edges has each item weighted by some value. For example, for an array $x = [0.0, 0.1, 2.5, 5.8, 5.9]$, with weights $w = [2, 3, 1, 0, 0]$, and the same bin edges ($[0, 2, 4, 6]), h_w = [5, 1, 0]$. In SIFT, the histogram weight at a pixel is the magnitude of the image gradient at that pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 4.1: Retrieve gradient magnitudes and orientations of the image (3 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_magnitudes_and_orientations(\n",
    "    Ix: np.ndarray,\n",
    "    Iy: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    This function will return the magnitudes and orientations of the\n",
    "    gradients at each pixel location.\n",
    "\n",
    "    Args:\n",
    "        Ix: array of shape (m,n), representing x gradients in the image\n",
    "        Iy: array of shape (m,n), representing y gradients in the image\n",
    "    Returns:\n",
    "        magnitudes: A numpy array of shape (m,n), representing magnitudes of\n",
    "            the gradients at each pixel location\n",
    "        orientations: A numpy array of shape (m,n), representing angles of\n",
    "            the gradients at each pixel location. angles should range from\n",
    "            -PI to PI.\n",
    "    \"\"\"\n",
    "    magnitudes = []  # placeholder\n",
    "    orientations = []  # placeholder\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError('`get_magnitudes_and_orientations()` function needs to be implemented')\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return magnitudes, orientations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "from pa2_unit_tests.test_part4_sift_descriptor import (\n",
    "    test_get_magnitudes_and_orientations,\n",
    "    test_get_gradient_histogram_vec_from_patch\n",
    ")\n",
    "print(\n",
    "    'get_magnitudes_and_orientations:', \n",
    "    verify(test_get_magnitudes_and_orientations(get_magnitudes_and_orientations))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 4.2: Retrieve a feature consisting of concatenated histograms (5 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_histogram_vec_from_patch(\n",
    "    window_magnitudes: np.ndarray,\n",
    "    window_orientations: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\" Given 16x16 patch, form a 128-d vector of gradient histograms.\n",
    "\n",
    "    Key properties to implement:\n",
    "    (1) a 4x4 grid of cells, each feature_width/4. It is simply the terminology\n",
    "        used in the feature literature to describe the spatial bins where\n",
    "        gradient distributions will be described. The grid will extend\n",
    "        feature_width/2 to the left of the \"center\", and feature_width/2 - 1 to\n",
    "        the right\n",
    "    (2) each cell should have a histogram of the local distribution of\n",
    "        gradients in 8 orientations. Appending these histograms together will\n",
    "        give you 4x4 x 8 = 128 dimensions. The bin centers for the histogram\n",
    "        should be at -7pi/8,-5pi/8,...5pi/8,7pi/8. The histograms should be\n",
    "        added to the feature vector left to right then row by row (reading\n",
    "        order).\n",
    "\n",
    "    Do not normalize the histogram here to unit norm -- preserve the histogram\n",
    "    values. A useful function to look at would be np.histogram.\n",
    "\n",
    "    Args:\n",
    "        window_magnitudes: (16,16) array representing gradient magnitudes of the\n",
    "            patch\n",
    "        window_orientations: (16,16) array representing gradient orientations of\n",
    "            the patch\n",
    "\n",
    "    Returns:\n",
    "        wgh: (128,1) representing weighted gradient histograms for all 16\n",
    "            neighborhoods of size 4x4 px\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError('`get_gradient_histogram_vec_from_patch`' \n",
    "                              + 'function  needs to be implemented')\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return wgh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "print(\n",
    "    'get_gradient_histogram_vec_from_patch():', \n",
    "    verify(test_get_gradient_histogram_vec_from_patch(get_gradient_histogram_vec_from_patch))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 4.3: Get the adjusted feature from a single point (5 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feat_vec(\n",
    "    x: float,\n",
    "    y: float,\n",
    "    magnitudes,\n",
    "    orientations,\n",
    "    feature_width: int = 16\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function returns the feature vector for a specific interest point. To\n",
    "    start with, you might want to simply use normalized patches as your local\n",
    "    feature. This is very simple to code and works OK. However, to get full\n",
    "    credit you will need to implement the more effective SIFT descriptor (see\n",
    "    Szeliski 7.1.2 or the original publications at\n",
    "    http://www.cs.ubc.ca/~lowe/keypoints/). Your implementation does not need\n",
    "    to exactly match the SIFT reference.\n",
    "\n",
    "\n",
    "    Your (baseline) descriptor should have:\n",
    "    (1) Each feature should be normalized to unit length.\n",
    "    (2) Each feature should be raised to the 1/2 power, i.e., square-root SIFT\n",
    "        (read https://www.robots.ox.ac.uk/~vgg/publications/2012/Arandjelovic12/arandjelovic12.pdf)\n",
    "\n",
    "    For our tests, you do not need to perform the interpolation in which each\n",
    "    gradient measurement contributes to multiple orientation bins in multiple\n",
    "    cells. As described in Szeliski, a single gradient measurement creates a\n",
    "    weighted contribution to the 4 nearest cells and the 2 nearest orientation\n",
    "    bins within each cell, for 8 total contributions. The autograder will only\n",
    "    check for each gradient contributing to a single bin.\n",
    "\n",
    "    Args:\n",
    "        x: a float, the x-coordinate of the interest point\n",
    "        y: A float, the y-coordinate of the interest point\n",
    "        magnitudes: A numpy array of shape (m,n), representing image gradients\n",
    "            at each pixel location\n",
    "        orientations: A numpy array of shape (m,n), representing gradient\n",
    "            orientations at each pixel location\n",
    "        feature_width: integer representing the local feature width in pixels.\n",
    "            You can assume that feature_width will be a multiple of 4 (i.e.,\n",
    "            every cell of your local SIFT-like feature will have an integer\n",
    "            width and height). This is the initial window size we examine\n",
    "            around each keypoint.\n",
    "    Returns:\n",
    "        fv: A numpy array of shape (feat_dim,1) representing a feature vector.\n",
    "            \"feat_dim\" is the feature_dimensionality (e.g., 128 for standard\n",
    "            SIFT). These are the computed features.\n",
    "    \"\"\"\n",
    "\n",
    "    fv = []  # placeholder\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError('`get_feat_vec` function in needs to be implemented')\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "from pa2_unit_tests.test_part4_sift_descriptor import test_get_feat_vec, test_get_SIFT_descriptors\n",
    "print(verify(test_get_feat_vec(get_feat_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 4.4: Get all feature vectors corresponding to our interest points from an image (5 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SIFT_descriptors(\n",
    "    image_bw: np.ndarray,\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray,\n",
    "    feature_width: int = 16\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function returns the 128-d SIFT features computed at each of the input\n",
    "    points. Implement the more effective SIFT descriptor (see Szeliski 4.1.2 or\n",
    "    the original publications at http://www.cs.ubc.ca/~lowe/keypoints/)\n",
    "\n",
    "    Args:\n",
    "        image: A numpy array of shape (m,n), the image\n",
    "        X: A numpy array of shape (k,), the x-coordinates of interest points\n",
    "        Y: A numpy array of shape (k,), the y-coordinates of interest points\n",
    "        feature_width: integer representing the local feature width in pixels.\n",
    "            You can assume that feature_width will be a multiple of 4 (i.e.,\n",
    "            every cell of your local SIFT-like feature will have an integer\n",
    "            width and height). This is the initial window size we examine\n",
    "            around each keypoint.\n",
    "    Returns:\n",
    "        fvs: A numpy array of shape (k, feat_dim) representing all feature\n",
    "            vectors. \"feat_dim\" is the feature_dimensionality (e.g., 128 for\n",
    "            standard SIFT). These are the computed features.\n",
    "    \"\"\"\n",
    "    assert image_bw.ndim == 2, 'Image must be grayscale'\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError('`get_SIFT_descriptors` function needs to be implemented')\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return fvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "print(verify(test_get_SIFT_descriptors(get_SIFT_descriptors)))\n",
    "\n",
    "from pa2_code.utils import cheat_interest_points\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "image1_features = get_SIFT_descriptors(image1_bw, X1, Y1)\n",
    "image2_features = get_SIFT_descriptors(image2_bw, X2, Y2)\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print(f'SIFT took {duration} sec.')\n",
    "\n",
    "# visualize what the values of the first 200 SIFT feature vectors look like (should not be identical or all black)\n",
    "plt.figure(); plt.subplot(1,2,1); plt.imshow(image1_features[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the correspondences\n",
    "from pa2_code.utils import show_correspondence_circles, show_correspondence_lines\n",
    "\n",
    "matches, confidences = match_features_ratio_test(image1_features, image2_features)\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(X1)))\n",
    "\n",
    "# num_pts_to_visualize = len(matches)\n",
    "num_pts_to_visualize = 200\n",
    "c1 = show_correspondence_circles(\n",
    "    image1,\n",
    "    image2,\n",
    "    X1[matches[:num_pts_to_visualize, 0]],\n",
    "    Y1[matches[:num_pts_to_visualize, 0]],\n",
    "    X2[matches[:num_pts_to_visualize, 1]],\n",
    "    Y2[matches[:num_pts_to_visualize, 1]]\n",
    ")\n",
    "plt.figure(figsize=(10,5)); plt.imshow(c1)\n",
    "save_image('../results/vis_circles.jpg', c1)\n",
    "c2 = show_correspondence_lines(\n",
    "    image1,\n",
    "    image2,\n",
    "    X1[matches[:num_pts_to_visualize, 0]],\n",
    "    Y1[matches[:num_pts_to_visualize, 0]],\n",
    "    X2[matches[:num_pts_to_visualize, 1]],\n",
    "    Y2[matches[:num_pts_to_visualize, 1]]\n",
    ")\n",
    "plt.figure(figsize=(10,5)); plt.imshow(c2)\n",
    "save_image('../results/vis_lines.jpg', c2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pa2_code.utils import evaluate_correspondence\n",
    "num_pts_to_evaluate = len(matches)\n",
    "_, c = evaluate_correspondence(\n",
    "    image1,\n",
    "    image2,\n",
    "    eval_file,\n",
    "    scale_factor,\n",
    "    X1[matches[:num_pts_to_evaluate, 0]],\n",
    "    Y1[matches[:num_pts_to_evaluate, 0]],\n",
    "    X2[matches[:num_pts_to_evaluate, 1]],\n",
    "    Y2[matches[:num_pts_to_evaluate, 1]]\n",
    ")\n",
    "plt.figure(figsize=(8,4)); plt.imshow(c)\n",
    "save_image('../results/eval.jpg', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code runs in under 90 sec and achieves >80% acc on the Notre Dame pair\n",
    "from pa2_unit_tests.test_part4_sift_descriptor import (\n",
    "    test_feature_matching_speed,\n",
    "    test_feature_matching_accuracy\n",
    ")\n",
    "print(\n",
    "    'SIFT pipeline speed test:', \n",
    "    verify(test_feature_matching_speed(get_harris_interest_points, match_features_ratio_test, get_SIFT_descriptors))\n",
    ")\n",
    "\n",
    "print(\n",
    "    'SIFT pipeline accuracy test:', \n",
    "    verify(test_feature_matching_accuracy(get_harris_interest_points, match_features_ratio_test, get_SIFT_descriptors))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: write up\n",
    "<font size='4' color='red'>Finish it outside of this Jupyter notebook, (20 points)\n",
    "    \n",
    "<font size='4'>For this assignment, you must do a project report using the template slides provided to you. Do not change the order of the slides or remove any slides, as this will affect the grading process and you will be deducted points. In the report you will describe your algorithm and any decisions you made to write your algorithm a particular way. Then you will show and discuss the results of your algorithm. The template slides provide guidance for what you should include in your report. A good writeup doesn’t just show results – it tries to draw some conclusions from the experiments. You must convert the slide deck into a PDF for your submission.\n",
    "    \n",
    "<font size='4'>If you choose to do anything extra, add slides after the slides given in the template deck to describe your implementation, results, and analysis. Adding slides in between the report template will cause issues with the grading process, and you will be deducted points. You will not receive full credit for your extra credit implementations if they are not described adequately in your writeup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra credit (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**Implement a vectorized version of SIFT that runs in under 5 seconds, with at least 80% accuracy on the Notre Dame image pair (10 points).**</font><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sift_features_vectorized(\n",
    "    image_bw: np.ndarray,\n",
    "    X: np.ndarray,\n",
    "    Y: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function is a vectorized version of `get_SIFT_descriptors`.\n",
    "\n",
    "    As before, start by computing the image gradients, as done before. Then\n",
    "    using convolution with the appropriate weights, create an output\n",
    "    with 10 channels, where the first 8 represent cosine values of angles\n",
    "    between unit circle basis vectors and image gradient vectors at every\n",
    "    pixel. The last two channels will represent the (dx, dy) coordinates of the\n",
    "    image gradient at this pixel. The gradient at each pixel can be projected\n",
    "    onto 8 basis vectors around the unit circle\n",
    "\n",
    "    Next, the weighted histogram can be created by element-wise multiplication\n",
    "    of a 4d gradient magnitude tensor, and a 4d gradient binary occupancy\n",
    "    tensor, where a tensor cell is activated if its value represents the\n",
    "    maximum channel value within a \"fibre\" (see\n",
    "    http://cs231n.github.io/convolutional-networks/ for an explanation of a\n",
    "    \"fibre\"). There will be a fibre (consisting of all channels) at each of the\n",
    "    (M,N) pixels of the \"feature map\".\n",
    "\n",
    "    The four dimensions represent (N,C,H,W) for batch dim, channel dim, height\n",
    "    dim, and weight dim, respectively. Our batch size will be 1.\n",
    "\n",
    "    In order to create the 4d binary occupancy tensor, you may wish to index in\n",
    "    at many values simultaneously in the 4d tensor, and read or write to each\n",
    "    of them simultaneously. This can be done by passing a 1D Tensor for\n",
    "    every dimension, e.g., by following the syntax:\n",
    "        My4dTensor[dim0_idxs, dim1_idxs, dim2_idxs, dim3_idxs] = 1d_tensor.\n",
    "\n",
    "    Finally, given 8d feature vectors at each pixel, the features should be\n",
    "    accumulated over 4x4 subgrids using convolution.\n",
    "\n",
    "    You may find np.argmax(), np.zeros_like(), np.meshgrid(),\n",
    "    flatten(), np.arange(), np.expand_dims(), np.multiply(), and\n",
    "    np.linalg.norm helpful.\n",
    "\n",
    "    Returns:\n",
    "        fvs\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: YOUR CODE HERE                                                    #\n",
    "    ###########################################################################\n",
    "\n",
    "    raise NotImplementedError('`get_SIFT_features_vectorized` function needs to be implemented')\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return fvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check your implementation\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "image1_features = get_sift_features_vectorized(image1_bw, X1, Y1)\n",
    "image2_features = get_sift_features_vectorized(image2_bw, X2, Y2)\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print(f'SIFT took {duration} sec.')\n",
    "\n",
    "matches, confidences = match_features_ratio_test(image1_features, image2_features)\n",
    "\n",
    "num_pts_to_evaluate = len(matches) - 1\n",
    "_, c = evaluate_correspondence(\n",
    "    image1,\n",
    "    image2,\n",
    "    eval_file,\n",
    "    scale_factor,\n",
    "    X1[matches[:, 0]],\n",
    "    Y1[matches[:, 0]],\n",
    "    X2[matches[:, 1]],\n",
    "    Y2[matches[:, 1]]\n",
    ")\n",
    "plt.figure(figsize=(8,4)); plt.imshow(c)\n",
    "save_image('../results/eval.jpg', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pa2_unit_tests.test_part4_sift_descriptor import test_extra_credit_vectorized_sift\n",
    "\n",
    "print(verify(test_extra_credit_vectorized_sift(get_harris_interest_points, match_features_ratio_test, get_sift_features_vectorized)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
