{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming assignment 5, module 1 (70 points)\n",
    "(adapted from the work done by Erik Learned-Miller, which was originally developed by Fei-Fei Li, Andrej Karpathy, and Justin Johnson)\n",
    "\n",
    "## Overview\n",
    "<font size='4'> In this assignment you will practice putting together a Convolution Neural Network (CNN) classification pipeline. So far we have worked with deep fully-connected networks, using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, but in practice all state-of-the-art results use convolutional networks instead.\n",
    "\n",
    "<font size='4'>In this module, you will implement several layer types that are used in convolutional networks. You will then use these layers to train a convolutional network on the CIFAR-10 dataset.\n",
    "\n",
    "## Submission format\n",
    "* <font size='4'>`<your_nu_username>_cnn.ipynb`\n",
    "    \n",
    "## Note: \n",
    "* <font size='4'>Do not install any additional packages inside the conda environment. The TAs will use the same environment as defined in the config files we provide you, so anything thatâ€™s not in there by default will probably cause your code to break during grading. Failure to follow any of these instructions will lead to point deductions. \n",
    "* <font size='4'>We have some inline questions embedded in the Jupyter notebook files. Do not miss them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual, a bit of setup\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from utils.classifiers.fc_net import *\n",
    "from utils.data_utils import get_CIFAR10_data\n",
    "from utils.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from utils.solver import Solver\n",
    "from utils.layers import affine_forward, affine_backward, relu_forward, relu_backward, softmax_loss\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's download the data\n",
    "# %cd ../datasets\n",
    "\n",
    "# # 1 -- Linux \n",
    "# # 2 -- MacOS\n",
    "# # 3 -- Command Prompt on Windows\n",
    "# # 4 -- manually downloading the data\n",
    "# choice = 3\n",
    "\n",
    "\n",
    "# if choice == 1:\n",
    "#     # should work well on Linux and in Powershell on Windows\n",
    "#     !wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "# elif choice == 2 or choice ==3:\n",
    "#     # if wget is not available for you, try curl\n",
    "#     # should work well on MacOS\n",
    "#     !curl http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz --output cifar-10-python.tar.gz\n",
    "# else:\n",
    "#     print('Please manually download the data from http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and put it under the datasets folder.')\n",
    "# !tar -xzvf cifar-10-python.tar.gz\n",
    "\n",
    "# if choice==3:\n",
    "#     !del cifar-10-python.tar.gz\n",
    "# else:\n",
    "#     !rm cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train: ', (49000, 3, 32, 32))\n",
      "('y_train: ', (49000,))\n",
      "('X_val: ', (1000, 3, 32, 32))\n",
      "('y_val: ', (1000,))\n",
      "('X_test: ', (1000, 3, 32, 32))\n",
      "('y_test: ', (1000,))\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "cifar10_dir = '../datasets/cifar-10-batches-py'\n",
    "\n",
    "data = get_CIFAR10_data(cifar10_dir)\n",
    "for k, v in list(data.items()):\n",
    "    print(('%s: ' % k, v.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Convolution layers (14 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 1.1: forward pass of a convolution layer (7 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward_naive(x, w, b, conv_param):\n",
    "    \"\"\"\n",
    "    A naive implementation of the forward pass for a convolutional layer.\n",
    "\n",
    "    The input consists of N data points, each with C channels, height H and\n",
    "    width W. We convolve each input with F different filters, where each filter\n",
    "    spans all C channels and has height HH and width WW.\n",
    "\n",
    "    Input:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - w: Filter weights of shape (F, C, HH, WW)\n",
    "    - b: Biases, of shape (F,)\n",
    "    - conv_param: A dictionary with the following keys:\n",
    "      - 'stride': The number of pixels between adjacent receptive fields in the\n",
    "        horizontal and vertical directions.\n",
    "      - 'pad': The number of pixels that will be used to zero-pad the input.\n",
    "\n",
    "\n",
    "    During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides)\n",
    "    along the height and width axes of the input. Be careful not to modfiy the original\n",
    "    input x directly.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n",
    "      H' = 1 + (H + 2 * pad - HH) / stride\n",
    "      W' = 1 + (W + 2 * pad - WW) / stride\n",
    "    - cache: (x, w, b, conv_param)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    \n",
    "    ###########################################################################\n",
    "    # TODO: Implement the convolutional forward pass.                         #\n",
    "    # Hint: you can use the function np.pad for padding.                      #\n",
    "    ###########################################################################\n",
    "#     start = time.time()\n",
    "    p = conv_param['pad']\n",
    "    s = conv_param['stride']\n",
    "    \n",
    "    x_pad = np.pad(x, ((0,0),(0,0),(p,p),(p,p)),'constant')\n",
    "    N,C,H,W = x.shape\n",
    "    F,C,HH,WW = w.shape\n",
    "    \n",
    "    H_new = int(1+(H+2*p-HH)/s)\n",
    "    W_new = int(1+(H+2*p-WW)/s)\n",
    "    \n",
    "    out = np.zeros((N,F,H_new,W_new))\n",
    "    \n",
    "    for k in range(F):\n",
    "        for i in range(H_new):\n",
    "            for j in range(W_new):\n",
    "                out[:,k,i,j] = np.sum(x_pad[:,:,i*s:i*s+HH, j*s:j*s+WW] * w[k,:,:,:], axis=(1,2,3)) + b[k]\n",
    "            \n",
    "#     print(f\"original: {time.time()-start}\")\n",
    "    #raise NotImplementedError\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    cache = (x, w, b, conv_param)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward_naive\n",
      "difference:  2.2121476417505994e-08\n"
     ]
    }
   ],
   "source": [
    "# check your forward pass implementation\n",
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out, _ = conv_forward_naive(x, w, b, conv_param)\n",
    "correct_out = np.array([[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]])\n",
    "\n",
    "# Compare your output to ours; difference should be around e-8\n",
    "print('Testing conv_forward_naive')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\" color=\"red\">**task 1.2: backward pass of a convolution layer (7 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward_naive(dout, cache):\n",
    "    \"\"\"\n",
    "    A naive implementation of the backward pass for a convolutional layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives.\n",
    "    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x\n",
    "    - dw: Gradient with respect to w\n",
    "    - db: Gradient with respect to b\n",
    "    \"\"\"\n",
    "    dx, dw, db = None, None, None\n",
    "    \n",
    "    ###########################################################################\n",
    "    # TODO: Implement the convolutional backward pass.                        #\n",
    "    ###########################################################################\n",
    "    x,w,b,conv_param = cache\n",
    "    p = conv_param['pad']\n",
    "    s = conv_param['stride']\n",
    "\n",
    "    x_pad = np.pad(x, ((0,0),(0,0),(p,p),(p,p)),'constant')    \n",
    "    N,C,H,W = x.shape\n",
    "    F,C,HH,WW = w.shape\n",
    "    N,F,H_out,W_out = dout.shape\n",
    "    \n",
    "    dx = np.zeros((N,C,H,W))\n",
    "    dx_pad = np.zeros(x_pad.shape)\n",
    "    dw = np.zeros((F,C,HH,WW))\n",
    "    db = np.sum(dout, axis=(0,2,3))\n",
    "    \n",
    "    H_f = int(1+(H+2*p-H_out)/s)\n",
    "    W_f = int(1+(H+2*p-W_out)/s)\n",
    "    \n",
    "    for i in range(H_out):\n",
    "        for j in range(W_out):\n",
    "            \n",
    "            for f in range(F):\n",
    "                dw[f,:,:,:] += np.sum(x_pad[:,:,i*s:i*s+HH, j*s:j*s+WW] * (dout[:,f,i,j])[:,None,None,None], axis=0)\n",
    "            \n",
    "            for k in range(N):\n",
    "                dx_pad[k,:,i*s:i*s+HH,j*s:j*s+WW] += np.sum(w[:,:,:,:]*(dout[k,:,i,j])[:,None,None,None], axis=0)    \n",
    "\n",
    "    dx = dx_pad[:,:,p:-p,p:-p]\n",
    "    \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_backward_naive function\n",
      "dx error:  1.159803161159293e-08\n",
      "dw error:  2.247109434939654e-10\n",
      "db error:  3.37264006649648e-11\n"
     ]
    }
   ],
   "source": [
    "# gradient check\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = conv_forward_naive(x, w, b, conv_param)\n",
    "dx, dw, db = conv_backward_naive(dout, cache)\n",
    "\n",
    "# Your errors should be around e-8 or less.\n",
    "print('Testing conv_backward_naive function')\n",
    "print('dx error: ', rel_error(dx, dx_num))\n",
    "print('dw error: ', rel_error(dw, dw_num))\n",
    "print('db error: ', rel_error(db, db_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Spatial Batch Normalization (22 points)\n",
    "<font size='4'>Batch normalization is a very useful technique for training deep neural networks. As proposed in the original paper [1], batch normalization can also be used for convolutional networks, but we need to tweak it a bit; the modification will be called \"spatial batch normalization.\"\n",
    "\n",
    "<font size='4'>Normally batch-normalization accepts inputs of shape `(N, D)` and produces outputs of shape `(N, D)`, where we normalize across the minibatch dimension `N`. For data coming from convolutional layers, batch normalization needs to accept inputs of shape `(N, C, H, W)` and produce outputs of shape `(N, C, H, W)` where the `N` dimension gives the minibatch size and the `(H, W)` dimensions give the spatial size of the feature map.\n",
    "\n",
    "<font size='4'>If the feature map was produced using convolutions, then we expect the statistics of each feature channel to be relatively consistent both between different imagesand different locations within the same image. Therefore spatial batch normalization computes a mean and variance for each of the `C` feature channels by computing statistics over both the minibatch dimension `N` and the spatial dimensions `H` and `W`.\n",
    "\n",
    "\n",
    "[1] [Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
    "Internal Covariate Shift\", ICML 2015.](https://arxiv.org/abs/1502.03167)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='red'>**Task 2.1: forward pass of a (normal) batch norm layer (7 points).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(x, gamma, beta, bn_param):\n",
    "    \"\"\"\n",
    "    Forward pass for batch normalization.\n",
    "\n",
    "    During training the sample mean and (uncorrected) sample variance are\n",
    "    computed from minibatch statistics and used to normalize the incoming data.\n",
    "    During training we also keep an exponentially decaying running mean of the\n",
    "    mean and variance of each feature, and these averages are used to normalize\n",
    "    data at test-time.\n",
    "\n",
    "    At each timestep we update the running averages for mean and variance using\n",
    "    an exponential decay based on the momentum parameter:\n",
    "\n",
    "    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "    running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "\n",
    "    Note that the batch normalization paper suggests a different test-time\n",
    "    behavior: they compute sample mean and variance for each feature using a\n",
    "    large number of training images rather than using a running average. For\n",
    "    this implementation we have chosen to use running averages instead since\n",
    "    they do not require an additional estimation step; the torch7\n",
    "    implementation of batch normalization also uses running averages.\n",
    "\n",
    "    Input:\n",
    "    - x: Data of shape (N, D)\n",
    "    - gamma: Scale parameter of shape (D,)\n",
    "    - beta: Shift paremeter of shape (D,)\n",
    "    - bn_param: Dictionary with the following keys:\n",
    "      - mode: 'train' or 'test'; required\n",
    "      - eps: Constant for numeric stability\n",
    "      - momentum: Constant for running mean / variance.\n",
    "      - running_mean: Array of shape (D,) giving running mean of features\n",
    "      - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: of shape (N, D)\n",
    "    - cache: A tuple of values needed in the backward pass\n",
    "    \"\"\"\n",
    "    mode = bn_param['mode']\n",
    "    eps = bn_param.get('eps', 1e-5)\n",
    "    momentum = bn_param.get('momentum', 0.9)\n",
    "\n",
    "    N, D = x.shape\n",
    "    running_mean = bn_param.get('running_mean', np.zeros(D, dtype=x.dtype))\n",
    "    running_var = bn_param.get('running_var', np.zeros(D, dtype=x.dtype))\n",
    "\n",
    "    out, cache = None, None\n",
    "    if mode == 'train':\n",
    "        #######################################################################\n",
    "        # TODO: Implement the training-time forward pass for batch norm.      #\n",
    "        # Use minibatch statistics to compute the mean and variance, use      #\n",
    "        # these statistics to normalize the incoming data, and scale and      #\n",
    "        # shift the normalized data using gamma and beta.                     #\n",
    "        #                                                                     #\n",
    "        # You should store the output in the variable out. Any intermediates  #\n",
    "        # that you need for the backward pass should be stored in the cache   #\n",
    "        # variable.                                                           #\n",
    "        #                                                                     #\n",
    "        # You should also use your computed sample mean and variance together #\n",
    "        # with the momentum variable to update the running mean and running   #\n",
    "        # variance, storing your result in the running_mean and running_var   #\n",
    "        # variables.                                                          #\n",
    "        #                                                                     #\n",
    "        # Note that though you should be keeping track of the running         #\n",
    "        # variance, you should normalize the data based on the standard       #\n",
    "        # deviation (square root of variance) instead!                        #\n",
    "        # Referencing the original paper (https://arxiv.org/abs/1502.03167)   #\n",
    "        # might prove to be helpful.                                          #\n",
    "        #######################################################################\n",
    "        x_mean = np.mean(x, axis=0)\n",
    "        x_var = np.var(x, axis=0)\n",
    "        \n",
    "        x_norm = (x-x_mean)/np.sqrt(x_var + eps)\n",
    "        out = gamma*x_norm + beta        \n",
    "        \n",
    "        running_mean = momentum * running_mean + (1 - momentum) * x_mean\n",
    "        running_var = momentum * running_var + (1 - momentum) * x_var\n",
    "        \n",
    "        cache = (x, x_norm, x_mean, x_var, gamma, eps)\n",
    "        # raise NotImplementedError\n",
    "        #######################################################################\n",
    "        #                           END OF YOUR CODE                          #\n",
    "        #######################################################################\n",
    "    elif mode == 'test':\n",
    "        #######################################################################\n",
    "        # TODO: Implement the test-time forward pass for batch normalization. #\n",
    "        # Use the running mean and variance to normalize the incoming data,   #\n",
    "        # then scale and shift the normalized data using gamma and beta.      #\n",
    "        # Store the result in the out variable.                               #\n",
    "        #######################################################################\n",
    "        \n",
    "        x_norm = (x-running_mean)/np.sqrt(running_var + eps)\n",
    "        out = gamma*x_norm + beta\n",
    "        \n",
    "        cache = (x, x_norm, running_mean, running_var, gamma, eps)\n",
    "        \n",
    "        # raise NotImplementedError\n",
    "        #######################################################################\n",
    "        #                          END OF YOUR CODE                           #\n",
    "        #######################################################################\n",
    "    else:\n",
    "        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "\n",
    "    # Store the updated running means back into bn_param\n",
    "    bn_param['running_mean'] = running_mean\n",
    "    bn_param['running_var'] = running_var\n",
    "\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='red'>**Task 2.2: forward pass of a spatial batch norm layer (4 points).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_batchnorm_forward(x, gamma, beta, bn_param):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for spatial batch normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    - gamma: Scale parameter, of shape (C,)\n",
    "    - beta: Shift parameter, of shape (C,)\n",
    "    - bn_param: Dictionary with the following keys:\n",
    "      - mode: 'train' or 'test'; required\n",
    "      - eps: Constant for numeric stability\n",
    "      - momentum: Constant for running mean / variance. momentum=0 means that\n",
    "        old information is discarded completely at every time step, while\n",
    "        momentum=1 means that new information is never incorporated. The\n",
    "        default of momentum=0.9 should work well in most situations.\n",
    "      - running_mean: Array of shape (D,) giving running mean of features\n",
    "      - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output data, of shape (N, C, H, W)\n",
    "    - cache: Values needed for the backward pass\n",
    "    \"\"\"\n",
    "    out, cache = None, None\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the forward pass for spatial batch normalization.       #\n",
    "    #                                                                         #\n",
    "    # HINT: You can implement spatial batch normalization by calling the      #\n",
    "    # vanilla version of batch normalization you implemented above.           #\n",
    "    # Your implementation should be very short; ours is less than five lines. #\n",
    "    ###########################################################################\n",
    "    N,C,H,W = x.shape\n",
    "    x_t = x.transpose(0,3,2,1).reshape(N*H*W,C)\n",
    "    out_t, cache = batchnorm_forward(x_t, gamma, beta, bn_param)\n",
    "    out = out_t.reshape(N,W,H,C).transpose(0,3,2,1)\n",
    "    #raise NotImplementedError\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before spatial batch normalization:\n",
      "  Shape:  (2, 3, 4, 5)\n",
      "  Means:  [9.33463814 8.90909116 9.11056338]\n",
      "  Stds:  [3.61447857 3.19347686 3.5168142 ]\n",
      "After spatial batch normalization:\n",
      "  Shape:  (2, 3, 4, 5)\n",
      "  Means:  [ 1.38777878e-16  1.94289029e-17 -9.43689571e-17]\n",
      "  Stds:  [0.99999962 0.99999951 0.9999996 ]\n",
      "After spatial batch normalization (nontrivial gamma, beta):\n",
      "  Shape:  (2, 3, 4, 5)\n",
      "  Means:  [6. 7. 8.]\n",
      "  Stds:  [2.99999885 3.99999804 4.99999798]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after spatial batch normalization\n",
    "\n",
    "N, C, H, W = 2, 3, 4, 5\n",
    "x = 4 * np.random.randn(N, C, H, W) + 10\n",
    "\n",
    "print('Before spatial batch normalization:')\n",
    "print('  Shape: ', x.shape)\n",
    "print('  Means: ', x.mean(axis=(0, 2, 3)))\n",
    "print('  Stds: ', x.std(axis=(0, 2, 3)))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "gamma, beta = np.ones(C), np.zeros(C)\n",
    "bn_param = {'mode': 'train'}\n",
    "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "print('After spatial batch normalization:')\n",
    "print('  Shape: ', out.shape)\n",
    "print('  Means: ', out.mean(axis=(0, 2, 3)))\n",
    "print('  Stds: ', out.std(axis=(0, 2, 3)))\n",
    "\n",
    "# Means should be close to beta and stds close to gamma\n",
    "gamma, beta = np.asarray([3, 4, 5]), np.asarray([6, 7, 8])\n",
    "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "print('After spatial batch normalization (nontrivial gamma, beta):')\n",
    "print('  Shape: ', out.shape)\n",
    "print('  Means: ', out.mean(axis=(0, 2, 3)))\n",
    "print('  Stds: ', out.std(axis=(0, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After spatial batch normalization (test-time):\n",
      "  means:  [-0.08034406  0.07562881  0.05716371  0.04378383]\n",
      "  stds:  [0.96718744 1.0299714  1.02887624 1.00585577]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "N, C, H, W = 10, 4, 11, 12\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(C)\n",
    "beta = np.zeros(C)\n",
    "for t in range(50):\n",
    "  x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
    "  spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "bn_param['mode'] = 'test'\n",
    "x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
    "a_norm, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print('After spatial batch normalization (test-time):')\n",
    "print('  means: ', a_norm.mean(axis=(0, 2, 3)))\n",
    "print('  stds: ', a_norm.std(axis=(0, 2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='red'>**Task 2.3: backward pass of a (normal) batch norm layer (7 points).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for batch normalization.\n",
    "    \n",
    "    For this implementation you should work out the derivatives for the batch\n",
    "    normalizaton backward pass on paper and simplify as much as possible. You\n",
    "    should be able to derive a simple expression for the backward pass.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, D)\n",
    "    - cache: Variable of intermediates from batchnorm_forward.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs x, of shape (N, D)\n",
    "    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n",
    "    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the backward pass for batch normalization. Store the    #\n",
    "    # results in the dx, dgamma, and dbeta variables.                         #\n",
    "    #                                                                         #\n",
    "    # After computing the gradient with respect to the centered inputs, you   #\n",
    "    # should be able to compute gradients with respect to the inputs in a     #\n",
    "    # single statement.                                                       #\n",
    "    ###########################################################################\n",
    "    \n",
    "    x, x_norm, x_mean, x_var, gamma, eps = cache\n",
    "    N, D = x.shape\n",
    "    dbeta = np.sum(dout, axis=0)\n",
    "    dgamma = np.sum(dout*x_norm, axis=0)\n",
    "    \n",
    "    dx_norm = dout*gamma\n",
    "    dvar = np.sum(dx_norm*(x-x_mean)*(-0.5)*((x_var+eps)**(-3/2)), axis=0)\n",
    "    dmean = np.sum(dx_norm*(-1/(np.sqrt(x_var+eps))),axis=0) + dvar*(np.sum(-2*(x-x_mean),axis=0)/N)\n",
    "    dx = dx_norm*(1/(np.sqrt(x_var+eps))) + dvar*2*(x-x_mean)/N + dmean/N\n",
    "    \n",
    "    #raise NotImplementedError\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='red'>**Task 2.4: backward pass of a spatial batch norm layer (4 points).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_batchnorm_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for spatial batch normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, C, H, W)\n",
    "    - cache: Values from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs, of shape (N, C, H, W)\n",
    "    - dgamma: Gradient with respect to scale parameter, of shape (C,)\n",
    "    - dbeta: Gradient with respect to shift parameter, of shape (C,)\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the backward pass for spatial batch normalization.      #\n",
    "    #                                                                         #\n",
    "    # HINT: You can implement spatial batch normalization by calling the      #\n",
    "    # vanilla version of batch normalization you implemented above.           #\n",
    "    # Your implementation should be very short; ours is less than five lines. #\n",
    "    ###########################################################################\n",
    "    N,C,H,W = dout.shape\n",
    "    dout_t = dout.transpose(0,3,2,1).reshape(N*H*W, C)\n",
    "    dx,dgamma,dbeta = batchnorm_backward(dout_t, cache)\n",
    "    dx = dx.reshape(N,W,H,C).transpose(0,3,2,1)\n",
    "    #raise NotImplementedError\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  3.423838616073709e-07\n",
      "dgamma error:  7.0963199356067174e-12\n",
      "dbeta error:  3.275380797385891e-12\n"
     ]
    }
   ],
   "source": [
    "# gradient check\n",
    "np.random.seed(231)\n",
    "N, C, H, W = 2, 3, 4, 5\n",
    "x = 5 * np.random.randn(N, C, H, W) + 12\n",
    "gamma = np.random.randn(C)\n",
    "beta = np.random.randn(C)\n",
    "dout = np.random.randn(N, C, H, W)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fb = lambda b: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
    "\n",
    "#You should expect errors of magnitudes between 1e-12~1e-06\n",
    "_, cache = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = spatial_batchnorm_backward(dout, cache)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Adaptive average pooling layer (8 points)\n",
    "<font size='4'> In AlexNet and VGG-like networks, a 2D convolution feature map is usually flattened to get a 1D feature vector, which is then fed into a fully-connected layer. Since ResNet, such flattening is no longer used. Instead, an adaptive average pooing layer is used. Given a 2D feature map with shape of `(N, C, H, W)`, the mean across the dimension `H` and `W` are computed. As a result, we get a 2D feature map with shape of `(N, C, 1, 1)` that is equivalent to a 1D feature vector with shape of `(N, C)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='red'>**Task 3.1: forward pass of an adaptive average pooling layer (4 points).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_avg_pool_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass of the adaptive average pooling layer\n",
    "    \n",
    "    Input:\n",
    "    - x: Input data of shape (N, C, H, W)\n",
    "    \n",
    "    Returns of a tuple of:\n",
    "    - out: Output data, of shape (N, C)\n",
    "    - cache: (x,)\n",
    "    \"\"\"\n",
    "    out, cache = None, None\n",
    "    \n",
    "    ###########################################################################\n",
    "    # TODO: Implement the forward pass for adaptive average pooling.          #\n",
    "    ###########################################################################\n",
    "    out = np.mean(x, axis=(2,3))\n",
    "    cache = (x)\n",
    "    #raise NotImplementedError\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing adaptive_avg_pool_forward\n",
      "difference:  2.7173913643149127e-08\n"
     ]
    }
   ],
   "source": [
    "# check your implementation\n",
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "\n",
    "out, _ = adaptive_avg_pool_forward(x)\n",
    "correct_out = np.array([\n",
    "    [-0.05263158,  0.04842105,  0.14947368],\n",
    "    [ 0.25052632,  0.35157895,  0.45263158]\n",
    "])\n",
    "\n",
    "# Compare your output to ours; difference should be around e-8\n",
    "print('Testing adaptive_avg_pool_forward')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='red'>**Task 3.2: backward pass of an adaptive average pooling layer (4 points).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_avg_pool_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the forward pass of the adaptive average pooling layer\n",
    "    \n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives.\n",
    "    - cache: x as in global_avg_pool_backward\n",
    "    \n",
    "    Returns:\n",
    "    - dx: gradient with respect x\n",
    "    \n",
    "    \"\"\"\n",
    "    dx = None\n",
    "    \n",
    "    ###########################################################################\n",
    "    # TODO: Implement the backward pass for adaptive average pooling.         #\n",
    "    ###########################################################################\n",
    "    x = cache\n",
    "    N,C,H,W = x.shape\n",
    "    dx = dout\n",
    "    dx = np.expand_dims(dx, axis=(2,3))\n",
    "    dx = np.repeat(dx, H, axis=2)\n",
    "    dx = np.repeat(dx, W, axis=3)\n",
    "    dx = dx/(H*W)\n",
    "\n",
    "    #raise NotImplementedError        \n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    \n",
    "    return dx    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing adaptive_avg_pool_backward function\n",
      "dx error:  5.501115761541525e-11\n"
     ]
    }
   ],
   "source": [
    "# gradient check\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(4, 3, 5, 5)\n",
    "dout = np.random.randn(4, 3)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: adaptive_avg_pool_forward(x)[0], x, dout)\n",
    "\n",
    "out, cache = adaptive_avg_pool_forward(x)\n",
    "dx = adaptive_avg_pool_backward(dout, cache)\n",
    "\n",
    "# Your errors should be around e-8 or less.\n",
    "print('Testing adaptive_avg_pool_backward function')\n",
    "print('dx error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: ConvNet (26 points)\n",
    "<font size='4'>Now that you have implemented all the necessary layers, we can put them together into a simple convolutional network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='red'>**Task 4.1: Implement a CNN (14 points).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(object):\n",
    "    \"\"\"\n",
    "    A simple convolutional network with the following architecture:\n",
    "\n",
    "    [conv - bn - relu] x M - adaptive_average_pooling - affine - softmax\n",
    "    \n",
    "    \"[conv - bn - relu] x M\" means the \"conv-bn-relu\" architecture is repeated for\n",
    "    M times, where M is implicitly defined by the convolution layers' parameters.\n",
    "    \n",
    "    For each convolution layer, we do downsampling of factor 2 by setting the stride\n",
    "    to be 2. So we can have a large receptive field size.\n",
    "\n",
    "    The network operates on minibatches of data that have shape (N, C, H, W)\n",
    "    consisting of N images, each with height H and width W and with C input\n",
    "    channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_filters=[32], filter_sizes=[7],\n",
    "            num_classes=10, weight_scale=1e-3, reg=0.0, use_batch_norm=True, \n",
    "            dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Initialize a new network.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: Tuple (C, H, W) giving size of input data\n",
    "        - num_filters: Number of filters to use in the convolutional layer. It is a\n",
    "          list whose length defines the number of convolution layers\n",
    "        - filter_sizes: Width/height of filters to use in the convolutional layer. It\n",
    "          is a list with the same length with num_filters\n",
    "        - num_classes: Number of output classes\n",
    "        - weight_scale: Scalar giving standard deviation for random initialization\n",
    "          of weights.\n",
    "        - reg: Scalar giving L2 regularization strength\n",
    "        - use_batch_norm: A boolean variable indicating whether to use batch normalization\n",
    "        - dtype: numpy datatype to use for computation.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.reg = reg\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        assert len(num_filters) == len(filter_sizes)\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Initialize weights and biases for the simple convolutional         #\n",
    "        # network. Weights should be initialized from a Gaussian centered at 0.0   #\n",
    "        # with standard deviation equal to weight_scale; biases should be          #\n",
    "        # initialized to zero. All weights and biases should be stored in the      #\n",
    "        #  dictionary self.params.                                                 #\n",
    "        #                                                                          #\n",
    "        # IMPORTANT:                                                               #\n",
    "        # 1. For this assignment, you can assume that the padding                  #\n",
    "        # and stride of the first convolutional layer are chosen so that           #\n",
    "        # **the width and height of the input are preserved**. You need to         #\n",
    "        # carefully set the `pad` parameter for the convolution.                   #\n",
    "        #                                                                          #\n",
    "        # 2. For each convolution layer, we use stride of 2 to do downsampling.    #\n",
    "        ############################################################################\n",
    "        C,H,W = input_dim\n",
    "        self.M = len(num_filters)\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "\n",
    "        self.params['W1'] = np.random.normal(0, weight_scale, (self.num_filters[0], C, self.filter_sizes[0], \n",
    "                                                               self.filter_sizes[0]))\n",
    "        self.params['b1'] = np.zeros(self.num_filters[0])\n",
    "        \n",
    "        self.params['gamma1'] = np.ones(self.num_filters[0])\n",
    "        self.params['beta1'] = np.zeros(self.num_filters[0])\n",
    "        \n",
    "#         print(\"X - \",X.shape)\n",
    "#         print(\"W1 - \",self.params[f'W1'].shape)\n",
    "#         print(\"b1 - \",self.params[f'b1'].shape)\n",
    "        \n",
    "        for i in range(self.M-1):\n",
    "            self.params[f'W{i+2}'] = np.random.normal(0, weight_scale, (self.num_filters[i+1],\n",
    "                          self.num_filters[i],self.filter_sizes[i+1], self.filter_sizes[i+1]))\n",
    "            \n",
    "#             print(f\"W{i+2} - {self.params[f'W{i+2}'].shape}\")\n",
    "            \n",
    "            self.params[f'b{i+2}'] = np.zeros(self.num_filters[i+1])\n",
    "#             print(f\"b{i+2} - {self.params[f'b{i+2}'].shape}\")\n",
    "            \n",
    "            self.params[f'gamma{i+2}'] = np.ones(self.num_filters[i+1])\n",
    "            self.params[f'beta{i+2}'] = np.zeros(self.num_filters[i+1])\n",
    "            \n",
    "        self.params['W_affine1'] = np.random.normal(0, weight_scale, (self.num_filters[-1], num_classes))\n",
    "        self.params['b_affine1'] = np.zeros(num_classes)\n",
    "        \n",
    "        \n",
    "        # raise NotImplementedError        \n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Evaluate loss and gradient for the three-layer convolutional network.\n",
    "\n",
    "        Input / output: Same API as TwoLayerNet in fc_net.py.\n",
    "        \"\"\"\n",
    "\n",
    "        scores = None\n",
    "        mode = 'test' if y is None else 'train'\n",
    "        ############################################################################\n",
    "        # TODO: Implement the forward pass for the simple convolutional net,       #\n",
    "        # computing the class scores for X and storing them in the scores          #\n",
    "        # variable.                                                                #\n",
    "        ############################################################################\n",
    "        cache = {}\n",
    "        outs = {}\n",
    "        \n",
    "#         tic = time.time()\n",
    "        outs['conv_l1_out'], cache['conv_l1_cache'] = conv_forward_naive(X,self.params['W1'],\n",
    "                                self.params['b1'], {'stride':2, 'pad':(self.filter_sizes[0]-1)//2})\n",
    "        \n",
    "#         print(f\"Time taken for conv1 layer: {time.time()-tic}\")\n",
    "#         tic = time.time()\n",
    "        \n",
    "        outs['bn_l1_out'], cache['bn_l1_cache'] = spatial_batchnorm_forward(outs['conv_l1_out'],\n",
    "                                    self.params['gamma1'], self.params['beta1'], {\"mode\":mode})\n",
    "        \n",
    "#         print(f\"Time taken for bn1 layer: {time.time()-tic}\")\n",
    "#         tic = time.time()\n",
    "        \n",
    "        outs['relu_l1_out'], cache['relu_l1_cache'] = relu_forward(outs['bn_l1_out'])\n",
    "        \n",
    "#         print(f\"Time taken for relu1 layer: {time.time()-tic}\")\n",
    "        \n",
    "        \n",
    "#         print(f\"X: {X.shape}\")\n",
    "#         print(f'Conv1', outs[f'conv_l1_out'].shape)\n",
    "#         print(f'BN1', outs[f'bn_l1_out'].shape)\n",
    "#         print(f'Relu1',  outs[f'relu_l1_out'].shape)\n",
    "#         tic = time.time()\n",
    "        for i in range(self.M-1):\n",
    "            outs[f'conv_l{i+2}_out'], cache[f'conv_l{i+2}_cache'] = conv_forward_naive(\n",
    "                                         outs[f'relu_l{i+1}_out'], self.params[f'W{i+2}'], \n",
    "                self.params[f'b{i+2}'], {'stride':2, 'pad':(self.filter_sizes[i+1]-1)//2})\n",
    "            \n",
    "            outs[f'bn_l{i+2}_out'], cache[f'bn_l{i+2}_cache'] = spatial_batchnorm_forward(\n",
    "                                     outs[f'conv_l{i+2}_out'], self.params[f'gamma{i+2}'], \n",
    "                                              self.params[f'beta{i+2}'], {\"mode\":mode})\n",
    "            \n",
    "            outs[f'relu_l{i+2}_out'], cache[f'relu_l{i+2}_cache'] = relu_forward(outs[f'bn_l{i+2}_out'])\n",
    "            \n",
    "#             print(f'Conv{i+2} - ', outs[f'conv_l{i+2}_out'].shape)\n",
    "#             print(f'BN{i+2}- ', outs[f'bn_l{i+2}_out'].shape)\n",
    "#             print(f'Relu{i+2}- ', outs[f'relu_l{i+2}_out'].shape)\n",
    "        \n",
    "#         print(f\"Time taken for all other layers: {time.time()-tic}\")\n",
    "#         tic = time.time()\n",
    "        \n",
    "        outs[f'avg_pooling1_out'], cache[f'avg_pooling1_cache'] = adaptive_avg_pool_forward(\n",
    "                                                                outs[f'relu_l{self.M}_out'])\n",
    "        \n",
    "#         print(f\"Time taken for pooling layer: {time.time()-tic}\")\n",
    "#         tic = time.time()\n",
    "        \n",
    "        # def affine_forward(x, w, b): return out, cache\n",
    "        outs['scores'], cache['ln_cache'] = affine_forward(outs[f'avg_pooling1_out'],\n",
    "                                self.params[f'W_affine1'], self.params[f'b_affine1'])\n",
    "        \n",
    "        scores = outs['scores']\n",
    "        \n",
    "#         print(f\"Time taken for affine layer: {time.time()-tic}\")\n",
    "        # raise NotImplementedError\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0, {}\n",
    "        ############################################################################\n",
    "        # TODO: Implement the backward pass for the simple convolutional net,      #\n",
    "        # storing the loss and gradients in the loss and grads variables. Compute  #\n",
    "        # data loss using softmax, and make sure that grads[k] holds the gradients #\n",
    "        # for self.params[k]. Don't forget to add L2 regularization!               #\n",
    "        #                                                                          #\n",
    "        # NOTE: To ensure that your implementation matches ours and you pass the   #\n",
    "        # automated tests, make sure that your L2 regularization includes a factor #\n",
    "        # of 0.5 to simplify the expression for the gradient.                      #\n",
    "        ############################################################################\n",
    "#         tic = time.time()\n",
    "        loss, dscores = softmax_loss(scores, y)\n",
    "        \n",
    "        for i in range(self.M):\n",
    "            loss += 0.5*self.reg*np.sum(self.params[f'W{i+1}']**2)\n",
    "        \n",
    "        loss += 0.5*self.reg*np.sum(self.params[f'W_affine1']**2)\n",
    "        \n",
    "        dx, grads[f'W_affine1'], grads[f'b_affine1'] = affine_backward(dscores, cache['ln_cache'])\n",
    "        dx = adaptive_avg_pool_backward(dx, cache[f'avg_pooling1_cache']) \n",
    "        \n",
    "        for i in range(self.M, 0, -1):\n",
    "#             print(dx.shape)\n",
    "            dx = relu_backward(dx, cache[f'relu_l{i}_cache'])\n",
    "#             print(dx.shape)\n",
    "            \n",
    "            # def spatial_batchnorm_backward(dout, cache): return dx, dgamma, dbeta\n",
    "            dx, grads[f'gamma{i}'],grads[f'beta{i}'] = spatial_batchnorm_backward(dx, cache[f'bn_l{i}_cache'])\n",
    "#             print(dx.shape)\n",
    "#             print(cache[f'conv_l{i}_cache'][0].shape)\n",
    "#             print(cache[f'conv_l{i}_cache'][1].shape)\n",
    "#             print(cache[f'conv_l{i}_cache'][2].shape)\n",
    "            dx, grads[f'W{i}'], grads[f'b{i}'] = conv_backward_naive(dx, cache[f'conv_l{i}_cache'])\n",
    "#             print(dx.shape)\n",
    "            \n",
    "            \n",
    "        \n",
    "        grads[f'W_affine1'] += self.reg*self.params[f'W_affine1']\n",
    "        for i in range(self.M, 0, -1):\n",
    "            grads[f'W{i}'] += self.reg*self.params[f'W{i}']\n",
    "\n",
    "#         print(f\"Time taken for backward pass- {time.time()-tic}\")\n",
    "        # raise NotImplementedError\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        return loss, grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check loss\n",
    "<font size='4'>After you build a new network, one of the first things you should do is sanity check the loss. When we use the softmax loss, we expect the loss for random weights (and no regularization) to be about `log(C)` for `C` classes. When we add regularization this should go up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss (no regularization):  2.3024747776392935\n",
      "log(10):  2.302585092994046\n",
      "Initial loss (with regularization):  2.3037231687370543\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "\n",
    "N = 50\n",
    "X = np.random.randn(N, 3, 32, 32)\n",
    "y = np.random.randint(10, size=N)\n",
    "\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (no regularization): ', loss)\n",
    "print('log(10): ', np.log(10))\n",
    "\n",
    "model.reg = 0.5\n",
    "loss, grads = model.loss(X, y)\n",
    "print('Initial loss (with regularization): ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient check\n",
    "<font size='4'>After the loss looks reasonable, use numeric gradient checking to make sure that your backward pass is correct. When you use numeric gradient checking you should use a small amount of artifical data and a small number of neurons at each layer. Note: correct implementations may still have relative errors up to the order of e-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 max relative error: 3.041419e-05\n",
      "time:106.2176775932312\n",
      "W_affine1 max relative error: 4.277113e-08\n",
      "time:7.081507205963135\n",
      "b1 max relative error: 3.122502e-09\n",
      "time:0.7422418594360352\n",
      "b_affine1 max relative error: 1.296123e-09\n",
      "time:0.23305940628051758\n",
      "beta1 max relative error: 4.594574e-06\n",
      "time:0.7195770740509033\n",
      "gamma1 max relative error: 5.442050e-06\n",
      "time:0.7302432060241699\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 2\n",
    "input_dim = (3, 8, 8)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "np.random.seed(231)\n",
    "X = np.random.randn(num_inputs, *input_dim)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = ConvNet(input_dim=input_dim, dtype=np.float64)\n",
    "loss, grads = model.loss(X, y)\n",
    "# Errors should be small, but correct implementations may have\n",
    "# relative errors up to the order of e-2\n",
    "\n",
    "\n",
    "for param_name in sorted(grads):\n",
    "    tic = time.time()\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))\n",
    "    print(f\"time:{time.time()-tic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit small data\n",
    "<font size='4'>A nice trick is to train your model with just a few training samples. You should be able to overfit small datasets, which will result in high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['W1', 'b1', 'gamma1', 'beta1', 'W2', 'b2', 'gamma2', 'beta2', 'W_affine1', 'b_affine1'])\n",
      "(Iteration 1 / 250) loss: 2.292152\n",
      "(Epoch 0 / 50) train acc: 0.070000; val_acc: 0.098000\n",
      "(Epoch 1 / 50) train acc: 0.080000; val_acc: 0.097000\n",
      "(Epoch 2 / 50) train acc: 0.180000; val_acc: 0.125000\n",
      "(Iteration 11 / 250) loss: 2.264657\n",
      "(Epoch 3 / 50) train acc: 0.170000; val_acc: 0.120000\n",
      "(Epoch 4 / 50) train acc: 0.200000; val_acc: 0.130000\n",
      "(Iteration 21 / 250) loss: 2.299306\n",
      "(Epoch 5 / 50) train acc: 0.180000; val_acc: 0.128000\n",
      "(Epoch 6 / 50) train acc: 0.210000; val_acc: 0.133000\n",
      "(Iteration 31 / 250) loss: 2.074540\n",
      "(Epoch 7 / 50) train acc: 0.200000; val_acc: 0.131000\n",
      "(Epoch 8 / 50) train acc: 0.220000; val_acc: 0.137000\n",
      "(Iteration 41 / 250) loss: 2.217079\n",
      "(Epoch 9 / 50) train acc: 0.240000; val_acc: 0.150000\n",
      "(Epoch 10 / 50) train acc: 0.270000; val_acc: 0.153000\n",
      "(Iteration 51 / 250) loss: 2.013435\n",
      "(Epoch 11 / 50) train acc: 0.240000; val_acc: 0.137000\n",
      "(Epoch 12 / 50) train acc: 0.240000; val_acc: 0.130000\n",
      "(Iteration 61 / 250) loss: 1.906034\n",
      "(Epoch 13 / 50) train acc: 0.240000; val_acc: 0.132000\n",
      "(Epoch 14 / 50) train acc: 0.250000; val_acc: 0.152000\n",
      "(Iteration 71 / 250) loss: 2.165520\n",
      "(Epoch 15 / 50) train acc: 0.220000; val_acc: 0.141000\n",
      "(Epoch 16 / 50) train acc: 0.230000; val_acc: 0.137000\n",
      "(Iteration 81 / 250) loss: 1.990190\n",
      "(Epoch 17 / 50) train acc: 0.270000; val_acc: 0.141000\n",
      "(Epoch 18 / 50) train acc: 0.260000; val_acc: 0.149000\n",
      "(Iteration 91 / 250) loss: 1.873145\n",
      "(Epoch 19 / 50) train acc: 0.300000; val_acc: 0.145000\n",
      "(Epoch 20 / 50) train acc: 0.240000; val_acc: 0.142000\n",
      "(Iteration 101 / 250) loss: 1.938221\n",
      "(Epoch 21 / 50) train acc: 0.210000; val_acc: 0.121000\n",
      "(Epoch 22 / 50) train acc: 0.160000; val_acc: 0.084000\n",
      "(Iteration 111 / 250) loss: 1.927217\n",
      "(Epoch 23 / 50) train acc: 0.180000; val_acc: 0.095000\n",
      "(Epoch 24 / 50) train acc: 0.180000; val_acc: 0.088000\n",
      "(Iteration 121 / 250) loss: 1.954285\n",
      "(Epoch 25 / 50) train acc: 0.200000; val_acc: 0.119000\n",
      "(Epoch 26 / 50) train acc: 0.180000; val_acc: 0.105000\n",
      "(Iteration 131 / 250) loss: 1.816399\n",
      "(Epoch 27 / 50) train acc: 0.190000; val_acc: 0.110000\n",
      "(Epoch 28 / 50) train acc: 0.180000; val_acc: 0.107000\n",
      "(Iteration 141 / 250) loss: 1.907616\n",
      "(Epoch 29 / 50) train acc: 0.190000; val_acc: 0.094000\n",
      "(Epoch 30 / 50) train acc: 0.190000; val_acc: 0.100000\n",
      "(Iteration 151 / 250) loss: 2.104090\n",
      "(Epoch 31 / 50) train acc: 0.180000; val_acc: 0.100000\n",
      "(Epoch 32 / 50) train acc: 0.170000; val_acc: 0.089000\n",
      "(Iteration 161 / 250) loss: 1.576770\n",
      "(Epoch 33 / 50) train acc: 0.160000; val_acc: 0.079000\n",
      "(Epoch 34 / 50) train acc: 0.180000; val_acc: 0.078000\n",
      "(Iteration 171 / 250) loss: 1.844283\n",
      "(Epoch 35 / 50) train acc: 0.160000; val_acc: 0.081000\n",
      "(Epoch 36 / 50) train acc: 0.170000; val_acc: 0.086000\n",
      "(Iteration 181 / 250) loss: 1.712881\n",
      "(Epoch 37 / 50) train acc: 0.180000; val_acc: 0.093000\n",
      "(Epoch 38 / 50) train acc: 0.170000; val_acc: 0.087000\n",
      "(Iteration 191 / 250) loss: 1.925377\n",
      "(Epoch 39 / 50) train acc: 0.170000; val_acc: 0.089000\n",
      "(Epoch 40 / 50) train acc: 0.160000; val_acc: 0.083000\n",
      "(Iteration 201 / 250) loss: 1.438385\n",
      "(Epoch 41 / 50) train acc: 0.160000; val_acc: 0.087000\n",
      "(Epoch 42 / 50) train acc: 0.180000; val_acc: 0.096000\n",
      "(Iteration 211 / 250) loss: 1.560196\n",
      "(Epoch 43 / 50) train acc: 0.200000; val_acc: 0.106000\n",
      "(Epoch 44 / 50) train acc: 0.170000; val_acc: 0.090000\n",
      "(Iteration 221 / 250) loss: 1.639562\n",
      "(Epoch 45 / 50) train acc: 0.180000; val_acc: 0.090000\n",
      "(Epoch 46 / 50) train acc: 0.150000; val_acc: 0.087000\n",
      "(Iteration 231 / 250) loss: 1.570554\n",
      "(Epoch 47 / 50) train acc: 0.170000; val_acc: 0.083000\n",
      "(Epoch 48 / 50) train acc: 0.220000; val_acc: 0.097000\n",
      "(Iteration 241 / 250) loss: 1.414866\n",
      "(Epoch 49 / 50) train acc: 0.270000; val_acc: 0.133000\n",
      "(Epoch 50 / 50) train acc: 0.250000; val_acc: 0.114000\n",
      "Time: 261.3424937725067\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(231)\n",
    "\n",
    "num_train = 100\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = ConvNet(\n",
    "    num_filters=[16, 32],\n",
    "    filter_sizes=[7, 3],\n",
    "    weight_scale=1e-2\n",
    ")\n",
    "\n",
    "solver = Solver(\n",
    "    model, small_data,\n",
    "    num_epochs=50, batch_size=20,\n",
    "    update_rule='sgd_momentum',\n",
    "    optim_config={\n",
    "      'learning_rate': 1e-2,\n",
    "    },\n",
    "    verbose=True, print_every=10\n",
    ")\n",
    "tic = time.time()\n",
    "solver.train()\n",
    "print(f\"Time: {time.time()-tic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAHgCAYAAADg78rsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOy9fZBUV3rm+ZzMusAt1E0WbeyQsilQsw6IZhBVTXWrutmYWLSx0CNacraQKMsw69iddUfs7kQYWlGzxZoQSIuH8taqYTZiYifcY0eMR7JckpBrJdFj5AkRMzG4kbroKsRgw7ppIXBKnsYNiVtUQt3KPPtH5klu3jzn3nNv3q/Men8RClH5cfPcz/Oe9+N5GeccBEEQBEEQRDrIJD0AgiAIgiAI4j5knBEEQRAEQaQIMs4IgiAIgiBSBBlnBEEQBEEQKYKMM4IgCIIgiBRBxhlBEARBEESK6El6AGHxS7/0S3zt2rVJD4MgCIIgCMKTc+fO/R3nfJXsva4xztauXYvp6emkh0EQBEEQBOEJY+xj1XsU1iQIgiAIgkgRZJwRBEEQBEGkCDLOCIIgCIIgUgQZZwRBEARBECmiawoCCIIgiO5naqaIiVOX8UmpjIdyJkZ3rEdhMJ/0sAgiVMg4IwiCIDqCqZkiDrx5AWWrAgAolso48OYFACADjegqKKxJEARBdAQTpy43DDNB2apg4tTlhEZEENFAxhlBEATREXxSKvt6nSA6FTLOCIIgiI7goZzp63WC6FTIOCMIgiBSwdRMEVvH38PDYyexdfw9TM0Um94f3bEeppFtes00shjdsT7OYRJE5FBBAEEQBJE4Osn+4v9UrUl0O2ScEQRBEInjluxvN74Kg3kyxoiuh8KaBEEQROJQsj9B3Cf1njPGWBbANIAi5/xbSY+HIAiCCJ+HciaKEkNsMSf7k+Du4qUTPGe/DeCvkh4EQRCEwCtxnfAPJfs3I3LwiqUyOO7n4NG1tjhItXHGGPsigJ0A/nXSYyEIHWjS7n5o0oyGwmAeR5/ahHzOBAOQz5k4+tSmRespIsHdxU3aw5rHAfwzAJ9LeiAE4QW1llkc6CauE/6hZP/7UA7e4ia1njPG2LcA/Ixzfs7lM99hjE0zxqZv3LgR4+gIohVa6S4OaNIk4oAEdxc3qTXOAGwF8CRj7CqAPwHwGGPsZfsHOOe/zzkf4pwPrVq1KokxEkQDmrQXBzRpEnFAOXiLm9QaZ5zzA5zzL3LO1wL4dQDvcc73JjwsglBCk/bigCZNIg4oB29xk/acM4LoGEZ3rG/KOQNo0u5GSKWeiAvKwVu8MM550mMIhaGhIT49PZ3Y75MeDQHQdUAQBEHowRg7xzkfkr1HnrMQoCo9QkArXYIgCKJdUptz1klQlR5BEARBEGFBxlkIUJUeQRAEQRBhQcZZCFCVHkEQBEEQYUHGWQhQaf3ihFo1EQRBEFFABQEhQKX1iw8qAiGCQNW8nQ2dPyIuSEojJuim7i62jr+HoiSnMJ8zcWbssQRGRKQdp0EP1DzsJCzaGdD5I8LGTUqDwpoxIG7qYqkMjvteFgqDdS5UBEL4haq6Oxs6f0ScUFgzBtxualpxdSYP5Uyp54yKQAg7do+5KkZBBn1nQAsyIk7IcxYDdFN3H1QEQnjh9JirIIO+M6CqfCJOyDiLmKmZIjKMSd+jm7pzoabEhBcyj7kTMug7B1qQEXFCYc0A6Cb3i5VzRVJ0QTd156Nq1UTFHwTg7hlnAF0bHQZV5RNxQsaZT/xIKKhWzlnGyMvSpYQlsUEGXuejykukit7OhXrnEnFBxplPVMn9h9+62HhfTKiyBzMAqSeN6A7CKP4gDbXuYHTHeqn0AnnM0wUthIg0QjlnPlGFKkplC6NvnG+Sy5BnmtUgKY3uJIziDyrZ7w4oLzH9kMwRkVbIc+YTN4+YVWn2iHHUcktkfjLhbaMHdSudvJINQ2Kjm6t7O/ncBoHCYOmGZI6ItEKeM5/4DUm4BTBLZYtWaA46fSUbRkVXt5bsd/q5JbqPbl4IEZ0NGWc+KQzm0ddraH8+nzNdP0+hqmbSGNLz0+A8jFBWt5bsJ3VuF2uD+rD3++DUBaw78AOsHTuJdQd+gINTF0IaaXJ04kJosV7Piw0Kawbg0BMbWxJ9MwCqks9u27AK75z/VLktWqE1k7aVbJDk/HZDWd1asp/EuV2sxRVh7/fBqQt4+ey1xt8Vzht/HylsCmHEydBpRRuL9XpejJBxFgDZ5Dk3v4Bbc1bLZ09fuoHb5dbXBWleoSVB2toiJZWT0o25Skmc28WaUxT2fr/6/nXl60kZZ1MzRRx+6yJK9edrX6+BQ09s9LV/nbYQWqzX82KEjLOAOCfPh8dOSj/nJqvB4D+HrdtJ20o2Cm9PpybFtzvuJM5t2jyxcRH2fqvkf5KSBZqaKWL09fOwqvd//9ache++NosX3r6I0pylfY120kJIdf6KpTKmZoodsx+EN5RzFhJuuQuyHCIA6F2Sxf7JWcobsJE2+QG/OSlu+SBTM0UMvPAu9k3OdlxSfBjJ/Emc207MKQqDsPc7q2hBp3o9aiZOXW4yzARVXjPSOune8oPb+eu2fV3skOcsJFRegW0bVjVc0VnGUOEcOdPAnfkF3JmnvAEZaVrJ+vH2uOWDAGjZjqBsVfDca+cBpPf8hxVOibvlVdo8sXER9n4/++jqppwz++tJoOsB7IR7yw+y8yqg8GZ3QcZZSMhyF7ZtWIUT54qNG6nCOUwjC8ZaNdHoxgonhyRs/OSkeFUjujXBrnCuNNDTEAaNMjwYZZJzp+UUhUXY+y3yyl59/zoqnCPLGJ59dHVi+WZuepNO3O6tTkOMf9/krPT9bg/XLyYY75JWQkNDQ3x6ejrpYTSxdfw97QcIUMtB+2h8Z3QDSjGyHBIAMLIME09v7oiH6sNjJ1117XRw9l10Gi5AzQPiDAdGbcCprmW/fSJl45w4dZl6UBK+UD0v3Oim6yms+5FIFsbYOc75kOw9yjlrAy+9Gb+rmG7PgwHUx0yVQ2JVeMdowbmdP93MHOc1o6MNFoe4axjaa6pxqhYw5AUgVBQG85h4ZjNy5n0NyV7DfTqzX0+drhXWrVqIxH0orBkQr/yiiVOXlV6UnGng3kJ10eXBuB0zt4m4UyZpt3wQt1ZedlaYzYLFOuHEOMrrwwiTqcYpcjGdLIbFSlpIQ+jcL7L8xbWKqnng/vXUDVphUYXrO/E66FbIOAuIaqI58OaHuGtVlZMwA/CtzQ9iaM1K3zdBp984qmN2+K2LrjkkaZyk3c6FKh+EoxZ2+KRUBhggyyhwFr/paIPFJRfRbqGGajwiF3OxLVbSQjcYK4K8hmxRt2iFhV041U3XQTdAYc2AqCaasothBtQm6BPnai70M2OP4djIAAB4SmqEFbpK0p2vOmalsoVtG1YpL8ZtG1ZFN6gAuJ2LwmAeeYUxKfJBPhrfqXShlRxCxjrhi06Ri1CNJ8sYdm3Jp0Y+pVMJem+nsWVaUGT3CwOwZ7i/cT0lrX2X1pBqN10H3QB5zgKywjQaVYV+Ed4iANorlXZWe8LLUyyVm0Jrca6MpmaKyCjCV0Ctk8KKXkPZZSFNuHkAC4N5LRkDXbX8wmAe0x/fbKqS27WlecXcKXIRqrBvhXOcOFckg6xOEA+5rtdDtu2kjZUgqI6RTrgvyS4kafZOdeJ10M2QceYT8VAIapgJSmVLGv5SGVxBbxznw8BpGkXlzrc/PFeYBv7+rgW3wqpOyjlThV9LZatJpdttgtA1qKZmijhxrtgwaoUhM7RmZWN7nSIXIcbz3GvnW4z0TgwrRUHQyVtn8abadk6xKOKoVQWm7VryOkZe4b4kFzNpDqmmrXVep6fxtAsZZz6QyRpEgcwYUT1Ac71Gy2t2ZA8Dnd9rB+dx0jFkxQMgTQ8HGV4hCPGQ9ZogdAyqqZmi0pDZNzmLfZOzLVpwYnsiFJG2h1lhMI/9PjSaFtsDOujk3U7hyNKeTEvOn6BYKmP09fO+WyJFSbsGju69J6INomAlH8K+p9k7lSYPfJo9jHFBxpkPdAydMFhhGtg6/l7Tg0MlR+clU6dz0+sYP34mSb/Hyf4A8PtwiHvy9sq/8KNr52ZQiYeTV+/CW3MWRt84j+mPbzYJHqflYSY7P7or9DAf0J1i5AWdvFXH1P4sUV1Jt8sWjo0MKPXmrCpvLAzTcF2p7rGw7z27eLjYfrv7njbvlJ00eeDT7GGMCzLOfBDX6uYX9xYa3ibxQFAZO15eKS8lbQZ1wn3QXDU/x0m2GtV9ODiFKMUqXzWuMPDaN1WvQZlxAKhzDv0YuFaFN3LS7AR5mIVpxKiMq11b8k2GJCA3wsN6QLeTjxX3RBB08pZ5PYwMw535Bc9nxArTaHh6dYSUk5wkD05dUL7np8+n2zXhdu+1u+9p8k7JSEvrvDR7GOMitdWajLFljLEPGGPnGWMXGWMvJD2muFY3lWrrJOv24Nnz/R8q31M1XReI6lFnuM5ekSg+5xyTyouke5z6eg2cGXus6WFQGMxrV7Eefutii3CtVeWNYoso8AojVzjHlw6cxFpbJZaquvOFty8qjQ+/DyGVh61YKmtXg4UtZqsyrk5futHSAH3XljwmTl1uqmDTfUB7Vb+lRchXh6DiooXB1qbyDyzraWkTJ2N+4f6x0b13k5gkp2aKeEXS31Pg5WW243ZNeO2bHw+dE9l5okKYVjqlAj1K0uw5uwfgMc75Z4wxA8B/Yoz9O8752aQG5CYyGjUVzpUipmeu3MSe7/8Qr/zW11veKwzm8fr0NZy5clO5bftEJR5ObpWVAtVDTOc4GVmGQ09slL6nI/DrFqZpt1hDxcGpC9K8PyfCXhTjXmZkpBOB6vjY81x0cfu8bihGNWHtf20W+ydnfXuT3Iwr+wrdb6K6/QGt4xWLSsg3Ck9bO6Elp9fjYRdBVjtzVrVRyKL7jItrkrQf4wxjrl49lYSNDLdrwiva4MdDJyMt3qk0k3YPYxyk1jjjtaafn9X/NOr/JdoIVNxQ+ydnAw9EJZLY7vfOXLmJtWMnpWHCsz+95bl9Z/hUxzBQeZFkE8y2Datw+tKNwDlrIgleR2U/CrxW7SrcjDA3/BhmRpZh5KurW0KF9jHohGJUExZ3GJuAXthYN0TnJ1Hd+YB2u1YmTl3WznFT7XuxVJZWLLaTD+dl1IU1eftpDm4vZBF/i0rrO/MLTR64uCZJVe6XDLvIrA5u14SXgVrhvCUnOEpjKw3h9rhJIv8tbcc5tcYZADDGsgDOAfivAPxLzvn7CQ/JtdrMC4ZgLnHTyGLbhlXSvCInsklCZ6LPMubbiHDbbDsTjNsx0jVZVIaqDqrm3GEbhbI2Xn5xVmu+rDAgda47He0+Pzk3uqtflWFkT1QXhgJjtcWRMLzcQlDFUhn7Jmcha7moqzsntuO8p4Lmw8VZhebH028/js57N6lJy0/epV1kVge3a9NN8gVofo5HXSCxmKsW4/QwpvE4pzbnDAA45xXO+QCALwL4GmPsH9jfZ4x9hzE2zRibvnEjPqFSN5e+W+/dIJN7X6/RSKDW9aY482l03PB+PDWC23Vdr7DVrtsNGwiC5A3Jco/2T862lWcC1Lxbdkwji8NPbmzkn/glnzNxdXwnZp7f3qgwm/zguvLzOsdU97Dbj4Xb+dfNr1HdTxnGGguhPcP9uLdQxa05qyknzCsHEACsqmM/AamQr1tupvOeCpqwHIUKu+ocyI7/8iXyfYwzl0f3maGb17Z13UocKWzyNQava7MwmMdLuzdLuw34yb9tlzSq9qe1w0E7pPE4Mx5gUk4CxtjzAOY45/+X7P2hoSE+PT0dy1imZorK/olAzUCzTwjLl2RxZz6Yd0RM3H6NAwbU2gShliul8qg0Pq/o9eiGaWRc+4g6vToyZKtyt2MbhLzN++W1+t86/l7bhpiMnGlg+dIe5e+rflfmXTONLI4+VZuM7AnMXqfP63zoVOoJ9g73Y2jNSqn3wW+Cs45+oCqcHdT7mDMNzB7a3jIOlZyEGIO4p1TnS7ToUqE6xvZty8bkpsclO3bLl2QxN19p+s7UTBGH37rY4h11O2ey7Qc5x27bE+fW6enWvRed43E7Zqqqad3P61wbYeL3eokav9dD2kKFKpI6zoyxc5zzIel7aTXOGGOrAFic8xJjzATwLoDf45y/I/t8nMYZAAy++K5rcniG1cNEdeHGT2+XXRXyVQhnht+vZhnDS7s3N26Ejc//WWADsR0yDPje7oGWG1I1UTAAvT6MWd0cNFnukuyB4sdA8fP7AHDV5SZXTbJ9vQZ2PvJgS74e0KoJp8ve4X6pp8GPYSqub9k94GWgyHAmfvvx5Obq4U6dYg07x0dar0tAz/AKarS4GeEyY1H2G7u25BvXg86xEt+R5SR6GexBjVDVpOx1jdmPoVMqxw0xHrfzArTeM0aGAQwtOXWq8xj0eARF9XtZxlDlPHaDx+38OY3rdgz7uI26uM+rwM04S3NY80EApxljHwL4EYA/VxlmSXDoiY2uYZAqR1MIJohhBsCzQklFhfOmkN5cAoYZUDsOB978sOk1cdPK8ps4gDvzFWQz6hibeCefM7FnuN9zDLJ8OpE07nTL+wnv7B3u1wqriTG4IcIsObN5e7fmLJw4V8TojvX4aHxnQ3qkHUHkl89ew+CL77aEJbxCe3bE9S1DNxw1NVPEwAvvYu3YSeybnMXc/AKOjQyg6nPBWCpbuGtVsXe4X3v8gFpQWEfOIqgkwuiO9TWDQLIPa8dOYt2BHzS0vFShllfOXmuE3XWM2LJVwctnr0mvl94lPa5jdiuUUGmOucmSeF0b9lBSYTCPB5bppUWLidUtPCV7z6ryFrkRWThLhPKE5qOdKAskVPdkhfOmtAs3/bcw8crxtM85QUOFScjaBJWwiZLUFgRwzj8EMJj0OFSIB1rYITgnQXLBBPYEZT+VW2FTtpXqA3qJvk6tNzvfWLeySTbkj9+/pjR+VW1pBM7Ez9Ed67VX66cv3UBJ01tT4dyzJYwwupxGqyzRvN1z6ab47hba00G344TzOItuByoJDTfs+mkyj6wM1UQjjoN9O8skyaR+EpbtnoCW2d1GhfNGCoKyelbrF/UI2nkAuF98YvfCurUcmzh1Wes5ZB+T7v0FuHu9/eqyOfMqnf2JVaHYsHFWLco8pRzAK2evNfXbjQqv82d/VunkZaqKr+LuDpBEdagXqQ1r+iXusKYgqhylsBAx86mZYlsSIO1idw/7DR3KsIfmVDl1y5dk8bvf3qRtbIgH7QtvX9QyDhj05Qrc8vOMDMMDy3pQqntaVb9lz31Yd+AHbRnuMvKOHJyg17UqXGgnivvGfoycxpDqUKkmV6/wmCpvSWZ4A/5D0H6urXbQCU+6jT3LGK4cfVzrswy1wo5Xzl7z1CsTYwrrOvGbu8sAHKtfx35C0VHj9uz0G4ILEjrUzQ/9aHynZ6hQdY+5XT9J5NlFiVtYM7Wes05h24ZVnsn2SSK8GIXBPKY/vpnYWO03aRiTzqvvX28YZ0NrVuLVD663eNu+/ZX7ng2dyVGECHRNHvFA0/G0lZ0lgzbsvQvdfkswNeNduRtED67R/sqRg+MkA2CFwrulW/EZhcK8/Ri5idzaUZXMq1bvh9+62FSAIDtm9l6Mo2+cxwNLe/zL1KD2bHHmiYWt81eam2/yajvxihDYr0Mvj/gK08DkB9ddx+8MJYUh/G3fpu79zXFf+031rCqVLRycuoB3zn/a8LCqckTD8sC4PTvFPaVjdAWVjtDxrov70EtKR3WPqQS1F1N3ACDdOWepZ8/3f6ht7BgZ1iKnEAfFUhkbn/8zTM0UcaSwCX2aOVJhw4BAuU0qnJOCLAz6ytlrjYlHV7JCd+Izsqzx0Jt4ZrPusAMhHmgiR0snlB50Apfl4AD3ja4sY6ii5omSXc+cQys/JOwHrVt+iCqfTyCMLqA5t0hGqWxp5S013qt4G94qXj57DUt7MujrNRp5bd9Yt1IZFRWv50wDLimbTdyZr3ier8JgXpkzaX/dzeA2jSzmFyrSRQyr5eRL8/acuX0503DNR23armObhcG8r/vik1Kt9Znbr7189lpTCP3WnIWXbTmBYedLje5YrxzPQzlTO19Lt6WZSqLlzNhj2Dvc75p/53zuitzfiVOXXfMPK5y3nf/VDXIfZJwF5ODUBdeWSEBz4vrEM5sx8fRm5QQRJXfma8nva8dOKifVqBErUeD+TduOnpnOpCB+8+DUBTz32vlQQ0TLbYnUhcF8IK0yL+yTCwCMvn4+stZUXghbWBjFpbIFcEiNAJ2kX1VivB/sk7qsN6eTewtq76Xwgtj7yaYBUeywZ7gfd+4t4MyVm0oDQ+RAzR7aju/tHtBeAOmcr2cfXe35usrgzjKGo09twpzCe8w5mopdnAhj4KPxnTj85EatSUs8H+7cW8ALb19sXBd+FqcP5cxQxKfD1MsqDOalRVBGhnnma9nxygfzMvKmZoo4ca7YdGxk+oFA7RwAzR5lN41CsR37XCnL+VSRlj657UJhTR+o5B9k2HNZahfLh66hrbgQY0+iDZJowu3VG1OH4S/1NVqouMkJFEvlSEK5tx3XgJ9CAl3s+RUDL7zb9rb9ypR44TYer7Dl/XBzG/cFv59P6RWi0SlC0enAERSvwhQ3RIWmzsjEcfdSuVd9T4VIIRDHKMsYnn10dVMxgCqMJTxXYRRPTZy6rHUfNC0i6hRL5UYEw+7pVMlpjO5YH7gbjBPV8bU/D0UHDCG/pAqHDq1ZickfXW/21jL33/nE8exVPTOFge3VFm1ufqHlfY5akZR931QhadGeTTYPcQDvnP+0aTF1a87CvslZvPD2RU/tzCQKCqKAjDNN/Gju5EyjSQ/pu5OzSN4sayaJwgAGYPSN8675TIC3sOiSLMPZj241QplRTaiAuqG400sgq/Brh5xptG3IMqBR+Sj2w8hmYGSrnuegXdzClgenLoRiCHlNJPaHsU6OW9DxyCZ3J0KbLKhXzk8epMBPvuUK0/DsF3mksMlVid+r4q1Pkafox5vVrlfTqvKGR0bcpw8s61HmiXmJEgc5LwKn8eI0JFU5YBOnLrdca1aFu1bDrjANzz6l9tCh23F2e89+n3ktiG6X1QVQqmforTnLMzcuaPeOtEFhTU10V2zA/fwcUVaeNsMsKTjcJzCgta2RyDOxh2LnK9xVaiNMlva05gqq8h8Kg3nMHtqOq+M72w5zbnzoc02u+SBwALfrk6EzHCnymNrtlJUzDWl+yLYNq6Q5H6Kytl3DLAPg09tlrB076ZogLXJPdH7Nz7Gw54GJlAU3Tl+6EamYJSC/LguDeezaknfNmwKAX9xbCCUMJEKQx0YGANQS8MX5P/TExpZ7ycgyHHpio/b2w2jtVipbLXlikx9cb9ESBNT5sX29hpbGIqB+XngZL6pwqJv23J17C9LnFWNw/S1nvl/Q42w3QnVkWoI8J73CxKqFYacVFJDnTBM/VndpzmqsiqL06nQTQjrAvtK2t3FJKtdqzqrCyDD09Rqe4QY7OteL28rbK59RF9nCwKryRp6PTmm8CmFIA83eEmeVod0L8Or76v6fumRQ3y+NW0vHUyvwc6uW5izscXRacAvbCUMxbOx6W9s2rMLEqcvYPznbJPHhVSEJtOoKiiIJP62OBKow89GnNmHi6c1taUlF9Ty1qhyH37rYMhYd/Stn2oSzO4xqH3WeEbIKTDdKZUv6vHILz8okOIIcZ6cR6lZZav+sLBS+zMi4FtK4HQevKtFOgXTONPGjt5PPmZibXwhcpaXL3uH+VMt4+MGttdHasZMxjkSOXw0h3esln6A4sHiAr7C1P9IN1QjNKj9toPzua840ML9QaSSSC5mCNF3zYkx2OQUZRgboyQbPO1P9tsi/UWlGeU1yflCFb4WeoFdPzHZbDsWh1eh8DjllKbZtWCVtpeZMZ9DpK6zzjBC5y34XUM72fW6/JdMP09VRlIlpC9xa0tmvW/uxE+95yT4FbR+WNkjnLAR0G3IbGYZbd+4pK5PCIp8zcaSwCSc//DRyIzAOVFpLaamw8ZuvoKN/x6B/XUWBuG5KZQumkcXxeijKORnJ8sM47nsMhtas1GoO7ccwyzDgzvxCkyFwa87CH7+fHsMMuC+d4IVVBaxqOIaZc+IF1Hl3YRqDqrQOUQ0+/fFNHClscpVIAPQ1tYDWnqu6hhmrrzKWGRncW6hqt8+zP4dkHkD7uRb78ZX+FS2Gueh2IdtHe6cQt8WQ8PYEadUm2veJ3xfeM9lviXBfkBxXIXshM368vI4y4+1ufd60FxY48fKCdYph5gV5zjSZmikqJ1FWVyCPswJya72FUTthqTRhV9u2P7z8YGRqk2AU+FUD1/WcqQoOkkC1Gg2jo0Mn0U5lZVzkc2bT5JNk9w87qqR/GU5PGnB/Il/h8Jr6RSj8+3022qtLw+hO4Lyn3J7XOUW1Zjv3n/33D05daKn6NY0sdm3Je3p+/fyOHZVnzO34imtbtc9uHUjaabZu30Zcxp2b54yMM03cblQh1xRTjnoD0cJI1b6oE8mZRovHRJet61aGlqvlJMOA7+0eaLji3W5eN0M+zajaowy++G5XeGd12Tvc36isTEJyxi9GluGBpT3Sc5QzDdy5txCqxEsU6FS8+qGd1BJhaISxKHHeU14Gnyw82I6R6Px9e6V0ljEMf6kPP752W8uA9dtayU3hYK9HGy9VCoT9fpSFjr1aRnkxNVNsyVM1sgwTT2+OxEBzM86oWlMTt7BWlcdvmAG1sNLUTNHVBdxplMpW4Ad0VIYZUDu/h9+6qCXOKMIJncYKiUDy1EyxRdMtrWRYOALLorIynzNTb5gBNYPmnlWRVs0efnIjJp7xL34d5sSgU/jn1mUhCL1LgufaiWd9GNV9zm14pUfIKmVlFaNGhqFXQ5jV2fbtxLn7rd8qnOPMlZtahpkQhlVVV8qOlZvCwctnr7mK0G7bsEpaJWvfmggd249VuzIaL7x9USpT8sLbF7W+HyZknGmS1jLc707OpkrRPI2YRjaUtlWlsuWpwB0kPyQt3JlfaMnx+50/vZDIwiMIHMDIV1e3LWMiHuSdpKJvpe8AACAASURBVIs0Z1Wb5GecbYtmD233dVyMnvCmhnY7QQThr392R/meTP7FToYxPDx2sqFsHxTR4s2OzjzilIpwtrDK50yMfG21liyR/ffbeTYJgVmZoajKAfO6f1TGs/gtHQkYoe8m8COjIWvxpBpTEpEDMs40SWsZLmmouSMmKT9aSm54NR2OckKPeopzPuimZoqhdROIA86BE+eKGN2xHlfHd2LvcH8gvSbxIG9nQWZk5K2t7Iix5XMmtq5bGfi3BPY2R7JWSH562rq1uvLLfMSCx3751uYHXXutVjgHh1oIVYflS7LSUJhbb0w7zueI89y+c/5T11C1rE9pu8+mT0plz36Zdtq5fz4plXH60g1fXTEA+TUuMx5lEZCwukGEBVVrahJW+xEiPhjQlGcQ5fkTD6Kcj4ToDPMXDueIPlndqfDdaQh9rsJgHkcKmzC0ZqWvpHCG2oN64IV3Mb8QTPvtK/0r8BcuPTAFFc7BAKz9gomrP29v4vQKW4rE7E716obJy2ev4eSHn7Zo9KlaGvnJO5TljNkpDOYx/fFNz3ZcHLX8KdW23AzHLGPS/Cq3SmodxDNO1nlChGOnP77ZkBqRpUn4+S1dY1LWFcMroV/mRXQ7H0n0xCbPmSZTM8XIPRdxEoY6fNoRDzixotPJ0QiCfWUWdX3N0hDDTTL8KHynlVLZapxzv6EccfpKZct3paBowK5jmNl/78yVm02TppFh8HOpGhnWMDRkoRqRmB22kLMIsXUi9jZAwiOlqpq2v+r1DPmkVJZ6kewcKWzCsZEBz2NXLJWxb3IWG5//M1+SQqr92LZhlfY2ZFPDnC3t4YW3Ww190QNWeKOCXm/iearreXPul8qDbL83/Bip9vsrTqhaUwM/fTU7AVHlKatM6VaiFDDdaxNj7XTZiSwDPl9XN3drKJ92cqaB5Ut7YsvH7Os1MPP89lDkF/wgxICH1qyU9nUNW4hWIDxEYfWSTQp7FZ+u8KoOQqJC1q9TpwG5E2fFoFsFdZYxXDn6OABHZwGmt3g0sgwjX10tldcQ+xWlOoCQytCVidKRyggqOeUmth0GVK3ZJn76aqaZLGNNhkRhMO/ZEzDM306SW3MWJn90Hb/6y8tb3ms3YfnEuWJjRZnWwhFdKrx2rDiibSgfNaWyFauRVKpPlG7exihuAQ7gxLm/wYE3L0iNpLJVCd0wE71To/DGxY39Ggnzend6kUTYb8/3f4j99SIuP/eYMx/ULYf22UdXA2jNq9LePV4Tll6+tDXrqWxVArVgW74kK+2T7CSfM5va99nz21SUrQqee+28q3cxaDGEKE5IAjLONOjU8I6d4yMDuHL08ZYVQGEw75m4HAYVzkOpmGwHq8JbqrgYgJGvrW5rbCLPCaglpNJN1Xm0ewsIo9wtzyYqW7dsVSPPJVvak2mqAj196UbXLFgFYYdonUenbFVwxkfI24l9HioM5rFX0ng9W3+Ybx1/D/smZwNdF1aVu/bxDGLEzs1XcGxkALOHtmPiabm0Szv9L0VHBJWB1s4cntT8T/OIBmZEuUpxIowHJ1MzxdhUNtMoZCpWRoee2KhdySZD5DkVBvNYkbARGgVR5eulgb5eA99oo1pStOECOjuP023s9xaq+Ma6lY0cnm5YsAK1SV3k5vmpZk0Cp+Ev8+hUqrzhsWsHEYoNC477BUZC2uV4Pe/OKf0isHv+dHBKkNhpZ1+SioZ07xM3RMohlpUnhT1J2s7EqcuLXo6jWE/ibdf7IAzgNBqh7cLBsCQEgdc0cmvOwg9/GlzAeM9wf2NSKaXs3PsJ2XMOV+PkzJWbODhVS6IPe8JK0va3VxpGXXDTDk4dQpWBHMZaW+TIhWmsesmD6FRU+v0Nga6EiZN2vHntkt4rMUV0cOpNE7K4fLesgNtByCe0izCAk86vi4KyVemKMJaKdnbNniqQppzDvl4DDyzzp5bkZZy8Uk8ED3vCiqonri5lq4KXz17zzKHLJiCoK7AqvCkCEuW1JhasoisAQ/t5w85uBc6qYidB5ibVMSkM5n0brTJvXpyQcbaIqHCOfZOz2PP9HzZeS9NkokMUYYcwTY6JU5c7OpHejS7drbZwTlftGi2mkWkS+ARqk4SskEVFljEcHxnAzPPbfXvySmXL1cPAUevPmLQGXlImUiXhBYo9AhJ1fmuxVG6IOn80vhPVNh4Adg+USgBWeGUFqrlJZSPa0wtk+MkpFFW8SRlmABlnixJ7eMKv69qvBlPYsJQLVYhm2UkiihvCHkc3egTbxXk1FgbzbRWX3LWqODP2GK6O78SVo4/j6vhOjO5Yj5/emNPeRpXzxqQSZPHldYe9HEJOUzsYWYY9w/3SpPIMvDszdDr23K2oHzZ29f+Mz/tflU+mEoB9pd4rWqDqKaoahT29QIZK583pDU0ylGknlmmWMfbbjLHPsxp/wBj7MWNsexy/TcgR5dCFwTx2bclrT7wjX1uNnmxySbN+hUGTIEzzMcizt3dJD/p6jdDH8eyjqxM3PNOIMzzTTqswpzElvAx+vLHLjExjTO32h0wboi3SkcImaVL590YG8NOjO7v6OrWH++Jw5ImcPD/XoGlkmvLJAHgKwNqLBgB5T9EHlvVI9zlnGhhas9I1VKqSxPjc0h7XwoSkiKt90//IOf8XjLEdAPoA/GMA/xbAuzH9flv0GpmOMAr8IKqU/KyAs4zh5IefUguYlBO2V8MuxBil+GSnIo63CM/sGe5Hn482XnZKc/M4OHWhIV4aRAi4bFUbY+p0HTInF1/8ZtPforG7Ez+tivI5E3/32b1Q+4lGib21UzYGoWjRP9MPC1XeqF73IwAbtFl6qWy1tJPaPzmL6Y9vNnJCVdsulS0sX9qDY3Xx27QQV4BKLGQeB/BvOecXkVzagG+e2vLFpIcQCX4n8QrnXVmJmGaSDuLmcyaOjQw0VqWEOyI8s/ORBwN9/858pRE27HQh4CjxSijXTdcQnQ6qCeWSmUYmUB6t8GYNf6kvglE1E+QatIvm+qm6dBYNOHPT3IwGr1CpW4hfHE8/bbKiJi7j7Bxj7F3UjLNTjLHPAZ2j4JCUQjBBpIHpj2/60hta7HDU8rI+vzS9mllxomt8ZDw+K3L5ZJO2c2IVITG3dA2RW5RkB5iFKveVVmKnbFVw9edlLF+SzuusWCpjaqaoXXXpzPXy25xcBgewb3IWW8ffw7YNq1yvLzedtCSIyzj7JwDGAHyVcz4HwADwP8T0221DchNE2rA/yqNMgC6Wynj57DUKZQfg7+/RMRM5PPacHpmyPVBbrZetitQ7Iq7xh8dO4rnXzkubbtsn1j3f/yH2Tc66en3ENnWf71vXrQy9KMaqcJy+dAMv7d4cyIP2SamM3/32ptSK5x548wJyigIZ+5Hs6zVacr3CnHeLpTJeOXsNX+lf4Vq1maa5Pi7j7OsALnPOS4yxvQAOArgd02+3TafJTRDdD0etJdfV8Z343u6BpIdDdCA50wi9ZZEdBmDtF8xGKyAhbHqksMl1QcFRq8rr6zUa/RizGebZ87VYKmPr+HtYO3YSZ654iwrfma+4Gg9OfnztdiRhZpmmmFcPSsFD9V6UOj0ok6BsVVCas6T7Yj+SdyU53WHPuxw1pYJPb6u9jW7t1+ImLuPs/wEwxxjbDOA5AFcA/FFMv902ozvWt90cu1ug4xAfXov00TfuiwrTWUkvafRqmEYWh5/ciNEd66WSFGEgJkNZ+NErimhVOXqX9OCj8Z1YvrQHVkXPKPIbei9bFc/OCPbPRoVTU0z0oNQJywL31faPjwwov5MzjUSuRQ6g4nH+ZCHFqNppVXnNMJcdJWcXhiRhPIaEU8bYjznnX2GMPQ+gyDn/A/FaWL8xNDTEp6enw9pcC1MzReybnI1s+51APmdibn6BigI6iCwDNOc1IiL2DvfjnfOfJl452ddroDRn4aGciW0bViU2pnzOxN/evqvlhcr7qLoMCgNwbGQAE6cuJ55XKcRPBQ+PnVTmWR13VBf6qYoMQoYBjLHIhHgZgI/Gd2Jqptjwtq4wjdiv0ZxpYPZQPEpfjLFznPMh2Xtxec5+wRg7gJqExknGWAa1vLOOoTCYT6XbOE7OjD2Wut6BhDtkmCXPqx9cD6UhepYx/MrnlgT6bs40MPP8dnxUF7U9ca6YmLH4SamsVWXo1VYty1gobYVWmEbD86TKh4sLe86Tm/Brvh7OtBNGf2AGKEWUOaLtkPBQzmwp9iiVrdiV8lV9qOMmrv0eAXAPNb2zvwXwRQATbl9gjK1mjJ1mjP0lY+wiY+y34xioG0Gbp3YTaYrJE0QnUKmGI0FT4Rw/+8V8oO8yhsaE88LbF5Mt8GDA7HX3lGMG98o808jipd2bcWxkoO08MGH/TM0UceJcspNyTwaNvLn9ioIGlYJ9GMnsHFAuwL0Oc7spL8VSWVrsUUVNciTODiVpqNqMxTirG2SvAFjBGPsWgLucc6+cswUAz3HOvwxgGMD/yhj7csRDdSVI89Ru4uDUBdy+S54zgkiKoM+fW3MW9k3OYu3YycTTEng958f1My7vCVHUw29dxOgb59sez605qxFKS7oq2are9xbKjkGWMaWCfRgJ9PmcqdyOe/5bJhQ5EpWhfdeq4srRx12jV2Gabmmo2oyrfdNuAB8AeAbAbgDvM8aedvsO5/xTzvmP6//+BYC/ApC4fO9iDm2+fPbaomh+nWFq1z5BEMkiJvBS2dIuFPBi3+Rs4vlmOtj7pjoZ3bFeq8JThZFlGN2xXpqIbxpZDH+pr8UAMo0s9g73oxxxBx1hMMrGJsa0wjTQG1Lj5zQoNMTVvul3UNM4+xkAMMZWAfj3AN7Q+TJjbC2AQQDvRzQ+LaZmirh5516SQyBioMqBQ09sxOjr5xMTpySINJNBB6mIdxGeRkMbj6vlS3qaDD+7BMq2Datw4lyxZfMZBpz88NPgP6qBsypVjE10DBBjCiuHMi2Nz+Oq1rzAOd9k+zsD4Lz9NZfvPgDgPwD4Xc75m473vgPgOwDQ39+/5eOPPw534DairoQh0oOo3pr++Cb1kiQIIlXk68aS6L+6wjQwv1AJpf+zaGfl9M757cMcFqJ3qGxcgy++G0mIfvmSLObmKw1dvij7bbpVa8ZlnE0AeATAq/WXRgB8yDn/3zy+ZwB4B8Apzvn33D4btZRGUhcnkQw508CdewvkOSMIYlFhGlns2pJvGH9+msiHibMoxDSyjXy7oNJWvUbGlxFr/80oSFxKg3M+CuD3UTPQHgHw+xqGGQPwBwD+ysswi4OgCYK9RibS9jpENJTKFhlmBEEsOspWBa+cvabdcNyLoHlwzqevXag2aDVl3/KlOD4yoC28nGS/zdgkRDjnJzjn363/96caX9mKmi7aY4yx2fp/j0c8TCVBEwTnrKqnGjbQvlYPQRBEt5EzjUYbJ9GXczEXZcWFc8rymsJU01eWMd9FG27FWH6bqcu+XxjMY/bQdu2ir6QqNyMtCGCM/QLy88oAcM7551Xf5Zz/J6SoK83ojvX47uRsJEmwwo38ytlri1qqIwhZxlDlnI4bQXQhKqV2N+V8okaWMXze7Ak1L8vI1OQ+7Ij568S5YlNONoNaGkO5/SzDzkcedJ0LRT/UIPvFUJOEOn3phvb3k6rcjNRzxjn/HOf885L/PudmmKWVqKqTjj61CUcKm7AnYXXqqNi6bmUk2xVClB+N74xk+wQRJQxAlnIeApEGqYO089LuzaF3dFlwTIIMwK4teRwpbGpqvu4lIqzCqnC8+v511+/66YfqhAONkK0OSVZuxt0ZoWOJKu4s2nBMzRRx+tKNSH4jaf7y01+Esh3Tkb8n8gGmZoowQ9K3IYg4MI0sepdkI22H0+m4hZ2iaoqtotNM6L3D/SgM5kM3YmXhTtm81c5VreNtu122sGtLsCR9nRCtCKNHWQzgBc1omkQRdxZWub2fWDcSllu9LMnfK5bKOPDmBeza8kW6mImOQDz0vVTyFzuHntiofK8wmMfRpzZpJ3a3Q1+v0VEh1L3D/RhaszI2hYFP6nlgYc1hOvnXK0wjslZbnAO5XgOflMqNxX8S0HymSRRu9GV1b08a2oZ0MmWrglffv47f6NKwMNFZeE0txVI5kAxAEvT1Gol1y/DyWBQG81i+NHod9aTbXfnl5bPXsD/GjgcrTCO0OczIMgx/qc/zc4xB+XsMNa2ydrg1ZzUqVQ+8eSERA42MM02iaHp+a87qao+ZIGcakYcFKpzjxLmiciIJ44YlvDEyrK0WMt0AB0JrIxM2RpZh67qVnvdjljEcHxnAzPPbceiJjbGGEP2Qhh6IaSQKT9/SHvk1zZj3edB5IizJMjywtAdnrtz0/Kybwdy7JBuqVzopOY10PkFSSGEwj29EkNhetipdLaNhGlkcfnJjLGEBt0TR3iVZfPsr+Y7LHekkGAMmntmMiac3xxJuSjNhqLVHgZFh+ODqLc/7scI59tcbpU+cuoxdW/JtS1gw1IqDdAw908hgaqaIrePv4eGxk9g6/p7UexFHYUDONFJrnMbJPWc1QJ1bcxYyHnOYzvN/vsJD8VLqGmZ+5oIkFgFknPng6s+jOUEVzjvm5v+Vzy1pMSaF/pCsIe2uLXkUBtt/sOtyu2xJc1HuzFdw4lwxEgObqCEe0IXBPA4/uXHRG2hpZM6qautOiU8VS2WcOFfE6I71gUOcWcawZ7gfr/zW17USue8tVDH6xvkmIVRZeCnqSjqxuBSViAyg61qCX8mMpMmZhi/DPonqYDLOfBCV9SwShMXNf9/YSdfpYQAOPP5lvLS72TMyN7+Akx9+2pIDYK/kiau66qF69assF6VsVXD15+XEcmiiJA2hxEqVNxJoR98477sRsZFBw4jvZm9yJ1K2Ktg3ORvYs1HhHJM/uq5dlV7laDEiVeGlsC8Vu+itqNYrDOZxZuwxHBsZiCXPLWyMDNN67uVMIxXPkijJMODO/IKvdKIk5DRi6a0ZB1H31gTU/TVFc1YdMgxNFYduvbtqFTAfoiwJkZhGFl/pX4GzP70V66rFyNT2VVcBgAENHbKpmSImTl2ONMduaU8Gv7frEeyfnFWqHx8bGcDo6+dT2Z7JyDDf4zIywPKlhm9jyE6vkcG9Cm9b2oEBgQUiAeCqQ7Nu4/N/Fih/hDHg2O4BAOiY5PvFQF+vgVI92Too+ZzZaPh9Z37BtwK9FznTwOEnN2Li1OVGb0kxOR9480Iqi7eMTE2DTPXMy9WPe8ZlrjKNLJYZmY4rgPDCrrkmDFQ/+5gzDaUYcrsk3luzW5B5f0wji2cfXe0rfm1PFl6m8I6J0mS7YSZ+Q6zoXvmtr+PK0cdxdXwn9g73e3obxGqwHayqvmEG1Cp5BGL16ZyAw0SEQ3KKVaLwrD2wLJ2r3yCrVqsKbcOs18g0eQaOjwzg6vhO9C1fGorm1kM5s+2Huz3XyMhmAh0TztHweFC7n/Rwa85S3pu6iFBnqWyFbpihvl1RqCVCqvsnZ7FvcjaVhhlQewbsGe5vmYeMDENPljWqD1WGGUPNM9lthplpZHFsZADHRwaQz5kozVm+9lGEtZMgnTNUShHeLeeKqjCYx8tnr2lto8qbk4VvzVkYff180/bFb8jChCKcuG9yFs+9dh4VzpGvj+NIYRMAtYdPjDfO1Z9bz7WoPH5WheOeVYFpZJv20672HLZydlhEmUh+fGRAKU8QljdzdMf6wJ6qnGnYFiW181YqW42QjF+Py9qxk8jnTGzbsKqltQyRHJ/dXUh6CJ7Inr1hsyTLMB+icXmksAlDa1Y2zU9z8wtaxkj6Ygjtk2/T45llLFERWjLOfCJW407yOTPwBGdVOfZNzmL645sNA0uV31YqWw0viTBuRLKsGN/ojvUtYTsjwxqGJHDfwFxhthcO80L1YIg6FCuMHGEE5m2GNFAzVLtdwsTJvslZTJy6jG0bVuH0pRuNB/jaL4TnWZo4dTlQ65YMQyOU5HyIWlWO3iU9mHl+u29hTZHMvmtLHqcv3UCxVJaOz5luQESHV9i+nWdpJxF2WsW6Az9oPOuO1RdiD4+dDPU3OgWxEC8M5rF1/D3fhplbulFcUFgzJMJIeH/57DUcnKoZWX6rQ1qSZZ0eK9vfIrz40fjOyGLp9p+VlcDHFWoSlbB2wwyIv/1LWiiWyni53ltOhGzcdIX8Fk+I7frle7trk4lqUVIslbF1/D1s27DKtxxK2arg9KUbODP2GPI5Uzo+MszaZ0lIieSLwTADaqH3MLEv1vdPzuLg1IW2Q8gq0l4yYJ8PvQr5sqym/WcvyEvaMAPIOAsN0U6kXV55vxYeDWI8iItw4tTlllwMq8KVQnp+DSXTyLSUk6tENznkfUnjNI5kVV7ifFFVoDszz0drvAvsHk0Vwgv2DQ0RVSfi3lgsE38ShBmiWwxE+ezhqC32b0cUFQnjTGcifvSKtlJeGmwVzvHja7cxumM9PhrfiTNjjyVumAFknIVKGCdUrKaE8SCseR3ExKZaKahe99v9oGxVMb9QQdZ2d7nlSskmROf+5XOm0ksTxkNMtu+FwTxe2r05ciOxUxPSc6aBrePvRf479uPjZbSXrQrO/tRbRNXJQzkTUzPF1K/4AaDLlQwI1MJmOm2K2iVOj7Cf57SRZfiNR/sjle3gAPZPzmql0CTVBcANMs5CJsy2Lfbwo9cEb092V3kfVijEEwuDeWmljxtzVlW7uk8V2rTv35mxx6RtYkwji5d2b/YxMjmqYxJ1A2WR6xbWKrnXyES+4hSUylbkniaG+yHLqZlik9Guwm++YobVtPj2KeRV0sazj1KP2G7n3kIFs9dvJz0MKUEeLwxA1cd9aVU43jn/aeSVCH42n7ZWYGSchcjUTDGU0m6ZIePm3XJWlYzuWA9DMoPfmV9QNnA9UtiEY/Vy47BRhTadyLxpYr9Uxo34XLN4b2ungm0bVrn+blBhyeVLsg3VcOcq0DSyWPsFU3v1piLLWGP//vlTj+A3Ep68GdDkNW0Huwr9vslZDL74LgA08sPCoMo7p3l1zjTwimbld1ogR587suNT5fpthsLGqx1VoCcV8/+9UtlKldZkEl0A3CAR2hDxW0mmQlUpcnDqglSyY+9wf6PKUzD44rvSCSmfM3Fm7DHPMYS1L3bcpBy88LPvB6cu4JWz15oeFl7VNw+PnQz0UBKitoXBfENkV1RBbtuwqmUc3YKRYXhgWQ9KcxZWmAYYC88AEucKSEb0M0i1aRh0qwhoEvgVy46SNFUCG1mGiadrkQgvyZssY6hy3ri/S3NWzQiLeF8Y4q+mNzIME89sjj3XjERoY8KPW3T5Eve8GpmnaWjNSmlIS7RFsaPS8dIdYxQJ+7LeeLocKWxqEtrNMiY1zIBayyjn88MrpyDoqokDeO61842QnD1MKxtHtyDkLUTFb5iFA+JcOT2pUSZQ2wWe9wz3Sz3PUWD3ih59alNq9fc6Db9i2VGSlnEAaKw6dMSZK5yjd0lNhHXm+e21Ti8x7MsK08CdezFr4aXQ/Us6ZyHix9o3shkwVJTXusyImjh1WXqji0pMu9WvGouuESK2tf+12dBWSvZJNwhHCpukxpgTvwURQHviqRXOm3TmdH6vGxC5YsJTGKaHoFgqN3kz+3oN7HzkwcjEZDmavconP/w0Fg/Ws4+ublzTorKs05pIE8Gxe4miFOYWWNX7c4VMD9PJnfkKnrOJpEft0TIyLJKWXIC7R9yqcDz3WqsYfJKQ5yxE/Hibbpfd1c5lRpTbZO98T9Vqym8D156QvRXtGiz21j4iidyJygB1M0wLg/m2GqLLPHN+NYbcvKlpxa6XFvaq2r65W3MWJn90Hbu25JtyDMPEfm3G5cESXm/RGYEMs8XFnuH+xrM6rnMvrnPdNnaVukj62rGT+KRUDi3f1Ek+Z+KBZT2RGWZ7hvs9C43aie6EDRlnIaJTaSZ4KGcqP8cAqRHlZlw433NLrtdl4tRl6aoqZxqBJ8Z2ki7FBGY3CGQ3U1DDdOcjDwYeG9A8uU/NFD3b1NhDacdHBnDxxW92rOwGAFRR08CLKvxoVXhDTPaj8Z2+JWC8sFczqyqbw0as2H/nT/Vz60ibLz20cy72DvdjaM1KPPfaeV/e4HbPvpCV2Tr+nm/vMEfNWAt7ISnmvKgWRRy1dBevZ0bZqjTSVJKGjLOQEXlHbpOsMBRUVZV7hvulRpTq80aWSQ0PZw6UX3etyst1u2wFykkL4rmzI2vt4yYw69cwPX3phvT1nGlo5SDZDU+VYWvHHkqzV9rG2bkg7Gn+rlXFlaOPaxmZQR4+9mty4tTlUJ119mrmOO2fCue+KvcqnKcxRWbRcXV8py/5CDvLl2Tx8tlrviq5c6aBvl6jrWveNLLYtmFVY5EblLtWNdRrkKOeQhPiNp0US2WtZ0ZaPGhknEWE2yT7lf4V9w0FxxVuZBmG1qyUfq8wmMfEM5ubNLn6eg1MPB1NlYmbXpofEVkgnJYYfnLJnIYpAM9wqJsxOvHMZpguGnZOw1M3fOv8nOy4RpWbLpLfw/TEiGvGa4XKALAAO2a/JsPO6bN30Uh7Yn5Swc++XiNULcdORTzrgkYChDGuex4zDPjW5gcD5UGK20w8g09futF23maF89ClJ6KO6mYZ0zZI0yBKSwUBESGMkANvfoiyQz3/zJWbODh1Aacv3VC2WVIZMarG61GgShgVHgbnWETY0X7jh9lANtdrSB9Obg+JqZkiDr91sam5u7NRvH077kUUzcaESDC1N1UXchq6zxnZ2J3Hda3P5sU6UhBiBX3iXDG0XBe7gVoYzGP645tKKRERHnHiNnanhziK5GRh8MVdyp92+noNHHpiY+Ma90ok72aMLMOhJzZiaqYYS1WhOPYvvH0x0PervLkR+P6AhU92soxpFRSkCeFx1h1t0gVdtASKkMJgHvML8kvh1fevB6oqjBNVwqiqT2cYeW4qVDlc24vUGwAAIABJREFUqpCu+M6BNy80GWYC2cpI5e0szc3jhbcvtqw2Oe4/pOyTlu6krhvm1fFsmUYWx0cGcHV8p5aY8DIjg5MffhpK5aPqXAcRNuaQh1qXL8m2eIijCAHbPX9xhpfTTu+SnsaxFx78Ts6PDIK4zke+uhqH37qIfZOz0mdLmORzJmae347CYL6t6uGyVcHht2rGXRger2V17+nI11Z3TIid+RTKTVqUljxnEaPySlQ4R75NuYs48KuXFpVnT5XDtdw2aci+42Z8yEKK0x/fbBG7vTNfUeYE2WU0Dr91UbmKzNdFaU9futGQnhBGnR2nkO3ojvV49tHVUgHepT0ZzC9UW7YlzoHMkykISybCS9RYjEVX5FclJ5DrXdJyrMTfQSVQnDg9f0DtGiIPWqtsyrYNq+LXokqYXK/R8DbHJYwsnlEHpy60va1S2aotIHesb1vc+c58JbT7Lgx0PGJ+AgTt5keHARlnEaOabITHRRYGTPqisKMK78RVzSZwywfz+x2BzAhWFQW4IbxwbqvobRtWeWq0OY0pEX49+tQmfHTjM5y5crPp8xnGGt0JZIRpYORMo0V/yM+1qhMmZFAvZlTfLQzmQ9k/EZoG0GSEjO5YnyoDjQFaWmi9RgZLerKhenbEMSiWytLFQrdza86Kfb8zjPlOa3Bj4tRljO5Yj6U9mcZzxsgAjsybjoEB6F2SDbUVVpYx7NoSX/qQCgprRsyzj65Wvh5lGDAsgvTpjAKVNzHDmHIcbh5IlWERNKTs9b2Xz17zXP26VaNe/Xnr9nWSVkVhRNDQw9Z1K5HPmbhdtrB8SQ/6eo1A16qqt6m9JN/N3HAL7YYRghTeP6dUy/7JWRRL5dSEbh7KmVrVmmWritlD23F1fGdqxk74J2ztM7HgsxvtnWqY5XMmjo0MYC7kHqUVznHiXJGqNbsdr7ZD7cpdRI3fvLOoUE3AbmXPqu/09RpKw8LNoMuZhtJIeMijWhWo5Rm64ZaD2G5+omq/3PbJNDL48bXbDWOlVLZw16ri2MiA72tV5ZGsdcrwxm2SCqPN0+CL72Lf5Kw0r9D+/6QR3ivPEA7QqEr2K4ZMdC+MIfZetVHxSV0aI4rrOw3VmmScxcCRwiZcOfo4ro7vxJWjj2u1IEoT7fbpDAMxAcsmXtWNJPNMHh8ZaCTYynDTkjv85Ea8tHuzUuD20BMbXWUvvFbBbp0NgnQ9sKMS5j385EalVlPZqmrpyumgulZKHp0yBF7J56IdjfAs+aUbm40XS2WMvn7eNfSfFIyhrY4cRDDS1oCiHa+u8G5Hde8mXZhHxhnhSbuGQVgUBvNKQ8KtQMGPZ9JLS84tFF0YzLvm4nl5dNw6G7Tbjstt3H7PY5CHVjvXis5+2rtHEPdxawDupU0YJZzXDGIjS0HXxYqRYZ4tlZIk7rxqJ1QQQHiSpsKFdhu66+BVcer2vpt4qSr/0L5dAC3VmvbfcnvPC9W4Ved3mZHxrSunQvUbGQbXZF4GaCXnelXmhkXONPCLuwtt5wKpKrXjgqGWZzc1U0y26s6lal2XpI8lEYwqgKE1KxuRpI3P/1moif3tYtfzTALynBGepKlwIayG7lGhMlxMI6MVzk4iB1F1fg89sTG0Yy37jV1b8phfcM9GFj3xvHDz5mVDCqF5hYF1EYZRkh4DcZ0WBvOJhhfbTUbPMpZ4+ClOuqmvaqXKm4R102SYAfHnVTtJreeMMfaHAL4F4Gec83+Q9HgWO3F2JvAaB9CeBylKVB6io0+1l2eoktkAEMq+u53fsI618ze2jr+npS5eLJUx+OK7DYV6GW5SHUL9o0/RYUIHexeIdqU17EK3fhXW+3oNlOYsrJBIm2RQ80Y4yWZYUzcGp4F96ImNbetetUM7x/LZR1fj9KUbi8ZzFnb1ZtKkPdczScOf8ZSebMbYPwTwGYA/0jHOhoaG+PT0dPQDIwgPZEKy7RpQW8ffk05AXiKw7RDFftjRFaZ1kjMNMFYLIYtxAd5itEaGBWo14zzGbuK+XjjbmTnbi/X1Gtj5yIOY/OB6y1iXL8nid7/d/F37+ZmbX5BOdjnTwPKlPZ7ix8+9dt5z8s+ZBu4ttBaKJIGoep+aKWL/ZLRNs1VkGJQ5fX7x01qom1Bpgbb7WTd6MgwLGicuyucrADDGznHOh2TvpdZzxjn/j4yxtUmPgyD8EoWXMe5WX1F76oDg/StlfVKPPrXJc6K0qhymkcFdq6o9CRoZhrn5BTw8drLFsAniQbNXu4rrRGYoTf6oVXblznyl6RzYO0FMnLqs9EKUyhYOPyn3OtoNPJk3zo4I6wJo+o4wlJcZmZY+whkA2SxTbjMI2QzDS880t/KS/XaU9PUa+PKDn2sRhm6HxWiYAf68gcuMDKxKFfNtXk86hhmg1meMg9R6zgCgbpy9Q54zYrETt+csjt+TeaDa8W5t27DKU8GdAdgz3K9syA7UVudVzqXGiszzFcSLlmG1ajC7909sU3Xs7fsqzoHu7zvHrfquKjRqb3zuht3YW2ZkcG+hGppnyY44Bn6Ovz29QMdLKCPLGK4cfRwAsO7AD7ouzEg0I7tvwsTNc9bRBQGMse8wxqYZY9M3bvhvu0MQnULchRBxeOpEkYBdtkQmeKzDJ6UyjhQ2Yeu6la6feyhn4vSlG0rDzDSyeGn3Znw0vhPLl/a0eHycOm9iH0zD36O0WpeSEFpNdiFlr2Nsf1+3SrVsVfDca+ebxJpl31X5nnpdetjaEQUte4b7UbaiMcyA+8dAd/+dkjcv7d4cSGPLXnFNhln3k6QYbUcbZ5zz3+ecD3HOh1atSs79SBBRE3fFbJzadvdsFZtBE4TFuF75ra/j+MhAk8EnEMasm/FTtmoNnQdffFfpvXJ+vzCYx/xCexO1fRLwOsb29/0Yy85uGn6+69co9+qG0S6iA4JOWFlcC/snZzHwwrsYfPFd7J+cxTKfBvXWdSubKq67qXKSUJNUUUBHG2cEsZiIU2YjLk9dGPpkznEVBvOYPbQdx0cGAovuuhmJsu+H4UURhoZXr9A5W19bv8ayHyPQjp/PHpy6EItXSTffr1S2mlqQCY+l3xy1v7hys6k/rkq3MOtoE2Ia2VjkSsSvplXUtVOJW2xdkFrjjDH2KoAfAljPGPsbxtg/SXpMBNEtTM0UsXX8PTw8drLRg9FOXJ66oKtSxuA5LpUxq2rRJf0dx98qAzUMJwoDGqKX4tjLxnBrzmp4wGT7kgFclfc/cTECjQxr+a4fo/zg1AXPvD9dcqbRMGrS4KXiAF45e61xr6j6Jr/0zGapZmDUe8ARfXXhYiNJDc00V2s+m/QYCKIb0a3EjEPbLmjFJufA1fGdwX9Yc6YUE56XnIjZk8Fcm9WCHLVE9f2Ts02/JQvfCQ/Y6I71rfvC6oUVioo2uwAt0KpjJ3tN9zrwCmf+yueW4O8+s7QkO2YPbW96be3YSa0xRAlH7diI43GksEkqLm2vohXn8xvrVoZa3Sljsei9xUGWscTE1oEUG2cEQUSDLJQoJvu4H0SjO9YrNaryORN/e/uudCJvx5MyceqytrSDriciLBkHsa92g9mtOEO2L1UOpaEoCwHLDAmnQaareedldP2XX8wD8BYELpWtltY5fjWu8i7ab+2g4+2VLYB+9vd3Qx2HDHFf5EyjSXKG8E+V80TFzVMb1iQIIhri1kxzozCYx57hfmX4UDUZt5PTpLufRpZphzRUeSk502iEuHKmAc1oKoD7BrNq26LSUxdVCNjeNF5WPSp7/7uTs/jSgZNYO3YS6w78oJGLpWs06xhMzio53XNuGlkcHxnAmbHHpC3InJ/dO9wvLSBRoZODJFsABZGI8UuFc0zNFHFvIXmR4E4nqVwzARlnBLHIiLMSU4cjhU04pkjeVyU3i/ysIKj2025W9PUamHh6s/bKWZa/xXBfMHfPcH8gza9PSmXPAgEdRD9P2f64eVJV71dxX/C3wjlePnsNB6cuKJPkg2A3onXPtdMAdeZOmkamYSBnGcOuLXkMrVmJ5Ut7Wt5XIdqJuY0pqQq/nGnUPXbxCfJ2K0kK0AIU1iSIRYeq/2eSzeNV+W2qsKcz98cPbv1P2+kbCtzvGmBvxVMslV1Fb914KGe21ZHAvh0VXp5UXUPj1fev48rRx/HRjc9Cya2yj1lXa2rbhlUt59Aeuj3w5oUmo3Lyg+uY/NH1Rmi4bFWbxGrtrbXs3Jqz8Nzr5xvbd4Z9cwH6uPYaGVhV3lY3BQplhser71/H0JqViYU2yXNGEIuMuDXT2qEwmFcaNUG9E2Hvv6h83V/v7dnXa0iNSb/YDWZReerm1BGhU7/VlirDLcMYpmaK2h5VEXZ005rTxTlmPwaiClWoUSY0fODND7F/ctbV2KlUOf73Nz/EwAvvYt/kbFPY97O7C64Vs05ypoG//D/+EUa+ujrRytTlS9rz0HYTTl3AuCHPGUEsQuKoxAyLvKKis50wbFj7L0v8DoO8Iul+hSLR217d6LdhvcyTCNyfnHZtyePEuaKnHp3dqLB7q1TeJ7ft7NrSfH50q3rd8tL8GPO6YcE5qyotvhB9XBcq3NMwFz1Lp2aKOHGumGjngflK1bNYo5NZkmW++nImVSgFkHFGEETKSWMYVhCGiK4TtwpRlVPF/rpfw1N8VtZvsmxV8Or711HhvFEtaSqajMvyzVRGmmhSL6vArHCOE+eKTSGl0R3r8d3JWWV7KYGb1ymobEtQdAw8uxG+dfy90K8lv1gVjntWBaaRTXwsURCkYXpS8iRknBEEkTg1D9SHjQktw4DfeLQfRwqbmnKugmhvRYmuN8Y0ssgw4M6894QnOgDIcplUHo2S43U/3jPxWa/K2JphVsvHmv74ZpPR9uyjq6V6XwJn3peY+FW/WbYqOPzWxaYm6jq+LLeCBJWHMJthqMRQSWlHluMYJExvz20Mizmrir3D/Xjn/KeUw4bkBJDJOCMIIlGmZootXpEqR0NpXhhoaTDGnKi8MTnTwPKlPS3irjLjwInoAPD69DX8xZWbTYUFbuMQ6IoMyz7rhWigXuU8kJHsx9NYKlsN48DLC6VrIE5/fLOlOCMD4PO9Bkr1tk5hYGQZHljaIzWmVeKmfjx7rP75bRtWaYWc/XLiXNGzKCIsojAwwySpMDMVBBAEkSgTpy4rvSJRN9BuF1UP0sNPbmxpHSUKEXQoWxWcsRlmbjhDvCppjH2Tsy2tuoKEZSucS/XQdIhKYqLKOU5fuuE5ltOXbrQcU6vK0bukBx+N7wyvLyUHdj7yYEtrLSPD8NJuuUSLrmRKPmc2rqsjhU04+tQmmD6buHthz7WaPbQ90t6gaTbMgOQ8Z2ScEQSRKG4TdpLJ0Tr4rfx0024LgswL43Y8nQaV22d1BHOFJ03HQJuaKSIT0USnayyq9rdYKjd6lbarKQfUDL53zn8qba0lcPa3BdB0LelW3hYG87gbga6Z/Vh5ifnaMTII3VhMEvKcEQSxKHGrukxDw2svVA3WVYRlAADyFjNeVax2gVmvY68jB1HhHPsnZxtdAmSI8KnORMcQXNLBvm8y3PZXhH6dxvbxkQEcHxnwfc5KZatFpsOqcEycuqzsygCgcS3NHtqOiadbm6iL3D27YReF+cCBhqdVLEJ07kerGl47szjImYarMRml19ANyjkjCCJRtm1Y1cgvcxKm4nxaELlPqn0G9PNwhLFhLwBYpuG1EF4RVZI8UPP+2HPnMi69LTmAV85eU4p2eoVPRdVm3md+ntu+yXC71oRh52ZgtyMEbB9fO/1to5JvkSHLVwx6XtKISEEoDOax7sBJyIo57ya0r2ScEQSRKKcv3ZC+bhoZ1wTvTkXoWakQye1eid4Mtclz4IV3cWd+oUnl3gth1IkJd19dQNfJ7bLVpJ/mNjFz1OQ47NsVeOWaiUpQZ4GBveJ024ZVWhWEbt4x1bWmM06RN/jw2Ektw1nIhbS+zpQGlf11VWHHMiMTq3FkNxrD6FaRJsRCZmqmKDXMgOS8gGScEQSRKKoJMYo8mjTg5kWySywMrVnZJC/iRMwlfivpnHlLhcG8crK1GzpuemgCIVxr/7zYjtdk7vQcOSVUTl+6odR5U+2bEy8j0c2wE95J3RCiSp3DK7S77sAPmnTl7JStSiJeK/txs8uijL5+PpaG7lFxa85SLkyShnLOCIJIlLQ1Yo8aNwOhtZgg3Jw7VcGCqupUlnz+0u7NrqOS5X3p5tk5m50787LclOt12nC5XVMMrc2uRW7X2rGT2F9v0RQU3TNp15VLC7LjVhjMY+KZ+zlxnZAf2kmQcUYQRKLoGgbdgspAyNuanAPhdh8wjSyOjwwo86n8VJ0WBvPYM9zv+nvCyLL3HV3ak/GsAHU2O/ez/3PzC8r3xDhEU3oZHDV9L1HtaTcOxftByefM1EtGuOE0WgX2YpiXdm+OeVTxkUR/TTLOCIJIlE5qxB4GKi+S6AwgaFcTbPmSrK/jaZ9oR3esx+G3LmLt2EmsHTuJwRffbRrbkcIm1yq2h3Jmi+erVLaUoT4geLNzwa05C6NvtMp6yIwslYFm9/qFaRxHpe8WF165ekDt+tGRX+k1Mq5exL5eA3uH+5EzjabX3BDSI70RSXi4VQBHBeWcEQSROGntABAF96vemvPJRGcA8Zl2e0HOV6o4NjLg+7jKcolEbs6+ydlGRaWzZZSdufkFvPD2RW3jRqbXFmT/hVSFlwfSzYslDCm/BpVpZHDXqkq3/VDORGluXqt9lxNZ7lnc6B4LnfQzDuAb61bizJWb0vfvWlUMrVnZUgwkPJ9OnL1oRW5gmAULSRjX5DkjCIJIAFnBQ9mqYP9rsw1BVC8PA4PaCyQMFb9MnLrsmuQtqgZzLt6MW3OWa36YHdPISlXzZR5Gnawm50Tqd5IW+l5u+ydjmZHFnuH+ljGaRhbbNqzC/IL/Ahcjyzxz/OJAN/9TR2C5bFWVhlntfblW3bYNq6THVnhbReh63+Qs/vb2Xa3x6pJE/isZZwRBEDHjVvXHOTD6Rk2SQpXbtXe4HzPPb8dH4zu1vEB+0DFmylYFn90N1m/RNDKN5PEsY9i1pdVrKrwfZavS+Gw+Z2LPcL9nYYFzIg2SqF4slfHZ3QUtEV5Bac7CkcImHBsZaAnRn750I1hVY/0rURsHe12Oq07+pz2nLwycOYtrx0629ERlQOPacYauw/Q0JpX/SsYZQRBEzHgZTcLrNbRmZYuBYGQZhtas1PqdIJO6rjETVOmkbFWbKhLtSfhAa46YXQNN9JK05yM5cSavB52orSpHT4ZpHw+7dpyzY4SOkSz7Hatauw50q11zpuF6bFS8+v51aQha1aTdjvN8hcEK0/AsyOC4nwsXZn6gHZ39jwoyzgiCIGJGx2gq1pXkVS2ABG7J0kFW/HHnNznDWG6N29eOncRzr53HtzY/qNxvZ/J6O71M7YakHVnLTFVF48GpC1qVmqrj/kmprNU+Sajdzx7ajuMjA75CoarflrUHcxKFYcSY3naLpXKoHjsnSeb6kXFGEAQRM7qeENWk42xKLQu/7R3uD7TiD7Mxuy72/fGaaCuc4+Wz15Q5bU4vVZi9TIGaMezMLXPKcAgOTl1wbdNlR2V32T1yL+3eLD3Xy5dkmzw80x/fDEW6I8OYp4xEFMnypTlLe7tRdyo48OaFRKQ0qFqTIAgiZsQkeviti64K/4zVctCcyJT77a2OnG2Q/ODWbzMoXr1C7T1CdfuKqlhhGtg6/l7TsTj61KbG8XHrEarDrTkLpy/daBmjrDfmq+9f194u5zXvl/24C4+cVwVi2arghbcvYt/kbNvHz46q44OddquKZXCko0oV0O95GjaMp2Dnw2BoaIhPT08nPQyCIAhfTM0UXVvIOCdse4unKMcUlhwBQ62w4dX3rysn2+N1yQ+/ISrnsTEyDGBoCgU7j5dXj1Cg5h3TrTa1wwB8NL6z8ffasZO+vm8aGWm7rmyGoZJwm6S8wuiXHU8jUzOs3IacZQzDX+rDX1wJx8sXJc7zGtp2GTvHOR+SvUdhTYIgiATxMrKSEOgVSe1ueUtb161sjEsVkssyhmMjAzhS2IRnH12t3I7YHz8hMpGsbT82DyzracnRc+a02UWPVfQu6QmUWJ9hDA+PncTW8fcwNVP0XSmq6qOatGEG1MKHo6+fx+CL72Lt2EmsO/ADrB07iYlTl7FrS77pPEw8s1nq8RUI+ZRXfuvrjepWFVnGOkZKJEworEkQBJEwKk9NX6+RqECvW8jqg6u3MPF0TZ/sYYWHyJ5QLkRFhQctyxiefXR1k9ionxDZs4+ubjk2qnE4jT7xvYfHTkq9Np+Uyjg2MuC7KbbwDAotuOEv9blqeiWN3xCoVeWN69S+ryfOFVsWDaqQPWPNPWTFuVB5Gauc46PxnZEm/rtBUhoEQRCLFFlSv5FlOPTExoRGVMNNCNdeNarbvP5IYROuHH0cV8d34srRx1tU4HWS9zOsVuzg/K6fcei8XhjMe7YNcqNsVXD152XsHe5v0nX71V9eHnibYZEzDRhZFlo4sWxVcPiti02vqZyGOdOQhkZV15k4R2EXdrghvHVJtpIj44wgCCJhCoN5TDy9uTk09HSran4S43KbwIUnI6zm9c4+qznTaHRCyOdMHB8ZwE+P7pQaZkHG4fX5Q09sDNSlQPBJqdxikM7NBxSIU9BrZHyJ5QLA39+1WsK/QDDBXkGpbDVVNarae8ledxNlFhIlzmsjSiqc49jIQEOnLgkorEkQBJEC0tpfNGcayopSMZmHWTHaznHwOw6vz8ve37ZhFU6cK7ZUVap6ajppV3oiy1hLu6upmaJn5a8dVQpblXPk26i+tFc1qkLUD+XMRsGJOKZuv3fiXBFDa1Y2rguxfb/FFn7ZNzmLF96+iENPbEzkviTjjCAIglDi5kyxV1+mxbj0Ow6vz8veH1qz0tNgU3ns2pGeyGYYXnqm1aMq/h5947zUI6bLQzmzLePR/l2ZJIvoM2p/vVgqu+a+iZBpEtfWrTnLU0okKiisSRAEQShRhacAfyG+bsLZokm0ldKpqm0nd8qtalPWTcIPwphspzLRqb8nOyanL91okTHxGrUzZAq4d8YIE1Uj9qghzxlBEAShxM3Tk7zAQ3rQ9dg5Q6V+RXFVgqjthkvtxqTfKlWgZqg7PYWyY7I/wLaB1v0+9MTGQOMMQhRdELxIteeMMfZNxthlxthPGGNjSY+HIAhisZGEjEC3Y/e8vbR7s+8iAxntanHZ8+x6DX+mgRAa1jFOg45TJoey19FGKyqS0DlLrXHGGMsC+JcA/hGALwN4ljH25WRHRRAEsbgoDOaxfIk8DBdXaKmbKQzmW3p1uqEyFNoJlzpFYP/5U4/Uui3YyABNRpt4O58zG0LDOgQdp2y/jxQ2eYrY6uB27JPSOUtzWPNrAH7COf8pADDG/gTArwH4y0RHRRAEscj43W9vakk2T4MOW7dwpLCppchg7RfMltZGboaCM1y6wjRwZ36h+Zwp2lvJwpH2bbXbr9Vt2zphXa/9LgzmPdtyMQDHRgakRQq7tuQx+cF1WI6cvr5eI7FqzdT21mSMPQ3gm5zz/6n+9z8G8Cjn/J/aPvMdAN8BgP7+/i0ff/xxImMlCILodpzyB2FN1oSado+57PtANEZXUDyNKgYc2z2gNcbatj6UtsESwsWqY5rE9e3WW7OjjTM71PicIAiCIDoPlU6bkWGYkEiHeHFw6oJrm7C04GacpTmsWQRg75T7xfprBEEQBEF0CfbQZBjeqyOFTak0xvyQZuPsRwB+lTH2MGpG2a8D+I1kh0QQBEEQRBSkRcg4DaTWOOOcLzDG/imAUwCyAP6Qc37R42sEQRAEQRAdTWqNMwDgnP8AwA+SHgdBEARBEERcpFbnjCAIgiAIYjFCxhlBEARBEESKSK2Uhl8YYzcAxCF09ksA/i6G3yH0oXOSTui8pA86J+mEzkv6iOOcrOGcr5K90TXGWVwwxqZVuiREMtA5SSd0XtIHnZN0QuclfSR9TiisSRAEQRAEkSLIOCMIgiAIgkgRZJz55/eTHgDRAp2TdELnJX3QOUkndF7SR6LnhHLOCIIgCIIgUgR5zgiCIAiCIFIEGWeaMMa+yRi7zBj7CWNsLOnxdDuMsT9kjP2MMfafba+tZIz9OWPsr+v/76u/zhhj/3f93HzIGPuK7Tu/Wf/8XzPGfjOJfekWGGOrGWOnGWN/yRi7yBj77frrdF4SgjG2jDH2AWPsfP2cvFB//WHG2Pv1Yz/JGFtSf31p/e+f1N9fa9vWgfrrlxljO5LZo+6CMZZljM0wxt6p/03nJUEYY1cZYxcYY7OMsen6a+l8fnHO6T+P/1Dr7XkFwJcALAFwHsCXkx5XN/8H4B8C+AqA/2x77f8EMFb/9xiA36v/+3EA/w4AAzAM4P366ysB/LT+/776v/uS3rdO/Q/AgwC+Uv/35wD8fwC+TOcl0XPCADxQ/7cB4P36sX4NwK/XX/9XAP7n+r//FwD/qv7vXwcwWf/3l+vPtaUAHq4/77JJ71+n/wfguwD+GMA79b/pvCR7Pq4C+CXHa6l8fpHnTI+vAfgJ5/ynnPN5AH8C4NcSHlNXwzn/jwBuOl7+NQD/pv7vfwOgYHv9j3iNswByjLEHAewA8Oec85uc81sA/hzAN6MffXfCOf+Uc/7j+r9/AeCvAORB5yUx6sf2s/qfRv0/DuAxAG/UX3eeE3Gu3gDw3zLGWP31P+Gc3+OcfwTgJ6g994iAMMa+CGAngH9d/5uBzksaSeXzi4wzPfIArtv+/pv6a0S8/Arn/NP6v/8WwK/U/606P3TeIqIedhlEzVND5yVB6qGzWQA/Q22iuAKgxDlfqH/Efnwbx77+/m0AXwCdkyg4DuCfAajW//4C6LwkDQfwLmPsHGPsO/XXUvn86gl7gwQRB5xzzhijUuMEYIw9AOAEgH2c87+vLfASe7wPAAAgAElEQVRr0HmJH855BcAAYywH4E8BbEh4SIsexti3APyMc36OMfbfJD0eosF/zTkvMsZ+GcCfM8Yu2d9M0/OLPGd6FAGstv39xfprRLz8l7pbGfX//6z+uur80HkLGcaYgZph9grn/M36y3ReUgDnvATgNICvoxaCEYtv+/FtHPv6+ysA/Bx0TsJmK4AnGWNXUUuDeQzAvwCdl0ThnBfr//8ZaguZryGlzy8yzvT4EYBfrVfaLEEtYfOthMe0GHkLgKiM+U0A/6/t9f++Xl0zDOB23U19CsB2xlhfvQJne/01IgD1HJg/APBXnPPv2d6i85IQjLFVdY8ZGGMmgP8OtVzA0wCern/MeU7EuXoawHu8luX8FoBfr1cNPgzgVwF8EM9edB+c8wOc8y9yztf+/+ydeXxU5dX4v89MVpKQBAJkgYR9J0ACbogoqKCi4t7Wti5Vu7m0VSy2aq27pdafvrV9a/tWa7VV3BAQQVTcV5KQhMWwSUIWQsi+LzPP7487E7LMJDOTmcwkOd/PJx+Ye597n/Pc9dxzznMOxvvifa311ch58RtKqQilVJT9/xjPnV0E6vPL37MnBsofxsyNfRjxHL/1tzyD/Q/4L1ACtGL49H+EEYPxHrAfeBcYYWurgKdt5yYXWNBhP9djBNEeAK7z97gG8h9wOkbMRg6w0/Z3vpwXv56TVCDLdk52Affalk/EeIkfAF4BQm3Lw2y/D9jWT+ywr9/azlUecJ6/xzZY/oAzOTFbU86L/87DRIyZr9nAbvt7PFCfX1IhQBAEQRAEIYAQt6YgCIIgCEIAIcqZIAiCIAhCACHKmSAIgiAIQgAhypkgCIIgCEIAIcqZIAiCIAhCACHKmSAIgwql1Ge2f8crpb7n5X3/xlFfgiAI3kRSaQiCMCixlc25Q2u90o1tgvSJ2oeO1tdprSO9IZ8gCIIzxHImCMKgQilVZ/vvo8BipdROpdQvbQXC1yqlvlZK5Silfmxrf6ZS6mOl1AZgj23Zeltx5N32AslKqUeBcNv+XuzYly2L+Fql1C6lVK5S6qoO+/5AKfWqUuobpdSLqmMxUkEQBAdI4XNBEAYra+hgObMpWdVa64VKqVDgU6XUO7a2acBsrfW3tt/Xa60rbCWRvlZKvaa1XqOUullrPc9BX5cC84C5QJxtm49s6+YDs4Bi4FOMuoufeH+4giAMFsRyJgjCUOFcjFp5O4EvMcq2TLGt+6qDYgZwq1IqG/gCo8jxFHrmdOC/WmuL1roU+BBY2GHfhVprK0bJq/FeGY0gCIMWsZwJgjBUUMAtWutORYptsWn1XX6fDZyqtW5QSn2AUfvQU5o7/N+CPHcFQegFsZwJgjBYqQWiOvzeCvxUKRUMoJSaqpSKcLBdNFBpU8ymA6d0WNdq374LHwNX2eLaRgFnYBSwFgRBcBv5ghMEYbCSA1hs7snngCcxXIqZtqD8MmCVg+22AD9RSu0F8jBcm3aeAXKUUpla66s7LH8DOBXIBjRwp9b6qE25EwRBcAtJpSEIgiAIghBAiFtTEARBEAQhgBDlTBAEQRAEIYAQ5UwQBEEQBCGAEOVMEARBEAQhgBDlTBAEQRAEIYAQ5UwQBEEQBCGAEOVMEARBEAQhgBDlTBAEQRAEIYAQ5UwQBEEQBCGAGDTlm+Li4vT48eP9LYYgCIIgCEKvZGRkHNdaj3K0btAoZ+PHj2fHjh3+FkMQBEEQBKFXlFL5ztaJW1MQBEEQBCGAEOVMEARBEAQhgBDlTBAEQRAEIYAYNDFnjmhtbaWwsJCmpiZ/i+JzwsLCGDt2LMHBwf4WRRAEQRCEPjColbPCwkKioqIYP348Sil/i+MztNaUl5dTWFjIhAkT/C2OIAgDlPVZRazdmkdxVSOJMeGsXj6NVfOT/C2WIPSZgXZtD2q3ZlNTEyNHjhzUihmAUoqRI0cOCQuhIAi+YX1WEXe9nktRVSMaKKpq5K7Xc1mfVeRv0QShTwzEa3tQK2fAoFfM7AyVcQqC4BvWbs2jsdXSaVljq4W1W/P8JJEgeIeBeG0PeuXM31RVVfGXv/zF7e3OP/98qqqqfCCRIAhCd4qrGt1aLggDhYF4bYty5mOcKWdtbW09brd582ZiYmJ8JZYgCEInEmPC3VouCAOFgXhti3LWgfVZRSx69H0mrHmLRY++7xV/9Jo1azh48CDz5s1j4cKFLF68mIsuuoiZM2cCsGrVKtLT05k1axbPPPNM+3bjx4/n+PHjHD58mBkzZnDjjTcya9Yszj33XBobA1fbFwRhYLJ6+TTCgju/EsKCTaxePs1PEgmCd1i9fBrB5s6hP+HB5oC+tkU5s+GrgMFHH32USZMmsXPnTtauXUtmZiZPPvkk+/btA+Cf//wnGRkZ7Nixg6eeeory8vJu+9i/fz8///nP2b17NzExMbz22mt9kkkQBKErq+Yn8ZMlkzotu37RhICe0SYIrrBqfhILxsdiV8/MSvHwJbMD+toe1Kk0OvL7jbvZU1zjdH1WQRUtFmunZY2tFu58NYf/flXgcJuZicP53YWz3JLjpJNO6pTu4qmnnuKNN94A4MiRI+zfv5+RI0d22mbChAnMmzcPgPT0dA4fPuxWn4IgCK4QGWq8Ej5cfSbnP/kxlQ2tfpZIELxDZX0ri6eO4uK5idz+SjYpcRH+FqlHxHJmo6ti1ttyT4mIOHFBfPDBB7z77rt8/vnnZGdnM3/+fIfpMEJDQ9v/bzabe41XEwRB8ITMgkqSYsJJGRnB2TPHsGVXCa1efgYKQn9T09RKXmkt6cmxnDNrDCFmE5uyS/wtVo8MGctZbxauRY++T5GDmRtJMeG8/ONTPe43KiqK2tpah+uqq6uJjY1l2LBhfPPNN3zxxRce9yMIgtAXtNZk5Fdy8gTDcn/BnATe3FnMZwfLWTJ1lJ+lEwTP2VlQhdaQnhLL8LBgzpg6is25Jdx9wQxMpsBMQyWWMxurl08jPNjcaZk3AgZHjhzJokWLmD17NqtXr+60bsWKFbS1tTFjxgzWrFnDKaec0qe+BEEQPKW4uonSmmbSU2IBWDJtFFGhQbyVU+xnyQShb2QWVGJSMHdcNAAXzk3gaE0TGQWVfpbMOUPGctYb9sBAX5R3+M9//uNweWhoKG+//bbDdfa4sri4OHbt2tW+/I477uizPIIgCF3JyDdeVHblLDTIzDmzxrBl11EeXDWHkCD5lhcGJhn5lUyLH05UmFF7etmMMYQGmdiUXczC8SP8LJ1jRDnrwKr5SQE9e0MQBMFXZOZXEh5sZnp8VPuyC1MTeT2ziE8OlLF0+hg/SicInmGxanYWVHHRvMT2ZZGhQSydPprNu45y74WzMAega1M+hQRBEAQy8iuZOy6aIPOJ18KiyXFEhwcHfPC0IDhj/7Faapvb2i3Cdi5ITaCstpkvv+2evioQ8KlyppRaoZTKU0odUEqtcbD+DKVUplKqTSl1eZd1f1BK7VZK7VVKPaWkeKQgCIJPaGhpY09JTbcXWEiQieWzxrBtTylNXWoTCsJAoKu73s7S6aMJDzbzVk5gfnj4TDlTSpmBp4HzgJnAd5VSM7s0KwCuBf7TZdvTgEVAKjAbWAgs8ZWsgiAMfHxR4WOokFNYjcWqu73AAFamJlLb3MZH+8r8IFl35Dw7R45NdzLyK4mLDCF5xLBOy4eFBLFsxmi27DpKWwCmi/Gl5ewk4IDW+pDWugV4Cbi4YwOt9WGtdQ7Q9choIAwIAUKBYKDUh7IKgjCA8VWFj6GC3bowf1x35ezUSSOJHRbMpgCwMMh5do4cG8dkFVSRlhyLI+fbytQEyutb+PxQ4Lk2famcJQFHOvwutC3rFa3158B2oMT2t1VrvdfrEgqCMChYuzWPxi5ut8ZWC2u35vlJooFFZn4lE0dFEBsR0m1dsNnEitkJvLvX/65NOc/OkWPTnfK6Zr49Xk+aA4swwJnTRhMRYg7ImMqAnBCglJoMzADGYih0S5VSix20u0kptUMptaOsLDBM7n0lMjLS3yIIwoCj2EEC6Z6WCyfQWpNZUEl6suMXGMCFqQk0tFjY/s2xfpSsO3KenSPHpjuZBVVA93gzO2HBZs6ZOYYtu48GXCUMXypnRcC4Dr/H2pa5wiXAF1rrOq11HfA20C1Nv9b6Ga31Aq31glGjJIO1IAxVEmPC3VounODb4/VUNrQ6fYEBnDRhBHGRIX53bcp5do4cm+5k5FcSbFbMSYp22mZlaiLVja18cuB4P0rWO75Uzr4GpiilJiilQoDvABtc3LYAWKKUClJKBWNMBvC9WzNnHTwxG+6LMf7NWdfnXa5Zs4ann366/fd9993Hgw8+yLJly0hLS2POnDm8+eabfe5HEIYyq5dPI6hLriKTgjvOneoniQYOzmazdSTIbOK82Qm8900p9c3+q+27evk0uqak8kYll8GAHJvuZOZXMisxmrAu1X86snhqHFFhQQHn2vSZcqa1bgNuBrZiKFbrtNa7lVL3K6UuAlBKLVRKFQJXAH9TSu22bf4qcBDIBbKBbK31Rl/JChiK2MZbofoIoI1/N97aZwXtqquuYt26E/tYt24d11xzDW+88QaZmZls376d22+/Ha11HwcgCEOXVfOTSIgOI9isUEB0eBBWDW1Wua96I7OgkuFhQUwa1XNIxcrUBJparbzvR9fmmdNGobWRRBTAbFI8fMlsSR4OnDV9NArayxAODwvikUvnDNlj09JmJbuwqsePDjAqYZw7M5539hyluS1w0sX4tEKA1nozsLnLsns7/P9rDHdn1+0swI+9Kszba+BorvP1hV+DpbnzstZGePNmyPiX423i58B5j/bY7fz58zl27BjFxcWUlZURGxtLfHw8v/zlL/noo48wmUwUFRVRWlpKfHy8m4MSBAGgoLyBI5WN3HXedH68ZBJWq+a7f/+C+zfuYdHkuCHt2umNjPxK0lJiey0AvWD8CEZHhbIpp5gL5yb22NZXvLO7FA3858aT2V1cw12v5zK1Q0WDocy7e0qxaHjxxpO5fV02k0dHDlnFDGBvSQ3NbdZelTOAlXMTeC2zkI/3HefsmYFRCSMgJwT4ha6KWW/L3eCKK67g1Vdf5eWXX+aqq67ixRdfpKysjIyMDHbu3MmYMWNoamrqcz+CMFTZlGsU574gNQEAk0mx9vK5WLTmzldzxDLthOrGVvaV1pHWw2QAO2aT4vw5CWzPK6O2qbUfpOvOxpxikkcMY05SNCtmxWM2Kb/HwQUKm3KKSYoJZ/64GNKSY8nMrxzS173dXe/Ktb1okq0SRk6xr8VymaFTW7MXCxdPzLa5NLsQPQ6ue6tPXV911VXceOONHD9+nA8//JB169YxevRogoOD2b59O/n5+X3avyAMdTZllzBvXAxjY08kmkweOYzfnD+Du9fv4oUvC/jBKSl+lDAw2Xmk59lsXblwbgLPfXaY9/Ye63erTHldM58dLOfHZ0xEKUVsRAiLJsexKaeYO5dPc5jHaqhQ1dDCx/uPc/3pE1BKkZYSw2uZheSXNzA+LsLf4vmFjIJKkmLCiY8O67VtSJCJFbPi2ZRTTFOrpccYtf5CLGd2lt0LwV1cH8HhxvI+MmvWLGpra0lKSiIhIYGrr76aHTt2MGfOHJ5//nmmT5/e5z4EYahyqKyOPSU1rLRZzTpy9cnJLJ4SxyOb95JfXu8H6QKbjPxKTArmjotxqf38cbEkRof5xcKwdXcpFqtut46CEQd3pKKR3KLqfpcnkHhndyltVt1+D9iVbbv1aCiSaXPXu8rKuQnUt1j4IC8w0nKJcmYn9Uq48CnDUoYy/r3wKWO5F8jNzWX79u0AxMXF8fnnn5Obm8uzzz7L3r17GT9+PAB1dXVe6U8Qhgr22ngXOFDOlFI8dlkqZqVY/UoOVpkg0InM/Eqmxw9vD7DvDZPNtfnhvjKqG/vXtbkpp5iJcRHMTBjevmz5zHiCzeLa7OjuBZgyOoqo0CAyC4amclZc1UhJdRPpya59dACcOnEkIyJCAsa1KcpZR1KvhF/ugvuqjH+9pJgJguA7NuWUsHB8LAnRzvM8/e6iWXx1uIJ/fvptP0sXuFismqyCSpddmnZWzk2k1aJ5Z/dRH0nWnbLaZr44VM7K1IRO7svoYcGcMWUUb+WUDNn4Kru7t+OxMZsU85Jjhqzl7ER6mBEubxNkNrFidjzv7T1GQ4v/0sXYEeVMEIQBy/7SWvJKa7lgTnerWUcuS0vi7Bmj+cPWPA4cE+s0QN7RWupbLKSluG5dAJg7NpqxseG8ldt/1qq3d5Vg1YZi2JULUhMoqmpszwY/1Niy+2g3dy8Yrs280lq/Td7wJ5kFlYQFm5ie4N5M3pWpCTS2WvyaLsaOKGeCIAxYNuWUoBSc34typpTi4UvnMCzEzO2vZNMWYKVa/IHd5ZWe7Lp1AYxjeUFqAp/sP05lfYsvROvGppwSpoyOZOqY7i/bc2aOISTI1O7eHmq8lVPSzd0LxixFrU9M+hhKZOZXMndsDMFm91SckyeMJC4yNCCupUE/W1NrPSRm8QxVk74QeKzPKmLt1jyKqxpJjAln9fJpDmf2udrOGVprNuUUc/KEEYwe3vuMrNFRYTxw8Wxu+W8WaQ9so7apzWvy9XUs/iAzv5K4yFDGjXA/B1xkaBBtVs38B7aR5OPxltY08fXhCn6xzHG1h6iwYM6cOorNuSXcfcGMXvO1DSbs7t6bz5rc7T03LzkGpQwX3+Ipvi1vGEjXf2OLhd3FNdx0xkS3tzWbFNPjI3l711EmrHnLr2MZ1JazsLAwysvLB73iorWmvLycsLDeX1CC4EvWZxVx1+u5FFU1ooGiqkbuej2X9VlFHrXriW+O1nKwrJ6Vqa4nRLVYNWYFNU1tXpPPG2PxBxkFlaSnxLj98bo+q4i/bD/Q/tvX4zXiyYzZdM64IDWBozVN7BhiMVZ2d+8FDu6B4WHBTBsT5XN3b6Bd/zmFVbRZtduxlGCM5avDxjXk77EMasvZ2LFjKSwspKwsMKbG+pKwsDDGju1WbEEQ+pW1W/NobO1cAqWx1cKa13I6xSh9vK+MpjZrt3Zrt+a5/JX6Vk4JJgUrZrteWWPt1jwsXb7V3JGvaztvjaW/KattJr+8gatPTnZ7W+Mc999438otYUbC8B7LS509YwxhwSbeyinmpAnuuWkHMnZ37zQnVRLSUmLZuLMYq1X7zKLo7J731/WfYXPXz3ch+WxX1m7NoyVA7uVBrZwFBwczYcIEf4shCEOG4qpGh8ub2qwUVjZ2+u3O9l2xuzRPmxRHXGRov8nXtV1PbV0diz+wx5u5kj29K87G5YvxFlc1kpFf2Wvx7ojQIJZOH83mXUe598JZmIeAa7M3dy9AenIs//mygP3H6pwqcH2lP68HV8jMr2JiXAQjIkLc3jaQxjKolTNBEPqXuKhQymq7lzxLignn7dsWt/9e9Oj7FDl44LlaA3N3cQ2Hyxv4yZJJbsmXGBPusF9X5evarqe2gVzPM7OgkmCzYrYtL5Y7ODuGvhivPTDbUYLhrqxMTWRz7lG+/Lac0ybFeV2WQMPu7nWU389OWodktL5SzvrzeugNrTWZBZUsnT7ao+0DaSyDOuZMEIT+o6XNSpADg0V4sLmb5WP18mmEdymREmxWvVpI7GzMKSbIpFg+y3WXprN+XZXPUTtnbUODTC6PxR9k5lcyOynaozI1jsYL8OMzvO+l2JRTzJykaFJG9l6C6KxpoxkWYh4yCWk35RQzPT6KyaOdu3vHjxzGiIgQn+Y7u2JB93AaZ/eKrzlc3kBFfYtH8Wbg3n3va0Q5EwTBKzz13n5Kapq54fQJJMWEozAsTY9cOqdbvMaq+Uk8cumc9nahQSa01i6VEdJa81ZOCYsmxxHrpuuia7+uyuesnaO2JgUjI0J6Te/hL1rarGQXVpPugUsTuo93dFQoZgUf7jvu1clXRyoayC6s7tEy1JHwEDPLZoxhy66jgz5Vij2v24UO8r51RClFWnIsWT6qFNDUamFDdjEx4cEk2GpYmhQ8fMls/8SbtSef9c613dN972vErSkIQp/ZeaSKv354kMvTx3L3ypncvXJmr9usmp/U/tArrWninD99yO3rdvLKT07rMWYou7CawspGbls2xSNZO/brjXZd227bU8qNz+/gz+/v51fnBp71bHdxNS1tVo9fYND92Pzzk2+5f9MeXsko5MoF47whZrsFrLcEwx1ZmZrAxuxiPj9U7vP0Ef5ksxvHJj0llnf3llJR3+JRHFZPrN2ax6Gyel684WQWTY7j31/kc8/6XSwY759JGRn5lUSFBTG5h8kjveHOfe9LxHImCEKfaGq1cPu6nYyOCuXeC3tXyhwxZngY9188m8yCKv7x8aEe227KLibEbOJcN12a/cU5M8dwaVoST39wkJzCwEsAarcuuFMUujeuPW08J08YwQMb9ziM2fGETTnFzBsXw7gRw1zeZsnUUUSGBrEpe3C7NjflFDM7aTjj43p399qV8Ewvuza/PFTOPz/9lh+cksKiyUaMn90a66+anpn5lcxPjh0Uue5EORMEoU88/k4eB8vqeeyyVIaHBXu8n4vnJbJ81hgef2cf+0prHbaxWjWbc0s4Y2oc0eGe9+VrfnfhLEZFhvKrddk0dUkz4G+yCqpIiglnjAuJe13FZFL88Yq5WLTm16/m9Nm9+e3xenYX17g0EaAjYcFmzpk5hi27j9I6SF2bdnevq/n9UsdGE2RS7SkmvEF9cxt3vJpN8ohhrDlvevvyafFRRISY/VLTs6aplX3Haj121wcaopwJguAxX31bwT8++ZarT07mjKl9cyMppXjokjlEhgVx+7pshy/XrCOVFFc3uRyH5C+iw4N57PJUDhyr44lt+/wtTjtaa3bkV/TJpemMcSOG8dsLZvDJgeO88GVBn/b1Vk4x0HtZLkesTE2gurGVTw4c75MMgYq77t6wYDOzEod7VWF6ePNeCisb+eMVc4kIPREd5c+C6zsLqtDa83izQEOUM0EQPKKhpY3Vr2YzNjac35w/wyv7jIsM5aFVs8ktquYv2w92W78xu4SQIBNnzxjjlf58yZKpo/jeyck88/Ehdhyu8Lc4ABRXN1Fa0+yzF9j3Tkpm8ZQ4Hn5rL/nl9R7vZ1NOCQtSYj1KYXD6lDiiwgava9MTd29aSiw5hVVesSZ+vL+MF78s4IbTJ7DQQWxZenIse0tqqG9u63Nf7pCRX4lJwdxx7qeHCUREORMEwSMeffsbCioa+OPlnb+e+8p5cxK4eF4i//P+fnYVVbcvt9hcmmdNG0VUH9yn/clvzp9BUkw4d7ySTUNL/76sHNHX2Wy9oZTiD5enEmRWrH4lB4vVfffmgWO1fHO01m2Xpp3QIDPLZ8Xzzp6jNLcFlku5r3jq7k1PiaWp1crekpo+9V/T1Mqdr+YwaVQEtzuZ7JKWEotVQ3Y/F1zPLKhkWvzwAfNs6A1RzgRBcJtPDxzn+c/zue60CZw8caTX9//7i2YxIiKEO17Jbn/B7jhcwbHaZrdqafqbyNAg1l4+l8PlDfxhS56/xSEzv5LwYDPTfZSQFCAhOpzfXTiLrw5X8Oyn37q9/aacEpTyzKVpZ2VqArVNbXy8b3C5Nj1196Z3SEbbF+7fuIdjtc08fuU8pzny7GWT+tO1abFqsgqqSE/pPRXPQMGnyplSaoVSKk8pdUAptcbB+jOUUplKqTal1OVd1iUrpd5RSu1VSu1RSo33payCILiG/et54qgI7lzhm1QRMcNCePSyOXxztJYn390PGC/tsGCTx9m//cWpk0Zy3aLxPPfZYT7zcxxUZkElc8dFE2T27Xf5ZWlJnD1jDH/YmseBY3Uub2eU5SrhpPEjGN2HCQuLJscRMyyYTTZlZrDgqbs3ITqcxOiwPilM7+4p5dWMQn66ZBLzeshHGB0ezJTRkf06Y3NfaS11zW0elSMLVHyW50wpZQaeBs4BCoGvlVIbtNZ7OjQrAK4F7nCwi+eBh7TW25RSkcCgmnqzPquItVvzKK5qJDEmnNXLp/VbbhVX+/a2jN7u1xfjCHQZ/XVOOra1p0r4xdlTPMow7ypLp4/hygVj+csHB1m34wjH61oIDzaxbU9pQOQhcoc7l0/ng7wyfvZiJuEhZo5WN/X7dfPYlm8oqW4iMjSI9VlFPj2GSikevnQ25z7xEdc99xUWq6akqvcxP7x5L8dqmykLD+6TjMFmE9PHRPHmzmLe3FkckPeeu33bj010uGfnb35KrNvpNDrKpxQkDA/lVhfyC6anxPL2rqM+LbjeUcb7NuwG4LEt32BSasA9Hxzhy8+nk4ADWutDWusW4CXg4o4NtNaHtdY5dFG8lFIzgSCt9TZbuzqtdYMPZe1X1mcVcdfruRRVNaIxsj3f9Xou67OKAqZvb8vo7X59MY5Al9Ff56RrWzt/+/CQz6/ZtOQYFHC8rgWAxlZrv90r3iQ8xMxFcxOpamylpLrJL9dNSXUTAHXNbf1yDEdHhXHx3ESOVDRSXOXamI/Z6rJWN7b2+drOLKhCQ0Dee570feLYeHb+0pNjKa5uoqTatTx0XeWzaqhoaGVzbu8TLdJSYqlubOXQcdetpp5gl7GqsRWA0prmAfl8cITyZrmNTjs23JQrtNY32H7/ADhZa32zg7bPAZu01q/afq8CbgBagAnAu8AarbXT6M4FCxboHTt2eH0cvqCnosqfrlnql75Dg0ycOulE7NDnB8tpbuturPRUxr726+t2/uy7r+18fU580XdfZeyPe8XbBNo9IM8b/x7D/j422UequPjpT3n6e2kupaLpy713sKyOZY9/yGOXzeGqhckuy+guA/35oJTK0FovcLQuUMs3BQGLgfkYrs+XMdyf/9exkVLqJuAmgORk310A3qbYSQZtZ8v7o+/mNiuV9S2dfruzva/79XU7f/bd13a+Pie+6NtV/HmveJtAuwfkeePfY9oRYvIAACAASURBVNjfx2Zm4nDCgk1k5Fe6pJz15d6bGBdBzLBgMvIrfaqcDabnQ1d8qZwVAR2LrI21LXOFQmCn1voQgFJqPXAKXZQzrfUzwDNgWM76KnB/kRgT7lDb9ySnj7f6TooJ582bT2//7eyLxFMZ46PD2t0qnvTr63b+7Luv7Tw9J65eC77o21X8ea94m77ee4Fy3biDv543/XWs+3IMR0SEUN7lI8iXfQebTaSOjXG5UkBiTBhFVd2f2a70q5QiPTnW5zM2R0WFtrt7OzIQnw9d8WXM2dfAFKXUBKVUCPAdYIMb28Yopewpx5cCe3poP6BYvXwaoUGdD314sJnVy31fJHn18mndiko76nv18mmEdwn2DjGbPJZx/MjuCRNd7bc/2vmz7760Mym449yp3cbiCt8/pfsXbV+PjbfxV7++IJCum/583vhDxv44hgq4delkj+SrbmylzWqla6i8r49NWnIse4qrXSopdua07hU/3Ok3LSWWg2X1VDV0V0C9gcWqCQ/ursIM1OdDV3ymnGmt24Cbga3AXmCd1nq3Uup+pdRFAEqphUqpQuAK4G9Kqd22bS0YMzjfU0rlYtwHf/eVrP3NqvlJrOhQtDk82Mwjl87plxkmq+YnMSM+iiCTQmF8pTnqe9X8JB65dA5JMeEoIMikGBZiYukM99MYfJB3jM8PVbB02qj2/bnab3+182ffnraLDg/Cqp27PXqi1WJlU04JESFm4qPDvHZsvI2/+vUFgXLd9Ocx9JeMvj6GcZEhaCCnQ5Jkd3hg0x7qmi384pwp/Xps0lNiabVocnuRu6S6kQ3ZJUwYOYzEmN6fD46wp7XIKvBNMtq/f3yI/IpGvn9K8qB4PnTFZxMC+puBNCEA4Cf/zmBPSQ2pY6PJKqjq1+DF5U98xLgRw/jHNQ7jEB2SVVDJZX/9jMvSxrL2irkub1fd0Mq5/+9DhocFs/GW032aemGoYbVqrv7Hl+QUVrHlF2e4Vc7liW37ePK9/fztB+ks7/ChIAiCazz01h7+/vG3PH/9SW7Vld22p5Qbn9/BzWdN5o5+tvCU1zWT/uC7rDlvOj9ZMslhG6011zz7NV9/W8Hbty1mfFyER301tLQx5753+OmSSV4f577SWlY+9QlLp4/mr99PQynfpuvwFT1NCJAKAX5Aa01GQSVpyTGkJcdSVNXo8vRmb/RdUNFAigM3Y0/MT47lJ0sm8UpGIe/tLXV5u99v3M3xuhb+1ENGacEzTCajVA7Ana/mYHWxVE5uYTVPbz/AJfOTRDETBA+5/dxpTB4dya9fy6HalsqhNyrrW7jr9Vymx0e5lC/M24yMDGVCXESPsWAvfX2Ej/aVcdf50z1WzACGhQQxM8G7BdfBsPr/at1OIsOCePCS2QNWMesNUc78QGFlI2W1RvFhe1mNzPz+qUNWVtdMY6uFZDesLHZuO3sK0+OjWPN6brfZfI7Yuvsor2cV8fOzJjNn7OAoRhtojBsxjHtWzuTzQ+X8+4v8Xts3t1m4/ZWdjIwM4b4LZ/WDhIIwOAkLNvP4FXM5VtvM/RtdC4m+d8NuqhuNj9WQIP+8ftOSjWS0jrxmRyoaeHDTHhZNHsn3T07pc1/pKbHsPFJFmxcKrtv5y/aD7Cqq4eFLZhMXGeq1/QYaopz5AfuXRFpKbKfpzf1BQbmRyzfZTcsZGAWFH79yLpX1LfzOlpHZGRX1Lfz2jVxmJgzn5rM8C5oVXOOqheM4c9ooHnl7L98er++x7RPb9rOvtI7HLksletjgKBAsCP5i7rgYfnbmJF7LLGTbnp49Cm/llLAxu5jblk1hZuLwfpKwO2kpMZTXt1BQ0Tmvu9WqWf1qtq14/VyvZPZPS4mlsdXCN0dr+7wvgF1F1fzP+/u5eF4iK2Z7Xnt1ICDKmR/ILKgkIsTMtDFR7dOb+6sOWb5NOUvxwHIGMCsxmluXTWFDdrHTTNFaa+5en0t1Yyt/umqu374QhwpKKR69NJUQs4k7XsnG4sS9mZFfyTMfHeS7J43jzGkDqz6lIAQqtyydwoyE4dzVg0ehrLaZu9fnMndstNNYr/7CWRH05z8/zBeHKrhn5QySvJSKIi3ZqMHpjfdbc5uF29dlMyIihN9fNPit/vLW9AMZ+ZXMS45pLz6cnhLLbhenN/eVgooGlIKkWM9vvp+eOYk5SdHcvX4Xx+u655jZmFPC5tyj/OLsqUyP998X4lAiPjqM3188i4z8Sv7x8aFu6xtbLNzxSjYJ0eH89oKZfpBQEAYnIUEmHr9iLtWNLdzz5q5u67XW/OaNXOpbLDx+5VyfF53vjSmjo4gKDeqknB0qq+PRLd9w1rRRXLlgXA9bu0dSTDhjhod6xTP05Lv7ySut5dHL5hAzLMQL0gU2opz1M/XNbewtqSHdNs0YjBgAV6Y3e4OCigYSo8MJDfI8OD/YbOLxK+dS19zGb9/I7RS7cKy2iXvf3MW8cTH8+IyJ3hBZcJFV85JYPmsMj2/bx/7Szm6Ex7Z8w7fH61l7RSqRoYFaGEQQBiYzE4dz27IpbMopYVNOcad1b2QVsW1PKavPncbk0VF+kvAEZpNiXnJMu8JksWrueCWb0CAzj16W6tUAe6UU6Sl9T0abWVDJ/354kCsXjGXp9DFeki6wEeWsn8k+UoVVw/yUjsqZYfrtj7iz/PJ6jyYDdGXqmChuP2cqW3eX8uZO42Gkteau13JpDJAvxKGGUoqHLplDZGgQt7+STastCPezg8d57rPDXHvaeE6bFOdnKQVhcPKTJZOYOzaae9bv4litkVm/pLqR323YzcLxsVx/+gQ/S3iC9JRY8kprqW1q5e8fHyKzoIr7L57FmOFhXu8rLTmWwspGSmu6VxtwhcYWC3esM6z+96wcOlZ/+YTuZ9onA4w7oZy5Mr3ZWxRUNLDMS18eNyyeyDt7SlnzWjaPvv0NR20336p5iUwaFemVPgT3iIsM5cFVs/nZi5mkPbCNuqY2TCZFXEQwd64Y+FmzBSFQCbJ5FJY/8RFnPLad5jYrIUEmrFbNH6+Y260yiz9parWgNcy57x0AUpOGc9HcRJ/0dSIjQSXnzXE9iH99VhFrt+a1l6762ZmTiAobOpOYxLTRz2QWVDJldGS3mXJpybFkFTie3uwt6pvbOF7X4tFMTUeYTYoVs+JpatPtihkYKTTWZ7laRlXwNi1tVsxKUdvUhsZwW9Q2W3hnt+v56QRBcJ9dRTWYTIqmNisaW/UO5bss+Z6wPquI5z473GnZvmN17R4QbzMrMZqQIJNbkwLWZxVx1+u5nWqKPvvp4SH1XhHlrB+xWjWZBVXtXxIdSU+J5Xhd9+nN3sS+b3cT0PZE15scoLHVytqteV7rQ3CPtVvzsHRR8pvb5JwIgq9ZuzWPVkvne6/VogPq3lu7NY+m1s55x5p8+MwOCTKRmhTtlmdo7dY8GrtMkGtstQTUcfQ1opz1I4eO11Hd2EqaE+UMfBt3Zk+j4Y2YMzvFVY4rGzhbLvgeOSeC4B8Gwr3nDxnTU2LZVVTjckaCgXAcfY0oZ/1Ie7xZcnflbMroyG7Tm73NEbvlbITnJTm6kugkH46z5YLvkXMiCP5hINx7/pAxLSWWFouV3cWuZSQYEeE4VUYgHUdfI8pZP5KRX0nMsGAmOqhXZuoyvdkX5FfUEx0e7NXM8KuXTyO8S83M8GAzq/u5oK9wAjknguAfBsK95w8Z7QYJV95v1Y2ttFmtdJ0+EWjH0deIctaPZBZUkZYc67QsRnpKLPts05t9QX65+wXPe2PV/CQeuXQOSTHhKIykg49cOodV85O82o/gOnJOBME/DIR7zx8yjooKJWXkMJeUswc27aGu2cIvz5kS0MfR10gqjX6iqqGFA8fquKSHiys9JRarhuwj1Zw+xfv5qI5UNDAryfsFyFfNTxpSN81AQM6JIPiHgXDv+UPGtORYPjlwHK2100S32/aU8mpGITefNZlbl03l1mVT+1XGQEIsZ/2EfSq1o3gzO/PGxaCUbyYFtFmsFFY2elxTUxAEQRA8JS0llrLaZgorHQf1V9a3cNfruUyPj+LWZVP6WbrAQ5SzfiIjvxKzSTF3nHPLVVRYMNPGRJHhgyLoJdVNtFm1192agiAIgtAb6b3End27YTfVjS386cp5hASJaiJHoJ/IyK9kRkIUw0J69iSnpcSSlV+J1erdZLT2HGfjxHImCIIg9DPT4qOICDE7VM7eyilhY3Yxty2bwszE4X6QLvAQ5awfaLNYyS6s6lTs3BnpybHUNrex/1idV2Ww5zhLGem9NBqCIAiC4Apmk2J+cvci6GW1zdy9Ppe5Y6P5yZJJfpIu8BDlrB/45mgtDS0Wh8lnu9Jeh8zLrs38inpCzCbifVDYVhAEQRB6Iy0llm+O1lDX3AaA1prfvJFLfYuFx6+cS5BZVBI7Lh0JpdTrSqkLlFJy5DzArmg5KtvUlZSRwxgZEeL1SQFHKhoYGxseUMV3BUEQhKFDWnIMVg05R4wJcm9kFbFtTymrz53G5NFRfpYusHBV2foL8D1gv1LqUaWUS5nglFIrlFJ5SqkDSqk1DtafoZTKVEq1KaUud7B+uFKqUCn1ZxflDEgy8isZMzyUJBeyGytlmH4zvayc5Zc3eK3guSAIgiC4y/wOkwJKqhv53YbdLEiJ5frTJ/hZssDDJeVMa/2u1vpqIA04DLyrlPpMKXWdUsphunmllBl4GjgPmAl8Vyk1s0uzAuBa4D9Oun4A+MgVGQOZjPxK0pJjneZ26Up6SiyHjtdTUd/ilf611hSUN0gaDUEQBMFvRIcHEz88lD9vP8Cpj7xPXVMby2fFi0fHAS67KZVSIzEUqRuALOBJDGVtm5NNTgIOaK0Paa1bgJeAizs20Fof1lrnAFYH/aUDY4B3XJUxEDlW00RhZaNLLk077XFnXrKeVTW0UtvcJjM1BUEQBL+xPquIsroWmtuMV74G/rRtH+uzivwrWADiaszZG8DHwDDgQq31RVrrl7XWtwCRTjZLAo50+F1oW+ZKfybgceAOV9oHMvZ4M1cmA9hJHRtNkEl5bVJAfoXM1BQEQRD8y9qteVi6pIlqbLWwdmuenyQKXFwt3/SU1nq7oxVa6wVelMfOz4DNWuvCnlyBSqmbgJsAkpOTfSBG38nIryQkyMQsN3K3hAWbmZUU7bVJAfnl9QCSgFYQBEHwG8VVjqsDOFs+lHHVrTlTKRVj/6GUilVK/ayXbYqAcR1+j7Utc4VTgZuVUoeBPwI/VEo92rWR1voZrfUCrfWCUaNGubjr/iUjv5LUpGhCg8xubZeeHEt2YRWtlm4eX7c5Yk9AGyvKmSAIguAfEp1MinO2fCjjqnJ2o9a6yv5Da10J3NjLNl8DU5RSE5RSIcB3gA2udKa1vlprnay1Ho/h2nxea91ttmeg09RqYVdRjVsuTTtpKTE0tVrZW1LTZznyyxsYHRVKeIh7CqIgCIIgeIvVy6cRHtz5PRQebGb1cpcSQAwpXFXOzKqDf9E2EzOkpw201m3AzcBWYC+wTmu9Wyl1v1LqItt+FiqlCoErgL8ppXZ7MohAZXdxNS0Wa4/Fzp1hnxTgDddmfkWDuDQFQRAEv7JqfhKPXDqHpJhwFJAUE84jl85h1XyXwtGHFK7GnG0BXlZK/c32+8e2ZT2itd4MbO6y7N4O//8aw93Z0z6eA55zUc6AIjPfMDampcT00rI7CdHhJEaHkVlQxXWL+ibHkYoGTpsU17edCIIgCEIfWTU/SZQxF3BVOfs1hkL2U9vvbcA/fCLRICIjv5LkEcMYHeVZyaS0lL4no21qtXC0polkSaMhCIIgCAMCl5QzrbUV+KvtT3ABrTUZBZWcPtlzi1V6SiybckooqW4kIdqzgMnCyga0lpmagiAIgjBQcDXP2RSl1KtKqT1KqUP2P18LN5AprGykrLbZo8kAduyxanb3qCfklxszNaV0kyAIgiAMDFydEPAshtWsDTgLeB54wVdCDQbsgfxpye7Hm9mZmTicsGBTnyYFFNjSaIhbUxAEQRAGBq4qZ+Fa6/cApbXO11rfB1zgO7EGPpkFlUSEmJk2JsrjfQSbTaSOjelTpYD88gYiQsyMjOhxcq0gCIIgCAGCq8pZs62k0n6l1M1KqUtwXrZJwLCczUuOIcjscvlSh6SnxLK7uJqmVotH2xdUNJA8MsLlouuCIAiCIPgXVzWH2zDqat4KpAPfB67xlVADnfrmNvaW1JDuQX6zrqQnx9Jq0eQWVXu0fUFFA8kjJPuyIAiCIAwUelXObAlnr9Ja12mtC7XW12mtL9Naf9EP8g041mcVsWTtdqwaXvyygPVZrlascszRGqPm2BX/+zmLHn3frf1ZrZqCigYpeC4IgiAIA4heU2lorS1KqdP7Q5iBzvqsIu56PZdGmwuyvL6Fu17PBfAo6d76rCIeeuub9t9FVY1u7a+0tomWNqtMBhAEQRCEAYSrbs0spdQGpdQPlFKX2v98KtkAZO3WvHbFzE5jq4W1W/P8sr8CWxoNyXEmCIIgCAMHVysEhAHlwNIOyzTwutclGsAUVzW6tdzX+8uXNBqCIAiCMOBwtULAdb4WZDCQGBNOkQPFKTHGs4D8vu6voLwBs0l53L8gCEOMnHXw3v1QXQjRY2HZvZB6ZeDtUxAGOa5WCHhWKfXPrn++Fm6gsXr5NMKCOx/S8GAzq5dP83h/4cHmLvszuby/gooGkmLCCe5jOg9BEIYAOetg461QfQTQxr8bbzWWB9I+BWEI4OpbexPwlu3vPWA4UOcroQYqq+Yn8fMzJ7f/TooJ55FL53g0GcC+v0cunUNSB8vXDYsnury//IoGcWkKguAa790PrV0s9a2NxvJA2qcgDAFcdWu+1vG3Uuq/wCc+kWiAMzXeqAiw8ebTmTM2us/7WzU/iVXzk6hvbiPtgW1UN7a6vG1BeT3nzUnoswyCIAQo3nQZVhe6t7yv+9QaOibHFvenILTjqb9rCjDam4IMFo5WNwEQHx3m1f1GhAaxdPpoNucexWLVvbavaWqlsqGVFLGcCcLgxNsuw+ix7i13hTBnH6ganpgNm34JeVsg8wVxfwpCB1yNOatVStXY/4CNwK99K9rApLi6kRCzySe1LFemJnK8rpkvvy3vta09jYa4NQVhkOJtl2H6tY6XTznXs/199XdoqgLVOW6WoDBI+yEkzoPsl+G/V8GGn4v7UxA64Kpb0/Pq3UOMkqom4qPDMJm8X8ty6fTRDAsxsymnhNMmxfXYtsCeRqM/cpyJO0IY6vjjHvCmG7KpBrJegNAYCB0GNSUwPAlCIiDjWZiwGGZd4vr+Mp+HzXfAtPNhxkWw/SHHx6atGQ5/Ai84SZvZF5eqIAxgXFLObIXO39daV9t+xwBnaq3X+1K4gUhJdSMJXnZp2gkPMbNsxhi27DrK/RfN6rGoekF/5Tizu1bsX712dwSIgiYMDfr7HtAadr8OygTa0n29u25IrWHDLVBVANe+BSmnnljXXAcvXAav3QDmUJh+fu/7y34ZNtwKk8+GK56DoFCY913HbYNCYfIyiB5nc2n2cSyCMEhwNebsd3bFDEBrXQX8zjciDWyKq5p8mlfsgjkJVNS38NnBnl2b+eUNjIgIISos2GeyADIbayiRs86IE7ovxvh3sMcDuTre937ff/dA6W7414Xw6vUQlWAoTF056Sb39vn1P2DPelh2T2fFDCA0Eq5+BeJT4ZVr4MC7Pe9r9xuw/icw/nS46gVD+XKFZfdCcJfnZnC4sXygMdTuk4HCADsvrlYIcKTEubrtkMFi1ZTWNPnMcgZw5rRRRIYG8VZOCWdMHeW0XUFFff/Em/lihpcQeLhjHRoMbm5H491wC5TugegkKD8I5QeMP1/cA12P4eI7oGyvEccVNhwu+JMRI7brtRPtIsdASz188ReYsRJGTOy9n+Is2PobI67stNsctwkbDj943VAKX7raUNYmnNG93TebDQvb2JPgey93V7Z6wn59vHf/CQva6b8KrOvGlevaXSuqq/fKYLin/MkA9PC4ajnboZT6k1Jqku3vT0BGbxsppVYopfKUUgeUUmscrD9DKZWplGpTSl3eYfk8pdTnSqndSqkcpdRVrg/Jfxyva6bNqknwoeUsLNjMOTPHsGX3UVrarE7bFVQ0+LamZlsLfPL/nK+P6DkmThhgOLOQbv0t1B0zXGMweJKOOrKGtTXBp08YsVSZ/4K6o0ZQe+hwx/uIGuNZ346O4abb4Mv/NRSyWzJh4Y/AZDZeLL/cBfdVwR158KOthpz/ushwU/ZEYxWsuwYiRsGq/wVTD6+D8Fj4wXqInQD/uQryP++8fv+7hmUtYa6hvIVEuD9u+1jWHDH6O/Kl+/vwFT1d11pD/XEo+BLe/rXj++TtX8ORr6ChwrV9utq34BoD0MPjqvXrFuAe4GWMmprbgJ/3tIFSygw8DZwDFAJfK6U2aK33dGhWAFwL3NFl8wbgh1rr/UqpRCBDKbXV5k4NWOw1LxN9aDkDw7X5RlYRnx44zlnTu2c0abVYKa5q4pJ5PlLO9r8LW35tWA3i58LxPOOF0I6C+jJ4+Qew/CGISXZv//KVGFhYrY7jgQDqj8EfpxgKyshJUJbn/CEYKOfQ2fVVWwr734H9W3uwein41V6Iij+Ro6vrV7md2mOw7XdwxmrDPegqjl4kYFjGVv6p523HzDKUqH9dZPxdtxmGJ3ZvpzVsuBlqiuDazRAxsne5IuLgh2/Cc+fD8xdDeDTUlRnLGyphzAz4/muGpa0vhA2H038J2+6F/M8g5bS+7a83XHneOHu5v/lzQ1lvqqZHGivg/84x/h8WAyMnw7E9jve55a7Oyu2WuwL/ngp0erJuW609f5j4CVdna9YD3SxfvXAScEBrfQhAKfUScDHQrpxprQ/b1nUyAWmt93X4f7FS6hgwCgho5azEluMsIdoFy1kfFJDFU+OICgtiU06JQ+WsqLIRi1Uzzplb01NT+ik/NWZW5W2GEZPg6tdgytnd2515F9QWw0ePw/5tsPhXcNqtsHeDd90C/nQJeLvvQFVI68qMGCJnDIuDJXeecPG1NjhuV10IVoth7bHjjzE7ur7W/xTefxCq8o1lUYnGy7Glvvv20WNheJfEzp1ccraxLLoNijLh0/9n9HnuAzD7Msh9xfmYGyvhwHvOFeG6Y66NMXGeoST9e9UJBS2yy3Piy7/B3o1wzgOQfLJr+wXDGnjSTYYlyC5PfRmgIP16w+LlDRbeCJ8/De89YMivPJj97qkbcsMthrs3Kt52XR90fk4sLTD/B4ayNXKysa/aku7tohLgwidP3CflB53fKw3H4aXv9T6+/ggdCdTnkrtEj3VyDjX8eYHxbpv3PeO+D5Axuzpbcxtwhd1ypZSKBV7SWi/vYbMkoOPRKATceAq0930SEAIcdHfb/sZuOes15qyP/u/QIDPLZ8WzdfdRmttmExrUOY9Qvm2mZspIB64FV/t21G7rb8AUAmffB6f87ESwb+qVjuVO/Q68c7cxjf6L/4WWWuNh1rXfWZcaL8byg7B5tZOvyTWGVWDERCOWpS/j6Gusgbf7DtR4iEMfwus3GUrDvKuNGYIdz01wOKx4pLOMT8x2/hBcOxmmnANTl0NTLWxd0/9jdmQBsbYZL9Sld8OU5RA/x1CiulrDegpQd3YPLLjOuKZf+xF88ChUFxjpI+CEIrBvC9QehYIvjNmXygTaQciCOzMXxy003IsvXGZYua7ZdMI6VpRh3JdTV8CpN7u+Tzuf/Q+GA6UjGj75Eyy83v39OSJkmGFx3HwHHHzPmPnpDj3dU7MvM36XH3D8vGlrMuL2wHD5jpgEwRHQ6khZH9fZmnnO/Y6vm3PuN657Orwynd0rkWOMc2fnxSugrrR7uwjnMcdeIVCfS54w9zvw0drOy4LDYf73jY+ozXcYH2jjToFvPzjhCfLjmJXWvWebV0plaa3n97asy/rLgRVa6xtsv38AnKy17vY0UEo9B2zSWr/aZXkC8AFwjdb6Cwfb3QTcBJCcnJyen5/f61h8yQOb9vDil/nsvX8FqqcvPWc3ZfQ4I+bCBbbnHeO6Z7/m7z9cwDkzO8e2/PuLfO5Zv4svf7OMMcO7KIrO+h4WZ3zZ2dl4m/EF15WoRLh9r0sytnPoQyOPkbWt+zqT7fvA0TpnRI8z3Gn2l1xHXB2HG8e6G309hv0hY1+wtMGHjxkPs7gpcPmzED/bM0sEQFA4pH3fUMj2v2O4eJzh6zHfF0N3xQJAGXFbHfHWF7TVAln/NrLhO1K6AMbMMV7eU5dDxSHY9IvuL/gLn3K//0MfwItXQsRowAo1xYbyFxYNt2TAsBHuj8edY9gX2lrgz+kQPgJu+sA965mze9QUZIzf/pHoFAW/PgzhMcZPR9e1s3PijrXclX06dJsrQMOcKwzrZ1drrjfwwnsqINAanj3PmOkcOtxw5Xc8L1ob8YBfPA173nS8Dx+NWSmVobVe4GidqzFnVqVUsta6wLbD8Ti+OztSBIzr8HusbZlLKKWGYxRa/60jxQxAa/0M8AzAggULetcyfczR6iYSo8N7VszAK7O7Tp8cR8ywYDblFHdTzgrK6wkNMjEq0sE0dmd9NByHl6/uvWNHJvvemLjEeEE5wtpmzMoaOdmIWXr1OuMF0pXIMYaVxj5LLudlx/tzdRzVR4x9jZzUeXlPD9bqIls8khMXh6t9uyNjc13neCVvm9y77u/UW4y0CgWfGday89eeiH9xZh3qiCMXX0cZrRbDcmOPv+k25kJobYLgDh8V3hyzM/eGI6uUK+N1BZPZCOTf+AsnDRT8tEOp4nEnGQqEN8Y88Uw4+cfw2VMnlmmL4bI98K5n+3TnGPaFoBBYsgbe/Jnhgp15kevbOnvOWdsMl7PdDfnqj4wQjK5Ejz2hmEHv13VHXL1uXN2no3ZL1hjehk+fhLy3DSvjKT8z7t1AP+xbyQAAH4VJREFUrrXqDw68BwWfwwWPw8Ibuq9XynDtJ5/s/MPDD2N2VTn7LfCJUupDDJV9MTaLVQ98DUxRSk3AUMq+A7jgSAelVAjwBvB8V2taIFNc3UhCjAuTAaLiHSs5bjzcgs0mVsyKZ2N2MU2tFsKCT7g288sbSB4xzHGVAmcP1sgxcHWHQ/3i5Y5N6Z4+gJ0+0MfB2R1S5p39e8dfk+c+aLgj7OR/1rdxAPxPmvGAnrrCSCVQUwRv/aqzGf/Nm43cTVVHoDTXWK7MjpN/utq3OzL+YYKRM2rqCuPF8v4D3nMzOHJbbLnTcF1f8gzM9XCSdE8vJ5PZUD6cJR1Fwx8mGkrF1OWGdfTde7035sW/MixYHemvfFr+UAzBuH67Ymn2PKB82b3uuXz7QupVRtze+w/C9As6xyz2hNNn7DjDxWjnHCfPG0dj8eY5cXefztrN+54RbvLu74wYvaZq49xC3+6Vbz82jrUjj8ZASgysNbx/vzEpbf4Pe2/fXx8eLuDSFAWt9RZgAZAH/Be4HXAwnajTNm3AzcBWYC+wTmu9Wyl1v1LqIgCl1EKlVCFwBfA3pdRu2+ZXAmcA1yqldtr+5rk/vP6lpKqp98kAtaWOrUgePNxWpiZS32Lhg7zOgcI9ptFYdm/3Wnd25Sch9cTfuQ96Nymkq0kmU680zPrR4wBl/OvIdeBsf66OY/nDcN4fjJv2q2fg+Ytg/c+6x59Ymo0JEGHDDcXxZ1/CJf/bt75dbbf4diP4uuoIvH2n8RD25nRwZ7MCI0Z4rpi5iqPzFxQOp95qxIeUZBsvlredxCB6OuZj3xj/RsbT4/XlC/yVaNXbFhBX71FvYA6Cs35jzAjPfaX39mCkEHEU8tCX502gMmICfPe/xuSsxooTipkdd++V6kJ45Vr410rDBWh2UCPanTJe/mbvBuNZcuZdhiW2NwIoGbKrEwJuAG7DcE3uBE4BPgeW9rSd1nozsLnLsns7/P9r2z67bvcC8IIrsgUKbRYrx2qbek6jUX/cUAJa6mHJr436c7Ulxgyn8/7g9gPhlIkjGBkRwsacElbMNmIOtNYUVDRw6iQnU+NTrzSmZrfUG0GP7pjS+2Ii97ZboC8ugY7tTv6x4To89EEPrkZlzBizM3q6d/p2td3yhwwX7P+kORbP05ess+1qnVjxvElvY9baSDXwVydpFDwZc9k+Ixv+guth5ROeyd0XvH1PuYovrAG+sCI5Y8bFxiSNDx4xJg/19JKtKTZmqGoLnHW3kY/OW27IQGbK2c5DR6oLoaXBmGRhp2uowFm/MY7dx48bcZFL1hju3282nWg3PBEwGR+zk5cZ1u1AxmqB9x+CuGmGBdYV/HWPOsDVCQG5wELgC631PKXUdOBhrbWTarX9z4IFC/SOHTv81n9RVSOLHn2fRy6dw3dPcpDXq7HSyLB9fP+JDNtWCzycZCSUXP6QR/3+9o1cXs8sIuOesxkWEkRZbTMLH3qX+y6cybWLJjiW47HxxgW3+HaP+hy0BHoArLfle3yGk3ibABkveHfM/7nKcIffkgmRPp7pFki4E8weqOx7B/5zhVEZYeGPHLepOwbPnm/MfP3hehjrMM568OJ0pjQQFGa8c6YuB0urgyTLtgkG01caXoXYFMf7qS83rGqVh410Lb7OQdcXsl+CN34MV/wLZq3ytzQO6WlCgKuZ15q01k22nYVqrb8BpnlLwMFASU9pNJpq4N+XGgk6v/PiidInJjOMmmpYCDxkZWoija0W3v/GcG0W9JRGA4z8PQCJTqwwQ5kAMmk7xJF8YGRtt7gx2xUM916nxME2Amm84HjMpiD3ZTy43UhZsfj2oaWYwcB33YGRgmXcycYMYkeu+PpyI2VITRFcvW7oKWbg/Pm16BfGhJTj++Ct2420RN2OoTZSc3znReeKGRjpWH74JgxPMmYBF/rPINIjbS2w/WGjYsUMNyaSBBCuKmeFSqkYYD2wTSn1JuDfvBUBRrEtAW23oufNdUaemqM5cOXz3fP1jJpxIg7GA06aMIJRUaFsyjaCXwsqjFw8ThPQtitnAR/C1/8E+kvMkXxTz4PDH8HL33ecNNUR+Z/BP88Fc7Dh+gnU8UL3MQcPM4KUh7mQ0d6O1WLk9YpJgZN7SKg7mOlY5umXuwLrHLuCUrD0HiMM5Ot/dF7XWGUk3K04BN99KbCtOb7E2fPrnN/DeY/BrTvh5187377eQTofR0SOhms2GJUh/n0pFO/0ivheJevfxmzWpfcEZPZ/V3C1QoA9AvA+pdR2IBrY4jOpBiAOLWetjfDf70DhV0auqGnndd9w9AzIecl4wHScuu0iZpPi/NnxvPT1Eeqa28gvb0ApGDfCycSEokwjkau3MnkPNgI9/sSRfF//w0im+a8L4Xvreq5runu9kVg2JtlwS8SmwJLVvpW5r3Qcc0sD/H2pMYaffOJafqesF6B0F1zxXOcUHcLAYsJimHgWfPwnSLvGmKTTVGMk2j2211DMJi7xt5T+pafnl1KGp8bZTGl3YhCHJxoK2rPnw78vgWvfgjEzPZPZ27Q2GhbWcae4n7w4gHBbpdRaf6i13qC17i2L35CipLqJyNAgova9Yfj+74sxYrsOfwyX/M25z3v0DOPfMs+tZyvnJtLcZuXdPaUUVDSQMDysW9WAdoqzxKU52Fh4A1z1ApTuMfKHlTsppvH5X4yZWInz4Efv9Oy+CFRChsGV/zJK37x6fe/u3ObaE5m/ZwZm3IngBkvvMWYlPjHLeMaunWTkzbvyX0ZQvNA73grfiEk2FLSgUPi/c+GP04xz8sRs/xZl//ofhoV12b2elf0KEAamvS8AKa5q5HvhXxiBt9VHAG3E9JiDe95wlG3m3zE3s+53ID05lvjhYWzKKaGgvIFkZ2k0akuNmIxEp4UdhIHK9Avgmo2GBfb/zoUPHjvxkfDELPj35bD1LqPdD9/0LDN8oDBqmjHbsuAz+ODhntt+8oRRTWLFwwP6QS3YqDhopAJqrgG0kenfHOy6S1/wbvjGiIlG8tuWWqg7CugT+dX8oaA11RiW1UlLYfyi/u/fi4hy5iVKqpu4qfUFB3myWnvOMxM9DkIi+6ScmUyK8+ck8NG+MvYfqyO5t3izJLGcDUrGLYQfbTPSUHzw8ImPhOpCOLgNJi414h4dTSoYaMz9jlFw+uPHYf+7jttUFcBnfzam0Sel9698gm947/7uCaAtLZ7nvRuqeDMG8atnui/rSy7CvvDFXw3L6tJ7+r9vLyPKmZcoqW5ipKXM8cqecjKZTIYloMxz5QxgeHgQLRYr1Y2tbNl1lPVZDiplFWcZZWHiU/vUlxDAxE2GYAdluwDK97ueYX0gcP5aGD0LXr/RKK3VlXd/b1zvgTT7VOgbg6Wk0GAiUM5JQwV8/mcjHcggMECIcuYFmtssHK9rpjZ0jOMGvQVa9nHG5vqsIv724Yk4o5qmNu56Pbe7glacaSTk61irURh81DipfzrYXmDB4UasUVtz9/izI1/DrlfhtFsGVrkZoWecnUs5x/7D3+ckZ50RwvGHCYa7e5CkURHlzAuUVhslMw5O/EH3la4EWo6eYcTF1Jd71P/arXk0tlo7LWtstbB2a96JBVrbJgNIvNmgx98Py/4kbgpc+CQc+cKoOwrGtb71LqNE06Lb/Cuf4F0CPRfhUMSf58SeYLnj7NMPH/PvhAQvIcqZFyiutqXRaMk3glWjEnEr0NJeDshD12ZxleMyp52WVxdCfdmgMPcKvTDUXmCp/7+9Ow+Ssr7zOP7+zgGMHDMcM8DMoMB4AEaEirJ4RAErHlEjSSmrOZbdssqtTVJJ3KyJpna1QjbJmmxiYq2b+yAxRtmsumhM1CCaxCQcMgpBJALxgB4cEGe4ZoY5vvvH80xoxgH6erofuj+vKqq7f/P007/hB93f/j2/3/d7XZBk89mvw5eb4HM1sH1NkA1ds8TFJe65CEvREWMSuuRz+RmTweoDF2q9W46llOdMjq2lvYN6djP+Lw8FpUXe85X0TlAX5odp3QSTL0z79etrqtgxSIB2REJcVQYoHTGqD5c3jefCc0vhYFIizQ3Lgv9Pxfx7l6K45yIsRf1jsvNP8K0L8velKC7r3SKgmbMcSLR1clPFo8GD8z+e/glGToSh1Rnv2LzlsjOoqjxyoXdVZTm3XJZUYSuxLih7M/7MjF5DTjAnekb4dD39H8CAOsFF8g1a5IQx/kwYXgdbn8rP6xXxEg4FZzmwb/d2bqhYic26AWomHf8JA5kFlzYzTES7cHYDX3r/WTTUVGFAQ00VX3r/WSyc3XD4oB3rgv84ypAuxaiIv0GLnDDMoGk+bHsa+vqOe3jWBqsAUCRLOHRZMwdmvX4vFfTChTdnfpK66fDi8mAxcwbJMhfObjgyGEvmHtQ/e8f7Bv+5yImuujH7kjQikr2mBbD+gaBk2sQI0zYd3AOblsOYJujtCtLpFNESDgVn2TrwJhfvfYRVwxdw/pipmZ+ndjp0/Aj2t8LIo6TkyNSebdDVrvVmUrwuuT3YtZW8OLhIvkGLnFCmzgtutz4VbXD26zuCiiiLHynK5Tq6rJmtVd9kKF2sblyc3Xmy3LF5TH/dDKA0GlKktItPJB5GTgg2uW1bGd1rvLYK1v0YzvtIUQZmoJmz7HS04au+xa96z6VywozszpW8Y3PqvGx7dqQd66Bi2OEi6yLFSLv4ROKhaQGs/m4wk53rcnG93fDozTCqES6+NbfnjhHNnGVjzXexrn3c07OQidVZLrQfXgtVY7KqsXlUiWaYcNbxi7CLiIhka+r8YB3Yq7/P/bn/+E1o3Qjv+XJR5zFUcJapQwfgD//Nnob5bPTJTKzO8tuBWTCzleGOzaPq64WWF7TeTERE8uOU86F8SO4vbba9HqTNOf0KmHZlbs8dMwrOMrX2h9Cxh+bJNwJQX5ODFBV104OZM/fjH5uq3X+G7gOqDCAiIvkx5CQ4eS5szXFw9qtbAQ9mzYpcpMGZmV1uZpvNbIuZve3isJldZGbrzKzHzK4d8LPFZvZy+CfL1fY51t0Jv78bplzMxrJgIf+EbC9rAtROCwq37k1kf65+O9YFt9oMICIi+TJ1fpBOY39rbs63+Zfw0qNw8Weg5uTcnDPGIgvOzKwcuAe4ApgB3GBmA1fNvwb8PXDfgOeOAe4A/gaYA9xhZqOj6mvamn8C+9+Ai26hpb2DcSOGMLSi/PjPO57+TQG53LGZWAdDRsDY03J3ThERkWNpmh/cbns6+3MdOgCPfTpIOXXeR7M/3wkgypmzOcAWd9/m7oeA+4Frkg9w91fcfT0wMJXwZcCT7r7H3d8CngQuj7Cvqes5BM9+AybNhckXkmjrzH69Wb/+3ZS53BSQaIaJs6BMV7BFRCRPJpwdbHLLRSmnZ74M7a/BVV8rmY1tUX5iNwDJKbu3h21RPzda6x8IMpFfdAuY0dLekZtLmgAnjQnqkrXmaFNAzyHYuQEadElTRETyqKwsSAu1dWV266jfeBH+8F8w+0PBRoMScUJPp5jZTWa21szW7tq1K9oXW78M7joTln8MyiqhYw8ALW2d1OcqOINwx2aOZs5aX4TeQ1pvJiIi+de0APbvzCwLQf9n7jfPA++D+nfmvn8xFmVwtgNIrgLeGLbl7Lnu/h13P8fdz6mtrc24o8e1fllQGqa/iHJfNzzycTqe+xn7unqYWJPDJHt104OZs1wUjU30bwbQTk0REcmz/nVn6V7aHPiZ633wxGeD9hIRZXC2BjjNzKaY2RDgemB5is99HLjUzEaHGwEuDdsKY8WSI2v2AXR3ULHy8wDZJ6BNVjstSH0xWBHndCWaoWo0jJ6c/blERETSUd0YbEZLN6XGUT5zWbEkd32LuciCM3fvAT5GEFRtApa5+0YzW2Jm7wUws3PNbDtwHfBtM9sYPncP8HmCAG8NsCRsK4z+6H2Aiv1Byov6nM6cJZVxytaO5uCSpln25xIREUlX0wJ49Vno6Ur9OUf5zD1qexGKdM2Zuz/m7qe7e5O7fyFsu93dl4f317h7o7sPd/ex7n5m0nN/4O6nhn9+GGU/j6u6cdDmg8MmALmeOTsjuM123Vl3R7DmTJc0RUSkUJrmQ/dBeH1V6s8ZfpRlSkf5LC5GJ/SGgLy55Pa3F2+trOLpSf+EGYwflcPgrKoGRtZnv2Nz5wbwXm0GEBGRwpl8IZRVpH5ps7c7OJ4BV3wqq4LP4hKh4CwVMxfB1XdD9STAgtur7+bpIfOoGzmUyvIc/zXWTQ9mvbLRXxlAZZtERKRQho6Exjmpbwr4wz2wLwFzP/K2z1xmLoq0q3FSUegOnDBmLnrbP4yW1atyl4A2Wd10WPNsULS8LMPKA4lmGDEeRk7Mbd9ERETS0TQfVn4RDu4J8nkeTdtr8MydcMaVcPkXgz8lSjNnWUi0d+Sm4PlAddOhpxPeeiXzcyTWBevNtBlAREQKaep8wI9fyumXnwlur7gz6h7FnoKzDLk7LW2dTBgVwcxZbZZlnDr3wu6XdUlTREQKr342DKs+9qXNl34Bmx+DebdCzaSjH1ciFJxlqL2jm47u3mhmzrLdsdnyAuDaDCAiIoVXXgFTLgpmzgYr5dS1PyhsXjcjWGsmCs4ylWjrBIhmzdnQEVBzcuY7NhPNwa2CMxERiYOp84Pk6m9uffvPnrkT9m6Hq+4qmcLmx6PgLEMt7UH24olRzJxBcGkz08uaiXVQfTIMH5fbPomIiGSiaUFwO/DS5hsbgx2asz8MJ8/Nf79iSsFZhhLtwcxZfRQzZwB10+DNl4OcL+lKNEODZs1ERCQmxkwJSgluS8p31tcHj94crEd7d+mUZkqFgrMM7WzvoKLMqB05NJoXqJsBvYdgz7b0nndwT7DLU5c0RUQkTpoWwF9+e3jS4fl7g8oBl/77sVNslCAFZxlqaetk/KhhlJdFlKqidlpwm+6lzUSYfFZlm0REJE6mzodD+2D7WjiwG568HU4+H2Z9oNA9ix0loc1Qor0jtzU1Bxp3OmCwK81NAf2bASaenfMuiYiIZGzKRcHtfYuga29w//RLlY9zEJo5y1BLeycTayJabwYw5KTgGn26ZZx2NMPYU4ManSIiInHx8hNgZYcDMwh2aq5fVrg+xZSCswy4Oy3tndRHOXMG4Y7NFGfO1i+Du94Bm38BexP6xy4iIvGyYgl435Ft3R1BuxxBwVkG3jxwiEM9fUyIOjirmw57tkJP17GPW78MHvl4kEMGoPtg8FgBmoiIxEX79vTaS5iCswy0RJmANlnddOjrgTe3HPu4FUuCbx/J9G1ERETipLoxvfYSpuAsA4kwAW0kpZuSpbpjU99GREQk7i65HSoHTGpUVgXtcgQFZxloaQurA0Q9czbuNLDy4+/YHD528HZ9GxERkbiYuQiuvhuqJwEW3F59d9AuR1AqjQy0tHcypLyMscOHRPtCFUNhbNOxZ87+8hs42AYYkFRQVt9GREQkbmYuUjCWAs2cZSDR3smE6mGURZWANlnttKMHZ6/9Ee67HsadCld+Vd9GREREioBmzjKwM+oEtMnqZsCmR4IF/snX6rc/B/deC6Mmwt8th5Hj4dwb89MnERERiUykM2dmdrmZbTazLWZ26yA/H2pmD4Q/X2Vmk8P2SjNbamYbzGyTmd0WZT/TlWjrpD7KBLTJ6qYBDrv/fLitZT3c+76gFll/YCYiIiJFIbLgzMzKgXuAK4AZwA1mNmPAYTcCb7n7qcBdwJ1h+3XAUHc/C3gn8I/9gVuh9fY5b+ztzN/MWe304LY/GW3rJvjJQhgyEhY/AtUN+emHiIiI5EWUM2dzgC3uvs3dDwH3A9cMOOYaYGl4/+fAJWbWv7J9uJlVAFXAIWAvMbB7fxc9fR5t6aZk/bUyH7oJvnoGfO/dUFYJi5fD6FPy0wcRERHJmyiDswbg9aTH28O2QY9x9x6gHRhLEKgdAFqA14D/dPc9EfY1ZYn+NBqj8jBztn4Z/OLmw4/37YRD++C8jwS7OEVERKToxHW35hygF6gHpgCfMrOpAw8ys5vMbK2Zrd21a1deOtbSHlYHiDoBLQye+R9g9Xejf20REREpiCiDsx3ApKTHjWHboMeElzCrgTeBDwC/cvdud28FngXOGfgC7v4ddz/H3c+pra2N4Fd4u/6Zs/qoE9CCMv+LiIiUoCiDszXAaWY2xcyGANcDywccsxxYHN6/FnjK3Z3gUuYCADMbDswFjpMmPz9a2jsZVllGzUmV0b+Y6pCJiIiUnMiCs3AN2ceAx4FNwDJ332hmS8zsveFh3wfGmtkW4J+B/nQb9wAjzGwjQZD3Q3dfH1Vf09HS3kF9dRXBvoWIqQ6ZiIhIyYk0Ca27PwY8NqDt9qT7nQRpMwY+b/9g7XGQaOvMz3ozOJzhf8WS4FJmdWMQmCnzv4iISNFShYA0tbR38K7T8rO+DVAdMhERkRIT192asdTd20frvi7q85WAVkREREqOgrM0tO7rwp38JaAVERGRkqPgLA0t/QloNXMmIiIiEVFwloZEfwLafOQ4ExERkZKk4CwNf505y9duTRERESk5Cs7S0NLeyYihFYwalocEtCIiIlKSFJyl6OHmHdy/+jX2d/VwwX88xcPNAytRiYiIiGRPwVkKHm7ewW0PbqCzpw+AHW0d3PbgBgVoIiIiknMKzlLwlcc309Hde0RbR3cvX3l8c4F6JCIiIsVKwVkKEuFGgFTbRURERDKl4CwF9UdJOnu0dhEREZFMKThLwS2XnUFVZfkRbVWV5dxy2RkF6pGIiIgUKxU+T8HC2Q1AsPYs0dZBfU0Vt1x2xl/bRURERHJFwVmKFs5uUDAmIiIikdNlTREREZEYUXAmIiIiEiMKzkRERERiRMGZiIiISIyYuxe6DzlhZruAV/PwUuOA3Xl4HUmdxiSeNC7xozGJJ41L/ORjTE5x99rBflA0wVm+mNladz+n0P2QwzQm8aRxiR+NSTxpXOKn0GOiy5oiIiIiMaLgTERERCRGFJyl7zuF7oC8jcYknjQu8aMxiSeNS/wUdEy05kxEREQkRjRzJiIiIhIjCs5SZGaXm9lmM9tiZrcWuj+lysx+YGatZvanpLYxZvakmb0c3o4uZB9LjZlNMrOVZvaimW00s0+E7RqXAjKzYWa22sxeCMflc2H7FDNbFb6XPWBmQwrd11JjZuVm1mxmj4aPNSYFZmavmNkGM3vezNaGbQV7D1NwlgIzKwfuAa4AZgA3mNmMwvaqZP0IuHxA263ACnc/DVgRPpb86QE+5e4zgLnAR8P/HxqXwuoCFrj72cAs4HIzmwvcCdzl7qcCbwE3FrCPpeoTwKakxxqTeJjv7rOSUmgU7D1MwVlq5gBb3H2bux8C7geuKXCfSpK7/wbYM6D5GmBpeH8psDCvnSpx7t7i7uvC+/sIPnQa0LgUlAf2hw8rwz8OLAB+HrZrXPLMzBqBK4HvhY8NjUlcFew9TMFZahqA15Mebw/bJB7Gu3tLeH8nML6QnSllZjYZmA2sQuNScOHls+eBVuBJYCvQ5u494SF6L8u/rwOfBvrCx2PRmMSBA0+Y2XNmdlPYVrD3sIp8vZBIPri7m5m2IBeAmY0A/hf4pLvvDSYEAhqXwnD3XmCWmdUADwHTCtylkmZmVwGt7v6cmc0rdH/kCBe6+w4zqwOeNLOXkn+Y7/cwzZylZgcwKelxY9gm8fCGmU0ECG9bC9yfkmNmlQSB2U/d/cGwWeMSE+7eBqwEzgNqzKz/i7ney/LrAuC9ZvYKwfKYBcA30JgUnLvvCG9bCb7IzKGA72EKzlKzBjgt3FEzBLgeWF7gPslhy4HF4f3FwP8VsC8lJ1wz831gk7t/LelHGpcCMrPacMYMM6sC3k2wHnAlcG14mMYlj9z9NndvdPfJBJ8jT7n7B9GYFJSZDTezkf33gUuBP1HA9zAloU2Rmb2HYK1AOfADd/9CgbtUkszsZ8A8YBzwBnAH8DCwDDgZeBVY5O4DNw1IRMzsQuC3wAYOr6P5LMG6M41LgZjZTIJFzOUEX8SXufsSM5tKMGszBmgGPuTuXYXraWkKL2v+i7tfpTEprPDv/6HwYQVwn7t/wczGUqD3MAVnIiIiIjGiy5oiIiIiMaLgTERERCRGFJyJiIiIxIiCMxEREZEYUXAmIiIiEiMKzkREsmRm88zs0UL3Q0SKg4IzERERkRhRcCYiJcPMPmRmq83seTP7dlgYfL+Z3WVmG81shZnVhsfOMrM/mtl6M3vIzEaH7aea2a/N7AUzW2dmTeHpR5jZz83sJTP7qSUXFxURSYOCMxEpCWY2Hfhb4AJ3nwX0Ah8EhgNr3f1M4BmCqhMAPwY+4+4zCaof9Lf/FLjH3c8GzgdawvbZwCeBGcBUgjqKIiJpqzj+ISIiReES4J3AmnBSq4qgkHEf8EB4zL3Ag2ZWDdS4+zNh+1Lgf8L6ew3u/hCAu3cChOdb7e7bw8fPA5OB30X/a4lIsVFwJiKlwoCl7n7bEY1m/zbguExr2iXXQuxF768ikiFd1hSRUrECuNbM6gDMbIyZnULwPnhteMwHgN+5ezvwlpm9K2z/MPCMu+8DtpvZwvAcQ83spLz+FiJS9PTNTkRKgru/aGb/CjxhZmVAN/BR4AAwJ/xZK8G6NIDFwLfC4Gsb8A9h+4eBb5vZkvAc1+Xx1xCREmDumc7gi4ic+Mxsv7uPKHQ/RET66bKmiIiISIxo5kxEREQkRjRzJiIiIhIjCs5EREREYkTBmYiIiEiMKDgTERERiREFZyIiIiIxouBMREREJEb+H+oRu/pJ90cJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the loss, training accuracy, and validation accuracy should show clear overfitting:\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='red'>**4.2: Train a good CNN (7 points).**\n",
    "    \n",
    "<font size='4'>By tweaking different parameters, such as number of convolution layers, learning rate, batch size, etc, you should achieve greater than 62% accuracy on the validation set **with 3 epochs using the sgd_momentum optimizer**.\n",
    "    \n",
    "<font size='4'>If you are really careful, you should be able to get nearly 66% accuracy on the validation set. But we don't give extra credits for doing so.\n",
    "    \n",
    "<font size='4'>It may take a quite while for your training to be finished. **Do not use more than four convolution layers. Your training shouldn't be longer than one hour.** (This is a rough reference as it depends on the hardware. Our implementation takes less than 10 minutes to finish.)\n",
    "    \n",
    "<font size='4'>Use a large filter/kernel size in the first convolution layer (for example, 7), so you can easily visualize the learend filters.\n",
    "    \n",
    "<font size='4'>Since it is relatively slower to train a CNN, you can simply report the best hyper parameters you found. You need report validation accuracy of other choices below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 1470) loss: 2.302755\n",
      "(Epoch 0 / 3) train acc: 0.095000; val_acc: 0.104000\n",
      "(Iteration 11 / 1470) loss: 2.155626\n",
      "(Iteration 21 / 1470) loss: 2.123425\n",
      "(Iteration 31 / 1470) loss: 1.962013\n",
      "(Iteration 41 / 1470) loss: 2.005328\n",
      "(Iteration 51 / 1470) loss: 1.956619\n",
      "(Iteration 61 / 1470) loss: 1.922907\n",
      "(Iteration 71 / 1470) loss: 1.817497\n",
      "(Iteration 81 / 1470) loss: 1.768881\n",
      "(Iteration 91 / 1470) loss: 1.720656\n",
      "(Iteration 101 / 1470) loss: 1.851751\n",
      "(Iteration 111 / 1470) loss: 1.695799\n",
      "(Iteration 121 / 1470) loss: 1.638893\n",
      "(Iteration 131 / 1470) loss: 1.611483\n",
      "(Iteration 141 / 1470) loss: 1.786724\n",
      "(Iteration 151 / 1470) loss: 1.694045\n",
      "(Iteration 161 / 1470) loss: 1.524201\n",
      "(Iteration 171 / 1470) loss: 1.700435\n",
      "(Iteration 181 / 1470) loss: 1.552813\n",
      "(Iteration 191 / 1470) loss: 1.538498\n",
      "(Iteration 201 / 1470) loss: 1.641194\n",
      "(Iteration 211 / 1470) loss: 1.610550\n",
      "(Iteration 221 / 1470) loss: 1.625187\n",
      "(Iteration 231 / 1470) loss: 1.520332\n",
      "(Iteration 241 / 1470) loss: 1.573455\n",
      "(Iteration 251 / 1470) loss: 1.682751\n",
      "(Iteration 261 / 1470) loss: 1.482284\n",
      "(Iteration 271 / 1470) loss: 1.543458\n",
      "(Iteration 281 / 1470) loss: 1.377317\n",
      "(Iteration 291 / 1470) loss: 1.490317\n",
      "(Iteration 301 / 1470) loss: 1.510061\n",
      "(Iteration 311 / 1470) loss: 1.248955\n",
      "(Iteration 321 / 1470) loss: 1.367397\n",
      "(Iteration 331 / 1470) loss: 1.412655\n",
      "(Iteration 341 / 1470) loss: 1.403886\n",
      "(Iteration 351 / 1470) loss: 1.359828\n",
      "(Iteration 361 / 1470) loss: 1.435979\n",
      "(Iteration 371 / 1470) loss: 1.419839\n",
      "(Iteration 381 / 1470) loss: 1.355227\n",
      "(Iteration 391 / 1470) loss: 1.383864\n",
      "(Iteration 401 / 1470) loss: 1.327285\n",
      "(Iteration 411 / 1470) loss: 1.358480\n",
      "(Iteration 421 / 1470) loss: 1.303319\n",
      "(Iteration 431 / 1470) loss: 1.456897\n",
      "(Iteration 441 / 1470) loss: 1.352656\n",
      "(Iteration 451 / 1470) loss: 1.324000\n",
      "(Iteration 461 / 1470) loss: 1.341052\n",
      "(Iteration 471 / 1470) loss: 1.441221\n",
      "(Iteration 481 / 1470) loss: 1.318544\n",
      "(Epoch 1 / 3) train acc: 0.271000; val_acc: 0.278000\n",
      "(Iteration 491 / 1470) loss: 1.366575\n",
      "(Iteration 501 / 1470) loss: 1.198488\n",
      "(Iteration 511 / 1470) loss: 1.117905\n",
      "(Iteration 521 / 1470) loss: 1.152656\n",
      "(Iteration 531 / 1470) loss: 1.237739\n",
      "(Iteration 541 / 1470) loss: 1.264013\n",
      "(Iteration 551 / 1470) loss: 1.402826\n",
      "(Iteration 561 / 1470) loss: 1.282252\n",
      "(Iteration 571 / 1470) loss: 1.105421\n",
      "(Iteration 581 / 1470) loss: 1.286997\n",
      "(Iteration 591 / 1470) loss: 1.235994\n",
      "(Iteration 601 / 1470) loss: 1.134135\n",
      "(Iteration 611 / 1470) loss: 0.945019\n",
      "(Iteration 621 / 1470) loss: 1.260269\n",
      "(Iteration 631 / 1470) loss: 1.129591\n",
      "(Iteration 641 / 1470) loss: 1.240074\n",
      "(Iteration 651 / 1470) loss: 1.433519\n",
      "(Iteration 661 / 1470) loss: 1.230130\n",
      "(Iteration 671 / 1470) loss: 1.118541\n",
      "(Iteration 681 / 1470) loss: 1.106978\n",
      "(Iteration 691 / 1470) loss: 1.173122\n",
      "(Iteration 701 / 1470) loss: 1.206035\n",
      "(Iteration 711 / 1470) loss: 1.279007\n",
      "(Iteration 721 / 1470) loss: 0.985819\n",
      "(Iteration 731 / 1470) loss: 1.082271\n",
      "(Iteration 741 / 1470) loss: 1.249572\n",
      "(Iteration 751 / 1470) loss: 1.115381\n",
      "(Iteration 761 / 1470) loss: 1.132023\n",
      "(Iteration 771 / 1470) loss: 1.089483\n",
      "(Iteration 781 / 1470) loss: 1.211772\n",
      "(Iteration 791 / 1470) loss: 1.173962\n",
      "(Iteration 801 / 1470) loss: 1.230154\n",
      "(Iteration 811 / 1470) loss: 1.203030\n",
      "(Iteration 821 / 1470) loss: 1.195625\n",
      "(Iteration 831 / 1470) loss: 1.126870\n",
      "(Iteration 841 / 1470) loss: 0.972688\n",
      "(Iteration 851 / 1470) loss: 1.018467\n",
      "(Iteration 861 / 1470) loss: 1.003075\n",
      "(Iteration 871 / 1470) loss: 1.019376\n",
      "(Iteration 881 / 1470) loss: 1.110365\n",
      "(Iteration 891 / 1470) loss: 0.893944\n",
      "(Iteration 901 / 1470) loss: 1.229680\n",
      "(Iteration 911 / 1470) loss: 1.076206\n",
      "(Iteration 921 / 1470) loss: 1.021955\n",
      "(Iteration 931 / 1470) loss: 1.095252\n",
      "(Iteration 941 / 1470) loss: 1.096961\n",
      "(Iteration 951 / 1470) loss: 1.109040\n",
      "(Iteration 961 / 1470) loss: 1.067866\n",
      "(Iteration 971 / 1470) loss: 1.084970\n",
      "(Epoch 2 / 3) train acc: 0.295000; val_acc: 0.311000\n",
      "(Iteration 981 / 1470) loss: 1.102854\n",
      "(Iteration 991 / 1470) loss: 1.246756\n",
      "(Iteration 1001 / 1470) loss: 0.878095\n",
      "(Iteration 1011 / 1470) loss: 1.186484\n",
      "(Iteration 1021 / 1470) loss: 1.033844\n",
      "(Iteration 1031 / 1470) loss: 1.001177\n",
      "(Iteration 1041 / 1470) loss: 1.255710\n",
      "(Iteration 1051 / 1470) loss: 1.065299\n",
      "(Iteration 1061 / 1470) loss: 1.036976\n",
      "(Iteration 1071 / 1470) loss: 0.981677\n",
      "(Iteration 1081 / 1470) loss: 1.232622\n",
      "(Iteration 1091 / 1470) loss: 0.948731\n",
      "(Iteration 1101 / 1470) loss: 1.026312\n",
      "(Iteration 1111 / 1470) loss: 1.131940\n",
      "(Iteration 1121 / 1470) loss: 0.899818\n",
      "(Iteration 1131 / 1470) loss: 1.180603\n",
      "(Iteration 1141 / 1470) loss: 1.214805\n",
      "(Iteration 1151 / 1470) loss: 0.988186\n",
      "(Iteration 1161 / 1470) loss: 1.032627\n",
      "(Iteration 1171 / 1470) loss: 1.013459\n",
      "(Iteration 1181 / 1470) loss: 0.984548\n",
      "(Iteration 1191 / 1470) loss: 1.001568\n",
      "(Iteration 1201 / 1470) loss: 0.966645\n",
      "(Iteration 1211 / 1470) loss: 1.047078\n",
      "(Iteration 1221 / 1470) loss: 1.041941\n",
      "(Iteration 1231 / 1470) loss: 1.090035\n",
      "(Iteration 1241 / 1470) loss: 1.045615\n",
      "(Iteration 1251 / 1470) loss: 0.908767\n",
      "(Iteration 1261 / 1470) loss: 0.948094\n",
      "(Iteration 1271 / 1470) loss: 1.063334\n",
      "(Iteration 1281 / 1470) loss: 0.968301\n",
      "(Iteration 1291 / 1470) loss: 0.919925\n",
      "(Iteration 1301 / 1470) loss: 0.995771\n",
      "(Iteration 1311 / 1470) loss: 0.833041\n",
      "(Iteration 1321 / 1470) loss: 1.033353\n",
      "(Iteration 1331 / 1470) loss: 1.046201\n",
      "(Iteration 1341 / 1470) loss: 1.005794\n",
      "(Iteration 1351 / 1470) loss: 1.156094\n",
      "(Iteration 1361 / 1470) loss: 0.984750\n",
      "(Iteration 1371 / 1470) loss: 0.999071\n",
      "(Iteration 1381 / 1470) loss: 1.062008\n",
      "(Iteration 1391 / 1470) loss: 0.867147\n",
      "(Iteration 1401 / 1470) loss: 0.957910\n",
      "(Iteration 1411 / 1470) loss: 0.927511\n",
      "(Iteration 1421 / 1470) loss: 0.798985\n",
      "(Iteration 1431 / 1470) loss: 0.914807\n",
      "(Iteration 1441 / 1470) loss: 0.952002\n",
      "(Iteration 1451 / 1470) loss: 1.045338\n",
      "(Iteration 1461 / 1470) loss: 0.836627\n",
      "(Epoch 3 / 3) train acc: 0.227000; val_acc: 0.259000\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best ConvNet that you can on CIFAR-10 with 3 epochs using    #\n",
    "# the sgd_momentum optimizer. Store your best model in the best_model variable.#\n",
    "################################################################################\n",
    "\n",
    "model = ConvNet(\n",
    "    num_filters=[16, 32, 64, 128],\n",
    "    filter_sizes=[7, 5, 3, 3],\n",
    "    weight_scale=1e-3\n",
    ")\n",
    "\n",
    "solver = Solver(\n",
    "    model, data,\n",
    "    num_epochs=3, batch_size=100,\n",
    "    update_rule='sgd_momentum',\n",
    "    optim_config={\n",
    "      'learning_rate': 1e-2,\n",
    "    },\n",
    "    verbose=True, print_every=10\n",
    ")\n",
    "tic = time.time()\n",
    "solver.train()\n",
    "print(f\"Time: {time.time()-tic}\")\n",
    "best_model = model\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAHgCAYAAADg78rsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOy9fZgU9Znv/b27p4AeVGZ4iZEGhCQGI0GZiEqWJKtmH/El6qwmEqPZ7G728WST3Y2sD2cx6xMwxz2yh7OJybVJvMzLk5ONMaCYWdTsYrKSmMVgApkZEAWVqEBDVhQGlWmgZ+Z+/qiunuruquqq7uqu6p7v57rmmpmq6upf/bq66lv3q6gqCCGEEEJIPEhEPQBCCCGEEDIKxRkhhBBCSIygOCOEEEIIiREUZ4QQQgghMYLijBBCCCEkRlCcEUIIIYTEiLaoBxAWU6dO1dmzZ0c9DEIIIYSQimzbtu01VZ3mtK5lxNns2bOxdevWqIdBCCGEEFIREXnFbR3dmoQQQgghMYLijBBCCCEkRlCcEUIIIYTECIozQgghhJAY0TIJAfXmjp4duH/LXlht4ieOS+If/ng+urvSkY6LEEIIIa0FxZkP7ujZgR9s2Vu07NjJYdy6tg8AKNAIIYQQEhp0a/rggaf3ua67/eHtDRwJIYQQQlodijMfDKu6rsvmRho4EkIIIYS0OhRnPkiKeK7v6c00aCSEEEIIaXUoznxw40UzPdev2bi7QSMhhBBCSKtDceaDu7rn46y3TXRdnxnINnA0hBBCCGllKM588tO/vTjqIRBCCCFkDEBxFhKMOyOEEEJIGFCcBcArMeC2B/sp0AghhBBSMxRnAfBKDBgeUSYGEEIIIaRmGi7ORGSmiGwSkWdFZKeIfN5hm5tEZLuI7BCRp0TkvEaP04m7uud7rj/AxABCCCGE1EgU7ZuGANymqr8VkVMBbBORn6rqs7ZtXgLwh6p6RESuAHAfgIsiGGsZSRHXorTTO1INHg0hhBBCWo2GW85U9aCq/jb/95sAngOQLtnmKVU9kv93C4AZjR2lO26uzWRCsHzJ3AaPhhBCCCGtRqQxZyIyG0AXgKc9Nvs0gH9rxHj8cFf3fNy8aFbRMgFw44Uz2QCdEEIIITUTmTgTkVMArAdwq6q+4bLNJTDF2d+5rL9FRLaKyNZDhw7Vb7AlLDxzMlJGsvC/Ali/LcNsTUIIIYTUTCTiTEQMmMLsflV92GWbcwF8G8C1qvq60zaqep+qLlTVhdOmTavfgEtYs3E3srnhomXZ3DCzNQkhhBBSM1FkawqA7wB4TlW/7LLNLAAPA/ikqj7fyPH5wa1dE9s4EUIIIaRWosjWXAzgkwB2iEhfftkXAMwCAFW9F8AXAUwB8A1Ty2FIVRdGMNYyvFyX7iVqCSGEEEL80XBxpqr/iQo6RlX/AsBfNGZEwfByXSpM8cbEAEIIIYRUCzsEBKRSodnb1rGNEyGEEEKqh+IsIJUKzQ6r4vaHd1CgEUIIIaQqKM4CsnzJ3KIyGk4wc5MQQggh1UJxFpDurjTuvm4+OlKG53bss0kIIYSQaqA4q4LurjT6Vl6Gxe+c7LoN+2wSQgghpBoozqqkpzeDzXsOu65nn01CCCGEVAPFWZUwpowQQggh9SCKIrQtQaWYsuUP9uPOR3ZiYDCH6R0pLF8yl/XPCCGEEFIRWs6qpFJMWW5EcWQwB4XZ1mnZ2j7MXvEYFq9+gmU2CCGEEOIKxVmVXHJ2sEbrmv+dGciyDhohhBBCXKE4q5JNuw5V/dpsbhi3revHHFrSCCGEEFICY86qpNY6ZsNq2tIsSxoAxqQRQgghhJazagmzjhk7ChBCCCHEguKsSvy0cQoCOwoQQgghBIhAnInITBHZJCLPishOEfm8wzYiIl8TkRdFZLuIvK/R46yE1cYpKeK5nQBoN0anOeGyOTsKEEIIIQSIxnI2BOA2VT0HwCIAnxORc0q2uQLAWfmfWwB8s7FD9Ed3Vxojqp7bKIDB3Ejh/xGHzVNGkh0FCCGEEAIgAnGmqgdV9bf5v98E8ByA0kj4awF8X022AOgQkTMaPFRfVGvxsgxo6Y4U7r5uPpMBCCGEEAIg4pgzEZkNoAvA0yWr0gD22f7fj3IBFwuqtXgpTGG2ecWlFGaEEEIIKRCZOBORUwCsB3Crqr5R5T5uEZGtIrL10KHq645FBZMACCGEEFJKJOJMRAyYwux+VX3YYZMMgJm2/2fklxWhqvep6kJVXThtWrCK/WFRSwkMJgEQQgghpJSGF6EVEQHwHQDPqeqXXTbbAOCvRORHAC4CcFRVDzZqjEGo1volMFtA9fRmsGrDTgxkcwCAznYDK6+eR1cnIYQQMkaJokPAYgCfBLBDRPryy74AYBYAqOq9AH4C4EoALwIYBPBnEYzTF9M7UshUIdAUwNpf78MPtuwtWn5kMIflD/UDYMcAQgghZCzScHGmqv+J0WRFt20UwOcaM6LaWL5kLpY/2I+cU42MCri9JjesWLNxN8UZIYQQMgZhb80asQTUrWv7KmwZDCYLEEIIIWMTtm8Kge6uNNIhB/czWYAQQggZm1CchUTYvTbZMYAQQggZm9CtGRKWe9OeeVktKSNRMd6spzeDNRt348BAFtM7Uli+ZC5j1AghhJAWgOIsRLq70ujuSpeVxwjKcVsvzlIRdsnZ0/Bo/8GifWcGsrj94R2FMRBCCCGkeaE4qwOWSLvpW7/C5j2HA7/eijfr6c3g9od3IJsbBmCKsNLSGxbZ3LBjhqeThQ0ArW6EEEJITKE4qyP3/9/vx7wv/juOnRz2/RoBigSUJcz8UJrh6STulj/YD4hZrsNaRqsbIYQQEh+YEFBn/uGP53sXdStBMSqSgpbTKM3wdBJ3uREtCDMLy+pGCCGEkOih5azOdHel8eDWvb7dmwJg8eoncGAgi4QIhtVfcduUkSzL8Awi7lhXjRBCCIkHtJw1gJdf9y98FKarUQHfwqyz3cDd180vc0sGqZXGumqEEEJIPKA4qxM9vRksuPNxzF7xWFW9N51oNxJIJoqdpEZSXBul+6295mR1I4QQQkg0UJzVgZ7eDJY/2F9zvbNSTgwphkv6cVp9OJ3o7krj7uvme3YvSIo4Wt0IIYQQEg0UZ3VgzcbdVTVCr4Sbm7OWeLERVQozQgghJEZQnNWBegXXJ8U571NhJhH09GaKllulNLzcqow1I4QQQuJFJOJMRL4rIq+KyDMu6yeJyCMi0i8iO0Xkzxo9xlrwI3gSAty8aJbvhukpI4kbL5rpGkNm1SuzC7RKddIYa0YIIYTEj6gsZ98DcLnH+s8BeFZVzwNwMYB/EpFxDRhXKCxfMhdGwru62YgCm3YdwiVnT6u4Pysb867u+Z4xZKX1yrwseOmOVOixZj29GSxe/QTmrHjM0ZJHCCGEkMpEUudMVZ8UkdlemwA4VUQEwCkADgMYasDQQsFvE/TMQBbrt1UWMMdzI9j6yuFCy6VJKcN1W7sgm96RcnRpJkUKLZsqtXealDIgAgwM5jxbPTl1I2DnAUIIISQ4oj5raYX+xqY4e1RV3+uw7lQAGwCcDeBUAEtV9TGv/S1cuFC3bt1ah5HWxuLVT4RSSkNgKtZKdKQM9K28DEC5YLKTMpK4/vw01m/LFK03ElLU3snpdU4WN7fjTHeksHnFpT5GTgghhIwdRGSbqi50WhfXDgFLAPQBuBTAOwH8VER+qapv2DcSkVsA3AIAs2bNavgg/bB8yVxXgRQEvxLanjNgCajb1vWXZXpmc8N44Ol9ZcsrZZnaXad2i5ubAGXnAUIIISQYcc3W/DMAD6vJiwBegmlFK0JV71PVhaq6cNq0yrFbUeCn1liYDAwWu1G7u9IYcbGO+u1AUIrlsrQ6GWQGsq79Q0uTIxiXRgghhHgTV8vZXgAfBvBLETkdwFwAv4t2SNXT3ZUuxHeVWtH8uiv9YokheyxZkB6dfkiKlFkCFeXHYs8G7enNlMXgMS6NEEIIKSeqUhoPAPgVgLkisl9EPi0inxGRz+Q3+R8A/kBEdgD4DwB/p6qvRTHWMLFb0QRmPNZNi2b5arHkl0vOnlZU3yxIj04LI+meaZoykq77U6Do2KzYNGs8TskRpRmmhBBCyFgnsoSAsIlrQoAfenozuHVtXyj7MhKmlayWBgX3LF3gmGkqAG5aNAubdh0KFPxfKSlCALy0+qrqB0wIIYQ0GV4JAXGNORtTdHelQ4tJy43UJszSHSl0d6UxcXy5x1th1mZza6ieGcg6xpFVSgpglwJCCCFkFIqzmLB8yVxPd2IjxwG4C6oDA1l0d6XxvlmTHNc7dSrwEl/sUkAIIYQUQ3EWJyL2MHekjEJgfke7c6HbjnYDd/TswOY9h133UxpH5mZpszofMBmAEEIIGSWu2ZpjjjUbd1esMVZPUkYSq66ZB8CMgSstyWFxPDeM+7fsrbg/u+XNEl+lnQgoygghhJByKM5iQtTFWi1r19ZXDmP9toyrES+bG/G1v1JXplVOpJE4taaiICSEEBJ3KM5igleV/UaRGcji/i17a/auGknxHUdWLwHFXp+EEEKaFcacxQS3uKxGE4ZjdeI4U/NX6gRQWo/NKZmgWtZs3F1WKJc11QghhDQDFGcxwSpQm5ToMzZrZSCb8yW66imgvLJNCSGEkDhDcRYjvPpgOpEyGvPxBZWLTu2dnERXPQWUW/kO1lQjhBASdyjOYoZf8ZDuSGHyxPF1Ho35Pn/wzsm+BZrAvV3UgYFsUePzhIuV0M8cVGqg7uQmZk01QgghzQDFWczwE3tmBdzX20WXMpKYPSWFp/Yc9h2LpjDrlzkxKWVU7PnpR0D5iVVz6mPKmmqEEEKaAfbWjCH2DMZJKQMnh4YxmC9h0dluYOXV89Ddla7Ys7IS6Y4UBk8O4YhLTbNqaDcSyA1rWc02Iyk4ZXyb53ulfWZruh23W29PC5bWIIQQEhe8emuylEYM8VsTbPmSuVj+YH9VxWsFwOYVl5aVnKiV7NAInPT+0LB6CjO7xWzx6ic8BVQ1sWosrUEIIaRZoFuzienuSuOUCdXp64QI5qx4DGs27sb156dDyxJ1M8RWko/Z3DBWbdjpK8uzmmB/ltYghBDSLERiOROR7wL4CIBXVfW9LttcDOAeAAaA11T1Dxs3wnjgxw3n1mapEla8V2Ygi/XbMrjxoplY++t9kbaQGsiWH4sl2uzzcMnZ07B+W6ZIbFWKVWNpDUIIIc1CVG7N7wH4ZwDfd1opIh0AvgHgclXdKyJva+DYYoFfN1wYnQWyuWE82n8wUM2MjpSBYyeHkBuuv5gbyOYKws0Sk9efn8aj/QcLyyfYyopYojYzkEVSBMOqhd+lWNY2p9f4jYEjhBBCwiSyhAARmQ3gUSfLmYh8FsB0Vb3D7/5aKSEA8B/03tObwa1r+xo5tAJGQkKztKWMJCYYCd/JCWaNt+J6akZCYCSlkDzh5z3vvm4+ALjG3VnbUKARQggJE6+EgLjGnL0bQKeI/FxEtonInzhtJCK3iMhWEdl66NChBg+xvvh1w3V3pTFxXH3bPrkZ1HIjGkqsWme7gbuvm4+VV8/z3cIqmxspE1O5EfUtzKz37O5KO8ajjb4P49IIIYQ0lrhma7YBOB/AhwGkAPxKRLao6vP2jVT1PgD3AablrOGjrCNu7srSoPee3gxODvkTJNXiNbHDqjVb0I4M5rD1lcO4q9u0YlnxZfX8QI/bRFyluDO/cWks1UEIISQM4mo52w9go6oeU9XXADwJ4LyIx9RQ/Fa4X7Nxd6RB/OmOFNZ87LyaW0ndv2Uv7ujZUSRu3IrZhoHdIlapI4HfjgX1auJOCCFkbFHTHVVEPi8ip4nJd0TktyJyWQjj+lcAHxCRNhFpB3ARgOdC2G/T4LfCfdTZhpecPQ2rNuxE1qc70Q0F8IMte4vETZjFcZ2w5s6rK4Pflk9Rl+qo1M6qWd+LEELGIrW6Nf9cVb8qIksAdAL4JIB/AfC414tE5AEAFwOYKiL7AayEWTIDqnqvqj4nIv8OYDuAEQDfVtVnahxr0+GnGG212ZpJEYyoYnpHCgODJ3HsZPAitEYCZSUtmgmFmXixfMlc3H3d/IrZml5uyyhLdTSywC6L+RJCSP2pKVtTRLar6rki8lUAP1fVH4tIr6p2hTdEf7RatqYXpe2dqilpcfOiWVh45uSCIGlVUkYS15+f9hSRfjIynTopCEyB59UGq1JLqTCotp1V3N+LEEJamXq2b9omIo8DmAPgdhE5Faali9SJUpEwkM3BSAg62w0MDOYKRVrtNcCcWL9tP+7fsreuQfdRY7d8eQlRp0K3pcH8Tm5La+4yA9lCGQ+7SPbrEq2VRlrtWMw3XJhEQghxolZx9mkACwD8TlUHRWQygD+rfVjEDSeRkBtRtI9rQ+8XR8P97uqe71kDrdYYsVoRcW/1VPG1qNwOyhJG1o3OchHPWfGY42tLC93eurYPdz6ys9BkvpL4yI0oOlIGJo5vK7vR1vsG7Dezt9neq9Whi5gQ4kat4uz9APpU9ZiI3AzgfQC+WvuwiBtBLBdWDa84ui2rFWZulf5LsYLxS29yQWL0jgzmCjdLP687ms2hb2VxPky9bsClru1GWe2WL5lb5t5tlIWw1fBKIqE4I2RsU6s4+yaA80TkPAC3Afg2zJZMY64PZqMIarlwupkGZVxScLIBbZr84EeYWTgJ1qDzkc0N49a1fehwEECl2D8Dezsop33etq4fy9b2VWVhc3JtA0BCgBFFXdtOWfv0Gme9LIWt5gKki5gQ4kat4mxIVVVErgXwz6r6HRH5dBgDI84EtVxYN69VG3aW3cT98vw/XOnqDgTMtkkXzunE5j2H/e+0SvxazgBnweokLtyC+e3YY/uODObKXKv2z8ApeaAUe+P52x/ega2vHC5KWvCysLl1NBjRcnduPfDKIq6npbDVXIB0ERNC3KhVnL0pIrfDLKHxQRFJIF8Sg9QHP5YLJ07YuggEEWZJEfT0ZjzderkRbYgwA4JZzpYvmeurobkfMQUUx/Z5WXG82kE5kc0N4wdb9jout7u4vKxx9tfctq4fAMpe1wiLU71cdfV0AUZlkaOLmBDiRq3ibCmAT8Csd/Z7EZkFYE3twyJe+Kl/ZieoWLAzrIrbH95RsRxFo/BrOetImc8I9ptfqbUKGBUwQ8P+jstyOXl9BmG6pax9+RWQgHmcVkLDVeee4dsiFxQnUVMvV1299hulRa7aBy1CSOtTkzjLC7L7AVwgIh8B8GtV/X44QyNhUesNLJsbxqZdh3D3dfNdsz8bhR9hlkwIVl0zz3dD89sf3gG/yat+XE4deden49gCuGXt71eNwD4ymHMslxKGxclN1Lgde62uunq5AKMOyg/6oEUIGRvU2r7pBgC/BvAxADcAeFpEPhrGwEh4hBHDcmAgi+6uNNJNEA8zPDJqIfMiM5DFsnV9vkWPABVdTj29GQy4CLOOlIF/uuE8GEnx9X52F1e1AttNBtYq2N1EzfHcsK+esG64tYby22s2KAzKJ4TEkVobn/89gAtU9VOq+icALgTw/9Y+LBImbje2II3FLYHn1YcyTvi18Pk1YgmAP3inWcjWraekZU1y2+XRfELGsI/M16RIUdeCsIPEa92fm3jJ5kaQzQ0jKaYAdesJ64RX83i/vWaD4jYPQeaHvUYJIWFTa8xZQlVftf3/OmoXfCRk3GJbAPiKY0oZSVxy9jQsXv1Eoa7WBCNRdWPypAgUGigxIWoUwFN7Dhd1BSiNTarkepzekcKajbsrttAwkoI1Hz2vSHhccva0qjs6eGWWVkulum/DqhUzR0tj1gZPDnm6GOvhAqw1KL8Vs0gJIdFTa2/NNQDOBfBAftFSANtV9e9CGFsgxlJvzTDp6c3gtnX9rnFQ6Xw7qNJkAKsfJVBcpqMSAuArSxdg6yuHHTMU/SIA2sclq2rYHib2npJe5UYAFMpweDEuKZg4vq3QiiuIiHbC6i26adehUIPOvbpP2HHruXlHzw7fYtM6Zypl3VZLLdmazdZrtNVqxRHSzHj11qxJnOV3fj2Axfl/f6mqP65ph1VCcVY9TpmA9mbgfm9AbttZCICbFs3CXd2mqLujZ0dNAq0jZeCN47lILXAC4KXVVwGofPy1vEeQQ7Rv324kMN5IFom9sG7GXV96vKLYtM+PRU9vBsvW9vk+po6UgRNDI47i1E/T+nriJsidjrsawhRTlb7nhJDG4iXOanZBqup6Vf3b/E8kwozURqV4Hr9B007xaFboe7ojhZsWzcKmXYcKsTkLz5wcKO6tlKPZHL58w4JC2YwosGKTenozOHzsRF3eI4gwMxKCNlvCwWBuBEcGc2UxXGGw8up5FbeZ3pEqi8m685Gdvo8pZSQhAl9Zt1EQRsyaG14xeNXglZlKCIkXVYkzEXlTRN5w+HlTRN7w8frvisirIvJMhe0uEJEhZoDWn+6uNDavuBQvrb4Km1dcWvQk7fcG5CTyvrJ0AV5efRWWL5mL9dsyRTeaZWv7qo5bs96/uyuNvpWX4eZFsypu7zNJ0jdW9mZPbwbLH+yPrJl8UqQw36dMaPNsMRXkZtzTm8GCOx/H7BWPYfaKx9D1pceLhEF3V9pTXFuxiqUCw+szF5juX/tDglv2q0WUmZX1yiIFwhdTzEwlpHmoKiFAVU+t8X2/B+CfYfbhdEREkgD+EcDjNb4XqZEgQdNuQdtONxq7hAjquitNUpjekcLid04uCtovJez2oApg2do+JALWLguT0pgyP6PwczN2cjkfGcxh+UPF3QdWXj3PMR6us93Ayqu9a805oQCO50bwlaULihItvNzFHRWsr2G7Bkv3dfd18+sSxxW2mGK7KEKah0gyK1X1SQCV+v38NYD1AF6tsB2pM2GUMah0Q9H8fu9ZusBXqQ6r5ZHdIvPbvUfxlaULELKBzBNFsJZSYWAvU2F1bsj4FGaA983Yspa5xQLmhrXIcuN0btyzdAF6v3gZurvSVQmJUutQpfItXtMfpmvQbV8AXK3OtRCmy7SnN4NjJ4bKlrNdFCHxpNZSGnVBRNIA/hjAJQAu8NjuFgC3AMCsWZXdWqR6ai1jUKn0AjBa6BaAZwapG9ZN3atCfyvw9kkTCtaZxaufCJzFmRnIYvHqJ8osPH5bRGUGsuj60uNFSQZumYlun3tHysDE8W2u50SpqBvflnAd11GPTOEwOgB49TStZzeBsHpvun2ulnUzjLEzC5SQcImlOANwD4C/U9UREXc7iKreB+A+wMzWbNDYSBU43WhKSeSbrFsX9WrKR2QGsg21nEWB3WJTrYurmjptdizxmxnIYvmD/bjzkZ2OGaFuAmPVNfM8M4HtiRaVzgMvS1Il12AlUeHn/YN8BkFETFi9N90+1/ZxbaEJM9Z6IyRc4irOFgL4UV6YTQVwpYgMqWpPtMMi1eLHImY1Wbdvb92YgijvsaDSLYuNH4uk1z5uW9ePZWv7atpPbkSLxJr9M6wkMC45e5qjC3Vg8GRByHgJo0qWJK84KyuRI2dr97X8weKYOj+C1a+bMYiIKRVx9hi8oNQ7ESDq/qSEtCKxFGeqOsf6W0S+B+BRCrPmx49FzK0i/OwVjzVsnM1CGFZCSyhb+wpD2JbemL1c4pt2HXJcfuzksC/L6QTDO2zWyzX4hYe3F4SZRW5EsWrDzoplZEr35Qe/IiZsS1S9EwGYBdr60G3deCIRZyLyAICLAUwVkf0AVgIwAEBV741iTKQx+MnAs1/UrYsCccaPmHp59VW+CuQqgmfNuuF1Y7Zf6L3ey4+L9chgzlO4eLUuG3QpfWLvduFlUQzaocCviAnbElVN7FqQm3GzZoFScPiDbutoiEScqeqNAbb90zoOhUSAZUkJI97IqrPVzAkACUHduxz4ifkDTGFmtZmqRai53Zj9Jh0EoZJwcbLcLV79hK99uwmbaqrq+xUxfkScX2Fhdw37bXsV9GYcVuJCI6Hg8A/d1tHAJuWkYZRWir/k7GmeBTwrxftY2WYrr54HI+HPwRfHZIF6VuKwuidYJS/8dFM4MphDZ7uBmxbNKpTI6EgZheKw1t+A83x63ZiD1j3zS1AXmtf29sK6YZSRsbjk7Gll8+U0V5VKaPgtD2LfDvDXjB7wV/zW/l1es3E3rj8/HcocNQp2S/AP3dbREMuYM9J6OD2prt+W8WzKXckNZ7m0rj8/7am6LAuQWxP3KEkZCZwc0rrUSjMSglXXzCuysiQ8sp/tHBnMYf22TKG5vZNb0MkCZvbyTODWtX2F5I+07TVBkg6CWO6CutC83JWlbancYubu6NmBB57eh2FVJEVw40UzC31j7XM+KWXg5NBwmRtVAFx/fvm+K1mi/FoyqrV4+Mlwdfoux12Q2aHg8E+zuq2bHYoz0hDcbhSbdh0q1MiybmhW9qAfd182N1y4QTphd+E4uXhE6mu5qkS9Wj4lRbD0wpkAikVUEBGYzQ1j1YadOHZiqCyj8ZQJbY4CdzA3UhAh9mSD5Q/2BzJbdrYbuOrcM8qEtJEQQFDUosrJ+lTJ7efm5p04Lolla/uwZuNuT1fh7Q9vL/rshlXxgy178dKht7DzwJtFcWsDLnXYFM4JEZUyXP0Ki2oFiNvNeFLe6trsbq6e3oxrVw8KjnKa0W3dClCckYZQzdO4X9wEhwBFwq9UpFitj9b+Zp9nP8pmZFgV67dl8Nj2gzVZCZ2Ehb10hl9KsyK9uHnRLCw8c7JrrBTgLFzsxWLtVjeneKJSATQpZeDYySEcO+kdgzR6HjmL6s17KjU+Kcbte2G31pU+tLgVWS4VFtVaPJYvmVtUYsTi2Mkh9PRmmtrqZH1+TteMOAiOOCYphFVvjwSD4ow0hEo3ilpikZI+noK9LHdrPnpe2U26FcRaNjccG/dtEB7tP1hkMXOKlXKqC2YX36Wfnj2eyOkms3j1E2VCtNQa1NObqapzhReVhJLTQ4tToLCTsKjW4tHdlcadj+wsE4BW6664uLmqETJe1xknF3MjiXOSQq0dYkhwmBBAGoJTf0T7jaLap24BcONFMz337bX/TL5llNUbsW/lZbhwdmdVYyHhMJDNBQ7W9iPuMwNZLH+ovyiQ/ta1fZi94rGKZRLwJvcAACAASURBVF28LC7V4kcoOR1Xqc3OLXatlmSGARfL6IGBbMXvskVpAlA1/UzdcEqKWJb/LL3ey+s6s35bJtQxBoVJCsQOLWekIVQyjVdToV4A3LRoFu7qnl9wg7k9RXv127S3jOrpzeCpgK6pONORMopixpoZqydoZiBb5ur0K+6DWkTDsOw64bevpZ/jUgA/2LIXm3YdKjvvq7V4uH0frYSSu6+bH6jtlSWE73xkZyj9PJ0+Dy83dqXjAqKPm6vGXRxHNygJB9Eoo6FDZOHChbp169aoh0GqxE/9K6tZdjUXogV3Pu4amJ3uSBVi0/wUa60XVgKEm5u2GtqNBE4MK4ZbQJy5kTKSSAgK8WJh7teyNM1Z8VgoxXmDFq4Nej7aM5NruVHf0bPDsa0W4K/Om9e4q60TB3g3oS/F6XoBeHcoEQAvrb4q8LjCwG3O7NcnO07XzFrmljQeEdmmqgud1tGtSWKB3QUDlCf2Wc2yLffj5hWXBroAHXURZkDxk2mUQc3j25K4Z+kC7Ln7SiR9lryw05EyyuqYDeZGIhVmNy+aVeYCC5tsbjhUYebkAnSLpwryOVk32SDnrZML0YtS61G1brrHth90XefH1eb1ParWVVdat60SA9lcWS04wLT6uX1uUWZrBnEXL7jzcdy6tq9hbtB6uqiJM3RrktjglKFWjZXM6bVe7gz7BXlSynC1sNUbu1ulGsvZ0WwO0ztSkY3fneax2iVFsOfuK4uW9fRmcPjYCcftb7xopquFyU7pTdbv+e2WVerHPVspCcKNnt5MxWzcastxlL4+yPe8VteyNR+WFSpu5SH8ZEX29GYcM2nthP2AGedEhVaG4ozEkmpjZdwuJNefn8baX+8ru6gZSSm6IFdhsAoV68Kadrm5eRVmnZQyYlfOwI9wiROlorhS6YxNuw4V2l25kRQpssL5udl5iZYgrj1r30Hea/DkUMX9VrIwXXL2NM/PfnpHKvBNP4xz29pHXMtDVLrurdm4u2L8aNjWv2ava9esUJyRpsXpBuZZMuNj52HVhp0Fy5JTULZbllqjsC6sbkVSO9oNTD1lHF549VjZa4+dHPJMfCCVSZfc2CpZaw4MZHHTolmeQmREtawOWyl2K5f9HAXKg+ntVOqikBTxvLH29Gaw/KH+giXOj+CzW5icvoMAsPbX+yq+3u27etu6/kJNt1qThkpp9iKzlY6/Hta/Zq5r18xQnJGmxO2p2+1GeiBfMqPSk14tNwDrRlltw/DSC+v4tkTZ8RwZzLmKr9ywQtWsot8K2ZmNxkl0VDoXJqUMrN/mHX8zKWV4JqRYVDqHjwzmCp0WLDFlP99Kz7uUkXTdl5X5OjB4MlAGa2nHDafv4AQj4Xr+2a2Iy9b2OW5j7yxht6Q5PbC4fdc6UgZODI24ui2rcdXFITPSK1mo1EIbFnGpazfWYEIAaUrcnrprDfQNGoBtR0t+B8EegG7dOKqJHRvI5uLZ3T3GlCYA3NGzA8vW9vmyUoigYhzUQDbn+7OstK/ciJaJKYV5Y7Z+A6PHY2/iXkpmIOs7kSJlJHHzolkAgGVr+7B49RO485Gdjt9BL8utZUUETCtwJawWYotXP4Fla/swvi2Bznaj8Jnd5JBwYiUP2ROMLAvimo27i9q4lb6XWzC9n2bzjQia94pF/acbzquLWPSbqEDCJRLLmYh8F8BHALyqqu91WH8TgL+Ded18E8Bfqmp/Y0dJ4oybSd2qJl9toK91cSt1LdULp9T3WgOfW6G7QSOxl07o6c3g/i17KwpsyyXuZv1pNNZNu7SbwqoNO6veZ7ojVbASXXL2tKKuDdVal62HpJ7eDN46Xjm2DSgWtwPZHFJGEl9ZuqDwIPNo/2iLMqdQhaAWdifcxNyqDTt9twwLA7f4xpSRKGrxFaZVL67xea1OVG7N7wH4ZwDfd1n/EoA/VNUjInIFgPsAXNSgsZEmwM3UnrbFnlV7IbHcn06B0mHEcyVFMKLqOjbGcjSWO3p24K7u+QDMG1AlYdaRMtD7xcsK20dVF88Ne0yZVwkZLzpSRlFtrcWrn/D9wNCRMvDG8RxKPZv25Bs/ge1u2C1cpULreEnihpeFPUjjc7fvpF04OrUMu22daVMIQ8i4CdqEAEMjWjgP6yEM2b6p8UQizlT1SRGZ7bH+Kdu/WwDMqPeYSHPh1TcwrAtJ6X6cij4KgPZxyUDuoUpxIWEEPgelI8ISIlFz/5a9WHjmZHR3pX0JY7vgqZSVGBXWcVSTIGIkBKuuKU488PvAkDKS+Mh5Z2Dtr/dhpET8LL1gJoBwCj0fGMj6yiIMYmEHTGHT9aXHy6xv1X4nh1Vx69o+/O26PoxobcWB3QStotxazmzK5qcZYs4+DeDfnFaIyC0islVEth46dKjBwyJRUkvfwDDf8ytLF+Af/ni+rzi1znbD1xhriXurho6Ugb6Vl+Hl1VfhnqULGvrecUCBQl9GP7Ycu3XFq1hrlFilKvy6Du3n9JqPlccuuVmUSgsfnxgaxg+27HUUEY/2H6xYRNZvKZvpeZerE/blbuO2rhelRZuBfOLFQ/1FMWO1fiet6ailOLDb8bqFoQWxwLPIbPyIrH1T3nL2qFPMmW2bSwB8A8AHVPV1r/2xfRNxolEZVqXvc8nZ07Bp16Gq37enN4NbfcQzdbYbODk0UlOF/ISYF3h7KQSnOJo4I0DRvNfT8mjFNQHw9RlFgZUI4NdqVtqrtPRcdbMav+ttE/Hiq8dCOUe8sktLuWfpgoouZeuYnLJYrYckLyteadskewZvrd8Lt5ZMXriN1c1F6/c9graBikPWaqvg1b4ptuJMRM4F8GMAV6jq85X2R3FGSmn23nN+bhx+KoYHwZofwLsHYVzpbDdw1bln+Arqr4WUkcQEI9GSNeXs3xH7jXiCkXAtxlsrluD1G8Nnfc72JAU/lIpPr56pbn02w3DLVtPD0+16dv356bJ5CHKdC9LT02sMfh5G4yLs7EK70oNJPfESZ7GscyYiswA8DOCTfoQZIU40e2Vrt3imZKJyYLWRAEbUvOgkRXDjRTPx2PaDFcWEPdi62YQZYFqKGhEDls0NN+X8+MEt4L4WYVZJzL6RHcKytX2YlDJgJKVixvGRwRzWb8vg+vPTeODpfb7anXW2G2Viw29bN7uoCEP0J0QwZ8VjvgWKvfyHk5hYeObkim2f3NYHKTLrdk21Pwy5JSSUPkhmBrJm3T40trZcqcB0q6sXNZFYzkTkAQAXA5gK4L8ArARgAICq3isi3wZwPYBX8i8ZclOXFrSckVLcnoqreWqNArcnWitGDAh2jH6tbFbYTyOvDIn8m7Zi7dxkQiJtPl8tbi3EqtnPJWdPw6P9B30lnRgJgZEUDPoQg1a5D7+zay8PYj3geH0nrLEHtdAFxakEiEWtbken8RsJwSkT2jAwmEMigFvUy9JYSkfKwMTxbYVxHDl2wvEztV/Pajluv1SyfFbjcq4WL8tZJAkBqnqjqp6hqoaqzlDV76jqvap6b379X6hqp6ouyP94CjNCnHALBm6WytZuT7T2bEG3Qp6ly60Lth/35/SOlOscWUVOneK2O9sN3LN0AV4OKHwFwJdvWIBPXDSrIfVzJ44LJ+GhI2X4uoAmALQbzZB7VUxYcXuZgSx+sGWv72zg3Iiic+J43LN0QcUEgcxA1lcxW/v29iKyALDmY+c5JgbYx15vK+mRwRxuXduHri89XhaM7+UBKA3kv6NnR1mx3Psdxp8bURwZzEHhXNjWrTZkkGvnQDZXNA43sT2QzTkmIwQtFOyXSokScSllFEu3JiFh4FVuoxnw0zbFzfB9ZNC84FnH6jd+zD4/bk+tbnFB7ePaCk+0QawuCmDrK4exflumIda6WpInLFJGEvOmn4rNew5X3DY3onjbaRMwPqQ6eWMBq93a1lcOV4wf9JuRWko2N4xb1/Yh3ZEyy3/8Zl/kBZyPDObKXGtuYsGpqb3TXAU9IstaaBW1nZQyIGL2HXZyO4eRNORUoy2Mnp5ObtFKZVHi8vAeWUJA2NCtSZyISwBqNfgx61dyMwQJXC/tm+jWJN6PKzVoooKIu9CMA3YXTbWB8fXIfPXqtdjMtBsJ/M/rzg3wUFG/ZAU3LGtbPeoD2gtVuxW/rsd3xo9LuXSb9nwv1TCFrdV2y2+ighNO189Kx9fohLHYuTUJaRTdXWlsXnEpXlp9FTavuLRphBngr5Zbpae8Sr0OLTrbjbKG1vabjr3yutt7TrK5hrq70jhlgn/DfJCbjJsLqhr81tWaN/1UAKa4qlaYBXG/+WUkHxheL6K6QQzmRnDr2j7f7sTjuRFPd7XfzzkIIsCqa+bBSIS/82HVgjvwreNDcHqLIN8ZvyPMjWjFWL/ciBZ9BwZzI4CiqOepV09XPxwYyLrWlhs8OeSrDpuTW9Tp+Ky5bUStzCBQnBESYyqJy7AK1lruFD9NoZcvmet4QzpWctEcqIMLz2pqfdbbJoayP1WzrVAlNu85XFMMlubfK+wCv17FWAHULNxGUF5oNo5MMBI4OeQuKvx+zkGwHnrWfOy8wgNUsg4qMDeiNSXKpIwkblo0K9TPsHQ4uRFF+7i2wnVq5dXzHJul37xoVtHDppuIm96RKjyclo7bKz7Pjl/35xmTUng5hg/vFGeENDF261qtWAKsUqyHm1UsN6xFwbphx27Yn2wHT4bowtLGBOwPZHPI5oZDS3qw4gO95jmMoP6j2RxWXVN+s40TJ4ZGKrrQh+sQT2bFR1kPUKUtq6LG+s4sPHMyTniI1zCwXzfcrP53dc/H5hWX4itLFwBwLpJsj3vt7kpj4nhnC7z9gdIJv9ef0utdXLolMCGAkCbH3qjdKUYtSLFUKzavUiKCm1XMfqFzSsgQAH/wzsl4+fVsoIbypXEmYWZU5UYUQw0sdVHLO1lxax35IO1b1/bVPcM1IYI7H9kZ67pufj4+L2mSkOrKuJTWTWx0X9xKcYwHBrK4dW1fQ2ITS8WQW49jp+uUhQC4/vzi13nNp1fdSqfrT6Vxl44tytpnFGeEtAjWxaM0AQLwn61pvaZSlqsfAec2Hj+tgewIzIvkgjsfL2SNudVmqpZ42TvcsYLA7fGA9R77sGrLZ5meNsEUu9UcZ2Ygi57eDLq70li+ZC6Wre1r2Pk0wUgAENfvjjWOeguzIFnwTmETFgrggaf34f4tews12vwIUCdKrz+TUgaOnRwqSlywj7unN4Nl6/rKYvmiKlzObE1CxgB+K5zfk3c3uGVq2vcXZoFI+/is1P0jg7mm6e1JmptaCy/bi7r6yeYNK7tUAHxl6YJIerzaM0qDZMEHKWRbzTgA9+tX6XXm5NCwr2LH9Spc3nTtmwgh4WJ3MXh1HgDKrWzHHS5efqxid/TsKLTWsVpI3dU9v+L4LPz0MGzVUhIkfIyEuMalWRbfal2SVlFXoHI2b1IktLIfk1IGurvSuG1df8O/ByOqRYLFqWwRgLIelmF/Z+3tl5Y/aM6D/WM+MpjD8oeK20TZxZsfoqh9RssZIWMML6uXW4HZoC1N7ujZ4djj8uZFs1wFWil+nrCjaDXlhV9LX5AWRaR2rBp+W1857HheThyXxLGTw01rqY2izpv1wLVp1yFkBrKxn7u0S8iGH+5ZuqAubk3WOSOEFPCqnxZGVW7AjBsJstwJP0+rXq2mokCBwry6JYBOHJfEmo+dh2f/xxW4Z+mCQqZtI1pXxY1G3YAyA1nctq7fUZgBo10j6iEu6lFeo5RGCzPAtFj9YMvewsNctXMXtEyc9WATlEw+OSKoMEsZiUhKbNCtScgYxC2Tyk+gvx/c3BZB3BmVnnK9Wk0B/jPwEgK8c9pEvPDqMd9jc8OyMPb0ZrDMJQ6oo31cYe7tn4PlFmoGK0RYNFJSROX+ptvdm2RCMGl8m+8uJn6yu8NkQkQlZCjOCCEFwupH6hZXEsSK4JRtZWVrOsW4VZulOqLAiyEIM/s8rdm421VcZQayWLz6ibJYvVLBbBdrUdGed5f5kRcpIxnrchu1svidk/HUnsNjQjSXUs/Yztywv2xgKyRizorH6jION+pRTNsPkYgzEfkugI8AeFVV3+uwXgB8FcCVAAYB/Kmq/raxoyRk7OG3/EUlbrxopqML6caLZgYej5/39trOT5ZqrbeddMk8VXID2xs9L3+wOFjZwjqmsLPb/GA1v/bbjD4p4hmz2Aps+d2RMSnMrHjUKDJC7azflsHCMyc3vJZcVGETUVnOvgfgnwF832X9FQDOyv9cBOCb+d+EkDrjVxB5YQX9+83WrBd+slRrRYCiZIme3kygOmy5EcWqDTtd57yRNyN7OZTFq5/w2XS8uIRKI+t8NZKx6J5MyGhR2KAZjmFj1RsLEtRv9bSt1g1ajdcgLCIRZ6r6pIjM9tjkWgDfVzOVdIuIdIjIGap6sCEDJITUzF3d8xsuxryodFH3ivNKimCCkSgEjtuxNzS3MmGD3sgHsjlHV6efcYdFQop7qPpNArELs+6udOQWFhIeIzpqsfrIeWe4JlQ0igMD2SLrfqWHFkV19RIFqNprEBZxzdZMA7Cnde3PLyOE+CAu/eHihFsjZcDMAEt59NccUYWRdF5v12Fe1c8rkcm7Xq2WMdZn5pRdWw1JEdyzdIFrj0wrecLKavMTHui0SbXjC5q1RxpDNjeMVRt2Yv226K8hCRHMWfFYwYLm95QJavOMg400ruLMFyJyi4hsFZGthw4dino4hMQCy3rjdrMfy3R3pdG38rJCCQtBvviuwLPm2PSOlKtL56hteVg9P+0WLGvc9obR1TCsWhB6fhIz/GS6KlA0TsC09FXTJL2B7U1rxqtUSisykM3FItljWLXomma3WodN1NfNuJ5eGQD2yOEZ+WVFqOp9qrpQVRdOmzatYYMjJM44WW9Kb/ZjHUvsvLT6Kkwc31bUb6+UlJEs9Phzwh4wHGbwcKnQs4vuarAsWt1daYyEGD9VOk5LALYq6Y4UvrJ0AdqS0ZRYICbZ3HDdS2pEed2MqzjbAOBPxGQRgKOMNyPEH2EVkh0reM2LVaB3065Djq4OAYoChv1YjTpSRpGLstPl6b9U6Hm5TCvZwSyBabm6EyEWRnUSpN1daUf3sRcdKSPyQrwpI4l7li7wtCwuXzK3Jvd1HDHoU3YlqutmJOJMRB4A8CsAc0Vkv4h8WkQ+IyKfyW/yEwC/A/AigG8B+GwU4ySkGXGz3sShkn4cY+Hc5sXessrNWqUoLoFhjw8DykVTykhi1TXzCla7zSsuxcqr55UJOqcsMbebhNWUudRV29luFATg9eensX5bpuDqDivz0CubbdU183zf9K15idqzaVlKvObHq5NGM9LZbmDNx85zfUgY64ypUhqqemOF9Qrgcw0aDiEtRViFZMOmtKenFdMBlNf4aiRe83VHzw7c75Gh5hT87lT136tmnN/acpW6N3iVQPFbFiMIpbXdSgmSVWeVawi7Tlq6I4XZU1LYvOew79d4dWgQmH1jg5RKiTvH87GWURVbjZKkABDBsEvAowC45OxoQqbY+JyQFsSPKGg0bnXGgjZVrwdO8wV41+wqre/ViDG6NayvNIawC9kG/cwqvX9HykDfysscj7GWMS5fMhe3retvGSFVL5IiOC3lr4VSWKSMBI777D4RJfX8nns1Pmf7JkJakDAKyYZNnGPhnOZr8eonPG8cjRRmQG3dG9ysbtX08DSSEtgKW6mQrpUJ62Zt86oz54QVY1dNzblqSIrgn244D91dacwO2F7ISIpnQkojGFbFW8eHGjYWS/BsfeUw7t+yN9YCzXJ1N/p6GteEAEJIixHnWDgnKiUKRCF+7Vmmm1dc6nsMTokKKSOJmxbN8pWcYNHZbmDNR88LfOxBymt0d6XLth9WxcmhERjJ8hi2znYDN5cch5XE4ccCF6Tfqxsj+TIlQelImfNZbW04O7UeR25EMTSsDYk9sx5s3BJt4kYUD5C0nBFCGkJcY+Hc8LI2NXLMQVzUbtv6tbrV4jr1otDaaV0fnAxZpYLAKRsyN6LoSBmYOL7N11ws8+hUYD+mMFyp9geMTp/tgqwkiEIniAf7kauy2FvKSBaSPko/u/fNmoQtvzuCYVUIgETCPcZKAbx1fKiqMfilI2VgzcbdNbX5qsbiWwtRPEBSnBFCGkJYTdUbhZOYFAA3LZoVWZyZVxJFpW2tH0vALVvbV6i0bm+/BFT/GXkJyYIIeai/yHVmJAVXnXtGUfsqNxfo0WwOfSsv8zUWt/1YTdpLx1Vt26lSN+/Kq+c5HuPSC2Zi065DjnOzZuNuR2FmJIChkcpCZIKRwMIzJ2PhmZOL5t9qXm+5dhWmu8xLhlYrEP3y5okhXz06ky4isrPdwFXnnlEmROtFVA+QTAgghBAXok6sCJJE4WfbelnGguy7dE4tAVEqgp3uTEESEYIeq9v8VcJICNZ87DzPYyw9b0rXh5Ghah0bMCqumz2rtN1IlHXuMBJSdwFpYY8lrAdeCQEUZ4QQElPcshyt2mZBt61nxmy1+/YriqoRkUFdwtW6N2sVjV4EceF1pAycGBppmQK5yQjFZSOysZmtSQghTUil2mZBt61nxmy1+/Za39luYGAwV7XVMkjWcpC6bKUcGMj6FoJBuwsEkSZ+3IVhkA7J2leJqIRZqes7CpitSQghMcUty9IpBsbPtvXMmK12317r28e1Bc5MrQUrG7aavMflD/UXOjB4Nc2OQ+mYWuhsN7B5xaV4ucRy20pUm30bJhRnhBASU+ztoOxlIpxuHH62DSL2glLtvr3WRyVkgopVBcrqg7k1zQ6y75SRdC1t4dQazG3bMMqFAGZiw8qr5xX+dysBYvWPbVbiUN6Hbk1CCIkxQV1zXtvWM2M26L7tbkAROJbYiOom6ZSpWw1O4rLSvpMiGFEt6lThlNhw/fnpsuxPp20Fpnuw1vithABLL5hZJvZL389ICETMY2+U+zMIlWL44lLeh+KMEELGEPXsHuF332VB8Q53yyhvktYx1Nr6yUlcWvtetWFnWYyYVxB6EEFtxc3ZhYjTcXgJlXFJwUmbNXBEgfXbMlh45uSi8UxKGZhgJDAwmMOklIFjJ4cKdd68+pSGSWnZDa+6b9efn8aj/Qdd4/MmGPFwKDJbkxBCSENxy9AstRpFHfdTSwann2y/epZq8ZpjqyCt090/KYIbLzJrsjm93ikj1DrWsBvX++XmRbMca8i5zW+lDOFG9c1ltiYhhJDY4BZLNqJaViIkylpzpa7aSSkDIsCRwVyZuDESglMmtAXKLq2nFdNrjt3cjfaSIHNceoQ6WZyyuWFHS6AfjIQAUh6zF4RH+w9i4vhyOeM2v5ViGaPqp2knEnEmIpcD+CqAJIBvq+rqkvWzAPwfAB35bVao6k8aPlBCCCGh47dESJAOCfXC7QYflWj0+75ec+yn7EnQ4rjVlvFwass1aHON+n1v6/39nCN+ji3qrNqGO1dFJAng6wCuAHAOgBtF5JySze4AsE5VuwB8HMA3GjtKQggh9cJvZqdTTTC3LMhGU20Tej/09GawePUTmLPiMSxe/UShJIclVv2U7PCaYz9lT9xeH7Qxup/XWMLqK0sXYPOKS7Hy6nll7x2EbG4Yt63rd5wXwPnYSpmUqn8DeC+isJxdCOBFVf0dAIjIjwBcC+BZ2zYK4LT835MAHGjoCAkhhNQNv5md9SyaG1e8rIVeYrV07irNsVMGqF0cu73e6bVeWC2lKr3GyeJVbb9TwEyAsO+v1OJoZbu6WdBCqj5SNVGIszSAfbb/9wO4qGSbVQAeF5G/BjARwB81ZmiEEEIagZ94qyAdEloFLwEWVKy6zbFfcez1GfkJ/k93pIpeXykurVRoVsr0bDcSOD40ArdWm3Yra6ngXb8tg7uvm49la/sc32MggFu1HsQjZ7ScGwF8T1VnALgSwL+ISNlYReQWEdkqIlsPHTrU8EESQgipH/UsmhtXvARYmB0eanHLWq/1KjTrZInrW3kZ7lm6wPN11vGv2bjbVZh1thu4edEsKMRVmNn35yV469k1oxaiEGcZADNt/8/IL7PzaQDrAEBVfwVgAoCppTtS1ftUdaGqLpw2bVqdhksIISQKgnRIaBW8xELcxKpb7FZnu+HZycJL2FnH7+W67v3iZdi065Av12qlBIi4zalFFG7N3wA4S0TmwBRlHwfwiZJt9gL4MIDvich7YIozmsYIIWSMUc9yE3HEqeq+JRZq6cJQj4zSWjpOeB0n4O7STvsQb6X7u/ORnY7Zn9NtbteoyrW40XBxpqpDIvJXADbCLJPxXVXdKSJfArBVVTcAuA3At0RkGUyX859qq1TLJYQQ0lCirJUWlEpiodouDPUqQ1KteK50nNWKN4vOdqPQB/St40Nl642kFPYVxwcAdggghBDSsjhV+W9UBfgocauCby80G3e8RHWl7g2VuhZ0pAz0rbysruOvBDsEEEIIGZMEKT/RSlRbhiROVkYvi5bd8uYkvipluB6tsmhuo4hrtiYhhBBSM2OxVhrgnVjgRpAit3HASi5wK0kWdoZrI6E4I4QQ0rI06825VqrJQoxzRwYvminD1S8UZ4QQQlqWZr0510o1ZUia1cro9Rk3azkWxpwRQghpWeJaKqERBM1CbNaODGFluMYJZmsSQgghZMxmtkYFszUJIYQQ4slYtjLGDYozQgghhABoThdgK8KEAEIIIYSQGEFxRgghhBASIyjOCCGEEEJiBMUZIYQQQkiMaJlSGiJyCMArDXirqQBea8D7NBOck3I4J+VwTsrhnJTDOSmHc1JOK8zJmao6zWlFy4izRiEiW93qkoxVOCflcE7K4ZyUwzkph3NSDueknFafE7o1CSGEEEJiBMUZIYQQQkiMoDgLzn1RDyCGcE7K4ZyUwzkph3NSDuekHM5JOS09J4w5I4QQQgiJEbScEUIIIYTECIozn4jI5SKyW0ReFJEViCHLUwAAIABJREFUUY+nUYjITBHZJCLPishOEfl8fvlkEfmpiLyQ/92ZXy4i8rX8PG0XkfdFewT1Q0SSItIrIo/m/58jIk/nj32tiIzLLx+f///F/PrZUY67XohIh4g8JCK7ROQ5EXn/WD9PRGRZ/nvzjIg8ICITxuJ5IiLfFZFXReQZ27LA54aIfCq//Qsi8qkojiUsXOZkTf77s11EfiwiHbZ1t+fnZLeILLEtb5l7k9Oc2NbdJiIqIlPz/7f2eaKq/KnwAyAJYA+AdwAYB6AfwDlRj6tBx34GgPfl/z4VwPMAzgHwvwCsyC9fAeAf839fCeDfAAiARQCejvoY6jg3fwvghwAezf+/DsDH83/fC+Av839/FsC9+b8/DmBt1GOv03z8HwB/kf97HICOsXyeAEgDeAlAynZ+/OlYPE8AfAjA+wA8Y1sW6NwAMBnA7/K/O/N/d0Z9bCHPyWUA2vJ//6NtTs7J33fGA5iTvx8lW+3e5DQn+eUzAWyEWct06lg4T2g588eFAF5U1d+p6kkAPwJwbcRjagiqelBVf5v/+00Az8G86VwL82aM/O/u/N/XAvi+mmwB0CEiZzR42HVHRGYAuArAt/P/C4BLATyU36R0Tqy5egjAh/PbtwwiMgnmhfU7AKCqJ1V1AGP8PAHQBiAlIm0A2gEcxBg8T1T1SQCHSxYHPTeWAPipqh5W1SMAfgrg8vqPvj44zYmqPq6qQ/l/twCYkf/7WgA/UtUTqvoSgBdh3pda6t7kcp4AwFcA/HcA9iD5lj5PKM78kQawz/b//vyyMUXezdIF4GkAp6vqwfyq3wM4Pf/3WJmre2BeLEby/08BMGC7sNqPuzAn+fVH89u3EnMAHALw/+Vdvd8WkYkYw+eJqmYA/G8Ae2GKsqMAtmFsnyd2gp4bLX/OlPDnMC1DwBieExG5FkBGVftLVrX0nFCcEV+IyCkA1gO4VVXfsK9T05Y8ZtJ+ReQjAF5V1W1RjyVGtMF0R3xTVbsAHIPpqiowBs+TTphP93MATAcwEU34BN8Ixtq5UQkR+XsAQwDuj3osUSIi7QC+AOCLUY+l0VCc+SMD0+dtMSO/bEwgIgZMYXa/qj6cX/xflhsq//vV/PKxMFeLAVwjIi/DdCNcCuCrMM3qbflt7MddmJP8+kkAXm/kgBvAfgD7VfXp/P8PwRRrY/k8+SMAL6nqIVXNAXgY5rkzls8TO0HPjbFwzkBE/hTARwDclBetwNidk3fCfLjpz19vZwD4rYi8HS0+JxRn/vgNgLPyWVbjYAbrboh4TA0hH/PyHQDPqeqXbas2ALCyYD4F4F9ty/8kn0mzCMBRm+uiJVDV21V1hqrOhnkuPKGqNwHYBOCj+c1K58Saq4/mt28pK4Gq/h7APhGZm1/0YQDPYgyfJzDdmYtEpD3/PbLmZMyeJyUEPTc2ArhMRDrzVsnL8staBhG5HGa4xDWqOmhbtQHAx/MZvXMAnAXg12jxe5Oq7lDVt6nq7Pz1dj/MBLXfo9XPk6gzEprlB2ZmyPMwM2P+PurxNPC4PwDT3bAdQF/+50qYsTD/AeAFAD8DMDm/vQD4en6edgBYGPUx1Hl+LsZotuY7YF4wXwTwIIDx+eUT8v+/mF//jqjHXae5WABga/5c6YGZKTWmzxMAdwLYBeAZAP8CM9tuzJ0nAB6AGXeXg3mD/XQ15wbMOKwX8z9/FvVx1WFOXoQZL2Vda++1bf/3+TnZDeAK2/KWuTc5zUnJ+pcxmq3Z0ucJOwQQQgghhMQIujUJIYQQQmIExRkhhBBCSIygOCOEEEIIiREUZ4QQQgghMYLijBBCCCEkRlCcEUJaChF5Kv97toh8IuR9f8HpvQghJExYSoMQ0pKIyMUA/h9V/UiA17TpaN9Lp/VvqeopYYyPEELcoOWMENJSiMhb+T9XA/igiPSJyDIRSYrIGhH5jYhsF5H/lt/+YhH5pYhsgFnBHyLSIyLbRGSniNySX7YaQCq/v/vt75WvUr5GRJ4RkR0istS275+LyEMisktE7s93CyCEEFfaKm9CCCFNyQrYLGd5kXVUVS8QkfEANovI4/lt3wfgvar6Uv7/P1fVwyKSAvAbEVmvqitE5K9UdYHDe10Hs0PCeQCm5l/zZH5dF4B5AA4A2Ayzv+Z/hn+4hJBWgZYzQshY4TKYvfj6ADwNs33QWfl1v7YJMwD4GxHpB7AFZhPls+DNBwA8oKrDqvpfAH4B4ALbvver6gjMljyzQzkaQkjLQssZIWSsIAD+WlWLmiDnY9OOlfz/RwDer6qDIvJzmH0vq+WE7e9h8LpLCKkALWeEkFblTQCn2v7fCOAvRcQAABF5t4hMdHjdJABH8sLsbACLbOty1utL+CWApfm4tmkAPgSzeTkhhASGT3CEkFZlO4DhvHvyewC+CtOl+Nt8UP4hAN0Or/t3AJ8RkecA7Ibp2rS4D8B2Efmtqt5kW/5jAO8H0A9AAfx3Vf19XtwRQkggWEqDEEIIISRG0K1JCCGEEBIjKM4IIYQQQmIExRkhhBBCSIygOCOEEEIIiREUZ4QQQgghMYLijBBCCCEkRlCcEUIIIYTECIozQgghhJAYQXFGCCGEEBIjWqZ909SpU3X27NlRD4MQQgghpCLbtm17TVWnOa1rGXE2e/ZsbN26NephEEIIIYRURERecVtHtyYhhBBCSIygOCOEEEIIiREUZ4QQQgghMaJlYs6cyOVy2L9/P44fPx71UOrOhAkTMGPGDBiGEfVQCCGEEFIDLS3O9u/fj1NPPRWzZ8+GiEQ9nLqhqnj99dexf/9+zJkzJ+rhEEKIM9vXAf/xJeDofmDSDODDXwTOvSHqURESO1rarXn8+HFMmTKlpYUZAIgIpkyZMiYshISQJmX7OuCRvwGO7gOg5u9H/sZcTggpoqXFGYCWF2YWY+U4CSFNyLHXgI1fAHLZ4uW5rGlJI4QU0dJuzTgwMDCAH/7wh/jsZz8b6HVXXnklfvjDH6Kjo6NOIyOEkBBRBd44ABzaBbz2vPn7UP539rD7647uA564C5jzIWDGhYAxoXFjJiSmUJzVmYGBAXzjG98oE2dDQ0Noa3Of/p/85Cf1HhohhARnZBgYeGVUeNmF2Mk3R7eb0AFMOxt4z9XAtLnAf37ZtKCVkhwH/PKfgCfXAMnxwMwLTaE250PA9PcBbeMad2yExASKMxs9vRms2bgbBwaymN6RwvIlc9Hdla5pnytWrMCePXuwYMECGIaBCRMmoLOzE7t27cLzzz+P7u5u7Nu3D8ePH8fnP/953HLLLQBGOx689dZbuOKKK/CBD3wATz31FNLpNP71X/8VqVQqjEMmhBBnhnPA4d8VW8AO7QZefwEYssW3nnK6Kb4W3AhMfbcpyKbNBSZOA+zhFhOnmTFmdtemkQKu/hrw7iXAK78CXv4l8NIvgE3/E9j0D4DRDsx6PzDng6ZYe/t5QJK3LdL68CzP09Obwe0P70A2NwwAyAxkcfvDOwCgJoG2evVqPPPMM+jr68PPf/5zXHXVVXjmmWcKWZXf/e53MXnyZGSzWVxwwQW4/vrrMWXKlKJ9vPDCC3jggQfwrW99CzfccAPWr1+Pm2++ueoxEUJIgVwWeO0FmwUsL8YO7wFGhka3mzTLFF3v+EPz99S5wLR3A6lOf+9jZWW6ZWvOvdz8AYDBw8DL/5kXa08CP1tlLh9/GnDm4lGx9rZ5QKLlQ6fJGGTMiLM7H9mJZw+84bq+d+8ATg6PFC3L5obx3x/ajgd+vdfxNedMPw0rr54XaBwXXnhhUbmLr33ta/jxj38MANi3bx9eeOGFMnE2Z84cLFiwAABw/vnn4+WXXw70noQQguNvmCLMEmCWGDvyCgA1t5EEMPkdpvA6+ypThE2bC0w5Cxh/Su1jOPcGf6Uz2icD51xj/gDAm/9lCjVLrD3/b+by1GRg9gdG3aBT311srSOkSRkz4qwSpcKs0vJqmThxYuHvn//85/jZz36GX/3qV2hvb8fFF1/sWA5j/Pjxhb+TySSy2WzZNoQQAsC0OtktYJYQeyMzuk1yHDDlXcD0LuDcj9tE2LuAtvHu+46KU08H5n/U/AFMy9tLNrH23AZz+SmnA7M/OGpZ65xDsUaakrqKMxG5HMBXASQBfFtVV5es/wyAzwEYBvAWgFtU9dn8utsBfDq/7m9UdWMtY6lk4Vq8+glkBspFT7ojhbX/7f1Vv++pp56KN99803Hd0aNH0dnZifb2duzatQtbtmyp+n0IIWMIVeDN3ztnRg7agu6NdtOaNPsDeQF2tmkV65zd3LFbk2aYMW4LbjTn4sjLpkh7+ZemaHvmIXO702bkrWofNEVbx8xIh02IX+r27RSRJICvA/i/AOwH8BsR2WCJrzw/VNV789tfA+DLAC4XkXMAfBzAPADTAfxMRN6tqsP1Gu/yJXOLYs4AIGUksXzJ3Jr2O2XKFCxevBjvfe97kUqlcPrppxfWXX755bj33nvxnve8B3PnzsWiRYtqei9CSIsxMgIc3WuzgO02g/IPPQ+cODq63YRJpvCae8VoQP60uaY4afWYLBFg8hzz5/xPmWLttReAl5/Mu0D/Hej/oblt55xRF+jsD5oWOUJiiKhqfXYs8n4Aq1R1Sf7/2wFAVe922f5GAH+iqleUbisiG/P7+pXb+y1cuFC3bt1atOy5557De97zHt9jrke2ZiMJeryEkJgwPAQceWk0I/LQblOIvfYCkBsc3W7i20aF19S5o3+fcjrdd26MjACvPjtqWXt586iwnTq32LLWPjnasZIxhYhsU9WFTuvqaddOA9hn+38/gItKNxKRzwH4WwDjAFxqe63dx7c/v6z0tbcAuAUAZs2aVfOAu7vSTSXGCCFNRu448PqLNgtY/uf1F4GR3Oh2p80wRdf5i4uFGMVDcBIJ4O3vNX/e/1mzTtvB/tF4tb4fAr/5lrnt6fNHxdqZf2BaJAmJgMiDDlT16wC+LiKfAHAHgE8FeO19AO4DTMtZfUZICCEBOfFWPhZst02I7TJjozSfZCQJM/Zr6lyzzlfBIvZuYPypUY6+bsTCO5FIAun3mT+LP2/Wc8v8dtQNuvU7wJavm5/PGQtGxdqs9wPjJlbePyEhUE9xlgFgj76ckV/mxo8AfLPK1xJCSOMZPDwqwiwB9trz+ebeeRIGMOWdwNvnA/M/Nlqodcq7xlSronrVkqyZpAHMusj8+dBy07q5/zejlrVffR3YfI/5OabPHxVrbDVF6kg9xdlvAJwlInNgCquPA/iEfQMROUtVX8j/exUA6+8NAH4oIl+GmRBwFoBf13GshBDijCrw1qvFFjBLjB17dXS7thQw9SzTwjLtU6OZkZPnmAJgjPOP/76rKOEKMGtJrtm4O17hJMaEfCmODwKXfAE4eQzYu2VUrP3yfwNP/i+2miJ1pW7iTFWHROSvAGyEWUrju6q6U0S+BGCrqm4A8Fci8kcAcgCOIO/SzG+3DsCzAIYAfK6emZqEEAJVs35WwQJmiwk7PjC63fjTTPfjWZeNuiKnzTUr6Ld6ZqQLIyOK146dwIGB48gcyeLAQBaZAfP3gaNZZI5kcWQw5/jaAwNZqCokrgkN4yYC7/qw+QMAx49WaDWVt6ydscB0oRJSBXXL1mw0YWRrNjtj7XgJqYqRYTP2qzQz8tDzQO7Y6HbtU50zI089Y8xlRh7PDRcJrszAcVN45ZcdHDheVrD7lPFtSHekML1jAqZ3pPBI/wG8cXzIcf+zJrfjyvln4CPnnoF500+Lr1BzorTV1KFd5vLxk8ykAraaIi5Ela1JquCUU07BW2+9FfUwCGl+hk4Ar+9xyIx8ARg+ObrdqdNN0fW+TxYXap04xX3fLYSq4vVjJ02hdcQSYMeLxNjrx04WvSYhwOmnmaLr3BkduPy9EzCjI4Xptp/TJrQViawLZk8uqyU5wUjg2gXTcfDoCXzrl7/Dvb/YgzOnmELtqvlNItTYaorUAYozQkhzc/JYvmdkSWbk4ZeAQjSEAJ1nmqLrXR8eLdQ69ayWL5dwPDeM3x89jozd1ThQLMJODBVbvdrHJfNWrxTem56EdMcEpDtTmD7JXPb2SRNgJINZgay4MrdszcPHTuLxnb/HYzsO4r4nf4dv/nwPZltC7dwzcM4ZTSDUALaaIqFAt6ad7euA//iS+WWaNAP48Bf9Nen1YMWKFZg5cyY+97nPAQBWrVqFtrY2bNq0CUeOHEEul8Ndd92Fa6+9FkBtljO6NUlLkx2wZUbuGhVjA3tHt0m0AZPfCUx796gFzOoZOa49urHXCVXFkcGczeKVLXM/vvbWiaLXiABvO3V8wcKVzv9Mz7sg0x0pTEoZkQqhw8dOYuPO3+MnOw7iqT2vY3hEMWfqRFw5/+24av50vOeMU5tDqJVS1mrqSeCt/zLX2VtNzfmQeQ8iLY2XW5PizGL7OuCRvwFytv6aRgq4+ms1CbTe3l7ceuut+MUvfgEAOOecc7Bx40ZMmjQJp512Gl577TUsWrQIL7zwAkSE4oyMbVSBY6/lLWC7it2Rb/1+dLu2CcCUs4oD8qfOBSa/o6Uy5k4OjeDg0RJX45F8kH1egB3PFVu9JhiJgthy+n36pPEY39Y8geqWUHts+0E8tec1jCjwjqkTCxa1s9/epEINGG019dIvRvuCZg+b69hqquWhOMP/396dh0dV3X8cf38TAglr2Pd9B4MCARStu4KCa1ERRMAFt1Zrq61d1Lr8WrvaatVqLeCCLCIgFXBD3BcSFhNAQGQNCTuBBJKQ5fz+uJNJJgQcMMnMJJ/X8+Qxc8+dyeE6ST4595zzBRY9ADtSj/0CaUlQmHf08eg60G5Q+c9plQCXPFF+Wym9e/dm8eLF7N69mzvvvJMPP/yQe++9l48//pioqCjWrVvHpk2baNWqlcKZ1AzOwcH0wBGw4hBW/MsJoHb9oyfkN+8J8R0jfiWcc47Mw/kBI17pB3IDRsF2Z+dR9kd0c9+oV9v4WNo0ivNuN5YKYI3rhnbUqzLtzc7jndU7WZCazhff7fWCWvN6jPAFtZ4tIziogUpN1TBaEBCM8oLZ8Y6fgGuuuYbZs2ezY8cOrrvuOqZNm8bu3btZtmwZMTExdOrUidzc3B/8dUTCTlEhZG4JHAErXhl5JKvkvLjG3m3IPpeXCmK9oGGbiJ2Hc6SgiJ0Hc4+63Vh6pePhI4E7BNWpVTLqdW7P5rSNr+u/1Vg81ys2JrJD6Q/RtH4dxgzpwJghHdiTnecfUXtmyQae/mADXZrXY2RCa0b0a0OPlvUjL6gdq9RUcVjzl5oyaHmKSk1VYzVn5Oz7PHlK4K7exRq1h3tX/aC+rV69mltvvZU9e/bw0UcfMWvWLDZs2MDTTz/NkiVLOP/889m0aROdOnXSyJlEpoIjsG9j+SsjC0r94VG/VUnwKj0vrF6ziAphzjkO5hQctZdXSRDLZWdW7lGjXs3q1/bmdvkm1rdt7BsB84WvpvVqR16gCAO7s0qC2lebvBG1rs3rMaJfG0b2a02PltWkHFZxqalNH3vlprYt9b6/VGoqIum2ZjAqac5ZsYSEBJo1a8aSJUvYs2cPl112GdnZ2SQmJvLll1+yaNEihTMJf/k5ZVZGrvVGwfZ9B0Wl9rCK7+ALXj1KrYzsAXHxoev7Ccgv9Ea9Sm8pEXD7MTOX7LzAPbtqR0f59/Qqb6J9m/i4Gj3qVVV2Z+Xx9uodLEhJ56tN+3AOureo799HrXt1CWpQUmqqeGQtLcn7PlSpqYigcBasSlitWZUUzqTC5B4MXBm5Z72vcPcWwPczw6K9CfhlN2pt1j3s/2o/mJtfcqtx/9Gbqu48mEtRmR+NTerV9kKWb9SrXePS+3rF0qxeHaKiNOoVTnZl5fLOqh28lZLB0s1eUOvRsiSodWtRjYIalJSaKg5r6SvAFZUqNXWOF9ZUaiosKJzVEDXt3ysV4NDewBGw4gn6Wekl50TXLn9lZNOuUKtO6Pp+DAWFRezKyitnxKtktWNWmVGvmGijdaOSEa6yG6q2jY8jrrZGvSLZroO5vL3aC2pJvqDWs2UD/6rPbi3qh7qLFa9sqakdqwAHMfWgw+kqNRViCmc1RE3790oZxxr5dQ6ydgSOgBXPCTu8p+T5MfW8eWClJ+QXr4yMDp+1Q9l5BSXBa3/grcbtmTnsOJhLYZlhr/i6MSXzvIo3VC0VvJrX16hXTbLrYC6LVnlz1JK2eEGtV6uSoNa1eTUMahBEqSlfWFOpqSqhcFZD1LR/r5RS3pxJi/bmfh3eC3kHS47Hxh89Ib95T2jYNuQ/kAuLHLuz8sqZ45VDmi+Ila3PWCvKaNUo9pgbqraJj6NenfAJlxJedh7MZVFqBgtSM0jest8f1EYktObS6hzU4OhSU/s2esdVaqpK1Ohw1qtXrxqx+sk5x9q1axXOaqpjrTaOrgMDbgy8HVm/Rch+0B7KKyDjQHHQyi2zxUQOOw7kUlBm1KthbK2S4BUw4hVL2/i6NG9Qh2iNekkF2HEgl0WrMliQ4gU18ILayH6tuTShNV2qc1CDo0tNFf9MUampSlFjw9mmTZto0KABTZs2rdYBzTnH3r17ycrKonPnzqHujoTC74+1x5HB7zOrpAtFRY492XmkZR59q7F4V/vMw/kBz4mOMlo1jA0Y5WpTZvSrQWxMlfRfpLSMAzksSvVqfS7zBbXerRv6g1rnZuG96OUHO16pqUbtfWHNdxtUpaZOSo0NZ/n5+aSlpdWIDV5jY2Np164dMTH6RVajOAdL/g8+/kv57RWwT1+xnCOFR91q3J6Zy/bMw6Rn5pJxIIf8wsCfJ/Xr1PIFrdijdrJvGx9HiwZ1qHWCBbRFqlp6Zo5vjlo6y7d6f+z0ad2QEf1aMyKhNZ2qe1CD45eaatKlJKyp1FTQamw4E6nW8nPhzTth1RvQ8UwK0pZRq7DkD5GC6FhqXfF0UNvBFBU59hzKO/pWo2/EKz0zl32HjgQ8J8qgZcPYMoGr1D5fjeNoqFEvqWbSM3NY6JujtsIX1Pq2KQlqHZvWgKAGKjVVARTORKqbQ3tgxhjY9hVc8DDz6l3Lp/Oe42fMoI3tJd015R+M5qyr7uTK/m3JzS8M3E6i7IT7A7kcKQgsoF23dnRJ6Goc5x8BK67n2LJhLDEa9ZIabHtmjn8xQXFQO6VtQ0YktGFEQms6NK0b4h5WobKlprZ8AfmHUKmpY1M4E6lOdq+H167xtse46t/Q9yrOfOIDtmfmHHVqTLTRKC6GPdmBo15m0MJfQDvuqPlebePjaBhXq1rP1RSpSGn7D/vnqK3c5gW1hLaN/CNq7ZvUoKAGR5ea2vqVV6tapab8FM5EqotNH8PMG7yNYUdPh/aDAOj8wAKO9Z18/eD2/j2+ine2b9kwltq1NOolUhm27TvsrfpM3cHXvqDWr10jb3uOmhjUQKWmyqFwJlIdrJjm7WXWpCuMnQWNOwHefLF+j7x7VK1HgLbxcXz2wPlV3FERKbZt32EWpmawMDWDr9O8OVmntvNG1C45pYYGNQi+1FTbgRBdPeeuKpyJRLKiIljyOHzyN+8H1rUv+wuI5+YXcv/sFP73dTrRURawM35cTDR/vDqBK/u3DVXPRaSUbfsOs8AX1FKKg1r7eEYmtOaShFa0a1xDgxqUlJoqvg26I9U7Xo1LTSmciUSq/ByYdyesnuNtJjvi7/6/IvcfOsKkV5JJ2ryfXw3vRauGdfjru+tJz8yhTXwc9w/rqWAmEqa27i0JaqnbvaB2Wvt4RvZrzSUJrWkbHxfiHoZYcamp4pG1alhqSuFMJBId2gPTr4e0pXDhI3DmPf5duTfvOcTEqUlsz8zhb9ecymWntglxZ0XkZG3Ze8gf1FZt90qt9e8Q75+j1qamBzUoKTVVHNaqQakphTORSLN7HUy7xtuR++oXoM8V/qZlW/Zxy0vee/0/NyaS2El7CIlUF5v3HGKhr4TU6nQvqA3oEM+Ifm24NKEVrRspqAElpaaKw9pRpaZ8I2thXGpK4Uwkkmz8CGaOg1q14foZ0K7ke3dBSgb3zlpJm0axTJk4uPqXkBGpwTbv8UbUFqRksCbDC2oDOzb2j6i1alQzVjV+L+dg/6bAuqARUGpK4UwkUix/Bd76GTTtBmNmQeOOgFc/9fmPN/LEorUkdmzMCzcm0qRe7RB3VkSqyqY9h1iYmsFbKRl84wtqiR0b+1d9KqiVUrrU1KaPvblr5ZWa6nw21G8R+NyUWbD4UW9krlE7uOChoKqsnAyFM5FwV1QEHzwGn/4dupwH177k30W7oLCIh+av5rWvtjKyX2v+es2pxMZUj9VKInLiNu7O9ge1tTuyABjUyRtRuyShNS0bKqgFKCqCXatLboNu+QzyvIBL814lYe3wHnjnN95CrGIxcXDZU5US0BTORMJZfg7MvR3WzIOBE+DSv/pXZGbnFXDXtOV8tH43d5zblfsv7klUVHjOnxCRqvfd7mwWpnglpNbuyMIMBnVswqUJrRTUjqV0qalNH3v7reUfOvb5jdrDvasqvBsKZyLhKns3zLge0pLhokdh6E/9k1czDuRw09Rk1u/M4vErT+H6wR1C3FkRCWcbdnkjagtSMli30xfUOjXxRtROaUULBbXyFRyB9OUwedgxTjD4fWaFf1mFM5FwtGutVyMze7dvRebl/qY16Qe5aWoS2XkFPDN2AOf0aB7CjopIpNmwK4sFKTtYkJrO+p3ZmMHgTk0Y0a81w09pRYsGCmpHefKUklWfpWnk7OQpnElE+W4JzBoPterAmBleiRKfJet28ZNpy2kYF8PkCYPo3bphCDsqIpHu251Z/lWf3+7ygtqQzt6I2vBTWtO8QZ1QdzE8pMzySuRpzlnFUTiTiLHsJVjwc2+zxDEzIb7kduVrX23lwTdX0bNlAyZPGKQVWCJSodbvzGKBb47ahl3ZRBkM6dyUS/u1ZnjfVgpqNWG1ppkNB/6XpBVSAAAgAElEQVQJRAMvOueeKNP+c+AWoADYDdzknNviaysEfMW12Oqcu5zjUDiTsFdUBIsfgc/+AV3Ph2um+ldkFhU5/vTOWp7/aCPn9mzOv8YMoH6dWqHtr4hUW8451u/M9o2opfPd7kNEGZzepSmXJni3PpvVr+FBrZKFJJyZWTSwHrgISAOSgOudc2tKnXMe8JVz7rCZ3QGc65y7zteW7ZyrH+zXUziTsJafA3NvgzVvwsCJvhWZXvjKzS/kF7O+ZkFqBmOHdOCRy/tSKzoya8WJSORxzrFuZxYLUzJ4KzWDjb6gdkZXX1Dr24qmCmoVLlTh7Azg9865Yb7HvwZwzv3xGOf3B/7lnDvT91jhTKqH7F0wfTRsXw4XPw5n3OVfkbnv0BFufTmZZVv285tLe3Hrj7pgYVpqRESqP+cca3dk+Vd9btxziOgo4/QuTRiR0IZhfVsqqFWQ44Wzyrxv0hYovewhDRhynPNvBhaVehxrZsl4tzyfcM7Nq/guilSyXd/AtGu9zQ2vexV6j/Q3bdpziIlTlpJxIJdnxw7g0oTWIeyoiAiYGb1bN6R364b8/KIefJPhC2qpGfxmbioPvrmKM7o0ZUS/1gzr20qVSipJWExqMbMbgETgnFKHOzrntptZF+ADM0t1zn1X5nmTgEkAHTpoDygJM9994K3IjImDiQuhTX9/U9Lmfdz6cjJRZrx26+kM7Ng4hB0VETmamdGnTUP6tGnILy7uwZqMg/4RtV/PSeV381YxtGtTRiR4Qa2xglqFCfltTTO7EHgaOMc5t+sYrzUVeMs5N/tYX0+3NSWsLJsKb/3cKw0yZibEt/c3zf86nftmfU27xnFMmTiIjk1VvFxEIodzjjUZB/2rPrfsPUx0lDG0a1NG9mvNxX0U1IIRqjlntfAWBFwAbMdbEDDGObe61Dn9gdnAcOfct6WONwYOO+fyzKwZ8AVwRenFBGUpnElYKCqC9x+Gz5+CbhfCqCkQ6+1T5pzj2Q+/4y/vrGNwpyY8P26gfoCJSERzzrE6/aB/H7Wt+w5TK8oY2q0ZIxNac3HflsTX1c+58oRyK41LgX/gbaUx2Tn3f2b2KJDsnJtvZu8DCUCG7ylbnXOXm9lQ4HmgCIgC/uGc++/xvpbCmYTckcMwdxJ88z8YdAsM/5N/RWZ+YREPzlvFjKRtXHFaG/48qh91aql4uYhUH8VB7a2UDBakprNtXw61oowzuzXz5qj1aUWjujGh7mbY0Ca0IpUta6e3IjN9BQz7A5x+h39FZlZuPndOW84n3+7hp+d34+cX9dCKTBGp1pxzrNp+kLdS01mQkkHa/hxion1BLcG79VnTg5rCmUhl2rkGXrsWDu+FH/8Xel3qb0rPzOGmqUls2JXNH65K4NpB7Y/zQiIi1Y9zjtTtB1iQksFbKRlsz/SC2lndmjGiXxsu6tOSRnE1L6gpnIlUlg2L4fUJEFPXq5FZakXmqu0HuGlqEjlHCnn2hgH8qLuKl4tIzeacIyXtgH+OWnFQ+1H35oxIaM2FNSioKZyJVIbkybDgPmjR21uR2aidv2nJ2l3c9dpy4uNimDxxEL1aqXi5iEhpzjm+TjvAgpR0Fqbu8Ae1s7s3Z0Q/L6g1jK2+QU3hTKQiFRXBew/CF/+C7hfDqMlQp4G/+ZUvNvPw/NX0adOQ/44fRMuGKl4uInI8zjlWbstkQUoGC1MzSD+QS+3oKM7u4S0muLB3SxpUs6CmcCZSUY4chjm3wtq3YPAkGPZH/4rMoiLHE2+v5YWPN3J+rxY8fX1/6ql4uYjICSkqcqxMKwlqGf6g1pyR/VpzQe8W1SKoKZyJVISsHb4VmSth+BNw+u3+ptz8Qu6duZJFq3Zw4xkdeWhkHxUvFxH5gYqKHCt8I2qLVvmCWq0ozvEHtZbUj9A/ghXORH6onau9Gpk5+2HUf6HnJf6mPdl53PpyMiu3ZfLbS3tz81mdtVWGiEgF84Laft5KyWBR6g52HPSC2rk9vDlqkRbUFM5Efohv3/dWZNap7038b32qv+m73dlMmLKUXQfz+Ofo0xh+ioqXi4hUtqIix/KtvqC2KoOdB/OoUyuKc3s2Z0S/NlzQq0XYTytROBM5WUkvwsJfQss+cP1MaNTW3/TVxr1MemUZtaKMF8cn0r+DipeLiFS1oiLHsq37/XPUdmV5Qe28ni0Y0a8154dpUFM4EzlRRYXw7oPw5TPQfZhvRWZ9f/ObK7dz/+sptG8Sx5QJg+nQtG4IOysiIuAFteQt+1mYWhLUYmO8oHZpQngFNYUzkRNx5BC8cQusWwhDbvfKMUV5dTCdc/zrgw387b31DOnchBfGJdb4EiQiIuGosMiRvHmfF9RW7WC3L6id36skqNWtHbqgpnAmEqyDGTD9OtiR6q3IHHKbvym/sIjfzEnl9WVpXNW/LU/8OEHFy0VEIkBhkSOpOKil7mBPthfULujVkksTWnNer+bUrV2LeSu285d31pGemUOb+DjuH9aTK/u3/f4vcBIUzkSCsSMVXrsOcjLhminQY5i/6WBuPne+upxPN+zh7gu6c++F3bUiU0QkAhUWOZZu8oLaolUZ7Mk+QlxMND1b1WdN+kGOFJbkoriYaP54dUKlBDSFM5Hvs/5dmD0R6jT0rcjs529K23+Ym6YmsXH3If54dQLXJKp4uYhIdVBY5Phq014Wpmbw2ldbKSonErWNj+OzB86v8K99vHAW1C6ZZjbHzEaYmXbVlOpn6X+8W5lNusCtiwOCWWraAa569nMyDuTy0k2DFcxERKqR6ChjaNdmPH5lAscaq0rPzKnaThFkOAOeBcYA35rZE2bWsxL7JFI1igph0QOw8D5vRebERdCwjb/5/TU7ufb5L6gdHcUbdwzlzG7NQthZERGpTG3i407oeGUKKpw55953zo0FBgCbgffN7HMzm2hmWqomkScvG2aMha+egyF3wOhpAVtlvPT5Zia9kkz3lvWZe9dQerRscJwXExGRSHf/sJ7ExQQu8oqLieb+YVU/HhX0GlIzawrcAIwDVgDTgLOA8cC5ldE5kUpxMN2b+L9zFVz6Vxh8q7+psMjxfwu+YfJnm7iwd0ueuv60kC61FhGRqlE86b+qVmseT1C/dcxsLtATeAW4zDmX4WuaaWaahS+RIyPFC2Z5B70d/3tc7G/KOVLIPTNW8O6anUwY2okHR/YhOkorMkVEaoor+7cNSRgrK9ghgaecc0vKazjWSgORsLP+HXh9IsTFw01vQ6sEf9PurDxueTmZlLRMHhrZh5vO6hzCjoqISE0W7IKAPmYWX/zAzBqb2Z2V1CeRivfV8zB9NDTrBrcsDghmG3ZlcdWzn7Fux0Gev2GggpmIiIRUsOHsVudcZvED59x+4NbjnC8SHooKvcLli34JPS7xrchs7W/+4ru9XP3s5+TmFzJz0hlc3LdVCDsrIiIS/G3NaDMz59ux1syigdqV1y2RCpCXDW/cDOvfhjN+Ahc96q+RCTBneRq/eiOFjk3rMWXCINo3UfFyEREJvWDD2dt4k/+f9z2+zXdMJDwd2O5tLLtzDYz4Gwy6xd/knOOpxRt48v31nNGlKf8eN5BGcdoRRkREwkOw4exXeIHsDt/j94AXK6VHIj9Uxte+FZnZMGYWdL/Q33SkoIhfz0nljeVpXD2gLU9c3Y/atVT4QkREwkdQ4cw5VwQ85/sQCV/rFsHsmyGuMdz8DrTs6286kJPP7a8s44uNe7n3wh7cfUE3FS8XEZGwE+w+Z92BPwJ9gNji4865LpXUL5ET9+W/4Z1fQ+tT4foZ0KBkcv+2fYeZODWJLXsP8fdrT+XqAe1C2FEREZFjC/a25hTgYeBJ4DxgIsGv9BSpXIUFXihb+gL0GglXvwC16/mbv96Wyc0vJXOkoJCXbxrCGV2bhrCzIiIixxdswIpzzi0GzDm3xTn3e2BE5XVLJEh5WTDjei+YDf0pXPtKQDB7d/UOrnvhC2Jjophz51AFMxERCXvBjpzlmVkU8K2Z/QTYDtT/nueIVK4D272J/7vWwMgnIfGmgObJn27isQVr6NcunhdvTKR5gzoh6qiIiEjwgg1n9wB1gbuBx/BubY6vrE6JfK/0lV4wO3IIxs6CbiUrMguLHI+9tYapn29mWN+W/OO6/sTVjj7Oi4mIiISP7w1nvg1nr3PO3Qdk4803EwmdtQu9zWXrNoWb34WWffxNh48UcPf0lbz/zU5uPqszv7m0t4qXi4hIRPnecOacKzSzs6qiMyLH5Rx8+Ry88xto09+3IrOlv3lXVi63vJTMqu0HeOTyvowf2il0fRURETlJwS4IWGFm881snJldXfzxfU8ys+Fmts7MNpjZA+W0/9zM1phZipktNrOOpdrGm9m3vg/dQq3pCgtg4X3eqszeI2HCgoBgtn5nFlc98znf7szmhXGJCmYiIhKxgp1zFgvsBc4vdcwBc471BN/t0GeAi4A0IMnM5jvn1pQ6bQWQ6Jw7bGZ3AH8GrjOzJnhbdyT6vs4y33P3B9lfqU5yD8Lsm2DDezD0brjwEYgq+bvi8w17uO3VZcTGRDPrtjNIaNcohJ0VERH5YYKtEHAy88wGAxuccxsBzGwGcAXgD2fOuSWlzv8SuMH3+TDgPefcPt9z3wOGA9NPoh8SyQ6kwbRrYfdauOyfMHBCQPPsZWk88EYKXZrXY/KEQbRrrOLlIiIS2YKtEDAFbwQrgHPupnJOL9YW2FbqcRow5Djn3wwsOs5z25bTr0nAJIAOHToc56UlIqWvgNdGQ/5huGE2dC0ZuHXO8eR763nqgw2c1a0Zz94wgIaxKl4uIiKRL9jbmm+V+jwWuApIr6hOmNkNeLcwzzmR5znnXgBeAEhMTDwqPEoEW7sA3rgF6jaDG+dBi97+pryCQh54I5W5K7ZzzcB2/OHqBGKiVbBCRESqh2Bva75R+rGZTQc+/Z6nbQfal3rczncsgJldCPwWOMc5l1fqueeWee6HwfRVIpxz8MUz8O7voO0Ab0Vm/Rb+5gOH85n0SjJfbdrHfRf34K7zVLxcRESql2BHzsrqDrT4nnOSgO5m1hkvbI0GxpQ+wcz6A88Dw51zu0o1vQP8wcwa+x5fDPz6JPsqkaKwABbdD8mTofflcNXzULtkDtm2fYeZMGUp2/bl8M/Rp3HFaUfd6RYREYl4wc45yyJwztkO4FfHe45zrsBX6ukdIBqY7JxbbWaPAsnOufnAX/DKQL3uG/3Y6py73Dm3z8wewwt4AI8WLw6Qair3ILw+Ab5bDGf+DC54OGBF5oqt+7n15WTyCx2v3DyYIV1UI1NERKonc656TNVKTEx0ycnJoe6GnIzMbV4ppj3rYMTfYWDgtnZvr8rgnhkradkwlikTB9G1ucq6iohIZDOzZc65xPLagppFbWZXmVmjUo/jzezKiuqg1GDbl8OLF3hbZoydHRDMnHO8+MlG7pi2nN6tGzLnzqEKZiIiUu0Fu8TtYefcgeIHzrlMvE1iRU7eN/+DKZdCrTpejcyu5/mbCgqLeHj+ah5f8A3D+7ZixqTTaVa/Tgg7KyIiUjWCXRBQXog72cUEUtM5B58/De89BG0HwvXTA1ZkHsor4O7pK1i8dheTzu7CA8N7EaXi5SIiUkMEG7CSzezveOWYAO4CllVOl6RaK8yHhffDsinQ50q46t8QE+dv3nkwl5umJvFNxkEeu6Iv487oFLq+ioiIhECw4eynwIPATLxVm+/hBTSR4OUe8K3I/ADO+jmc/2DAisy1Ow5y05QkMnPyeXF8Iuf3anns1xIREammgt2E9hDwQCX3RaqzzK1ejcy938Ll/4IB4wKaP/l2N3e+upy42l7x8lPaqni5iIjUTMGu1nzPzOJLPW5sZu9UXrekWklbBv+5ALLS4YY5RwWzWUnbmDglibaN45h315kKZiIiUqMFe1uzmW+FJgDOuf1m9n0VAkRgzZsw5zZvwv+EBdC8h7/JOcff3l3Pv5Zs4Efdm/Hs2AE0UPFyERGp4YINZ0Vm1sE5txXAzDoRWDFAJJBz8PlT3orMdoO9FZn1mvmb8woK+eXsFN5cmc7oQe157MpTVLxcRESE4MPZb4FPzewjwIAfAZMqrVcS2QrzYcEvYPlL0PdquPI5iIn1N+8/dITbXlnG0s37+OXwntxxTlcVLxcREfEJdkHA22aWiBfIVgDzgJzK7JhEqJxMeH08bPwQfnQfnPfbgBWZW/YeYuKUJNL25/DU9f25/NQ2oeuriIhIGAq28PktwD1AO2AlcDrwBXB+5XVNIs7+zV6NzL3fwRXPQv+xAc3LtnjFy4ucY9qtQxjUqUlo+ikiIhLGgp3kcw8wCNjinDsP6A9kHv8pUqOkJcOLF0JWBoybe1QwW5iawZj/fEmD2FrMuWOogpmIiMgxBDvnLNc5l2tmmFkd59xaM+tZqT2TyLF6Hsy9DRq0grGLoFl3f5Nzjhc+3sgfF61lYMfGvDBuIE1VI1NEROSYgg1nab59zuYB75nZfmBL5XVLIoJz8OmTsPgRaD8ERr8WsCKzuHj5tK+2MiKhNX+79lRiY6JD2GEREZHwF+yCgKt8n/7ezJYAjYC3K61XEv4K8+Gte2HFK3DKKLjimYAVmdl5Bfz0teUsWbeb28/pyi+H9VTxchERkSAEO3Lm55z7qDI6IhEkJxNmjYNNH8PZv4TzfgOltsLYccArXr5uZxZ/uCqBMUM6hLCzIiIikeWEw5nUcPs3ezUy922EK/8Np10f0PxNxkEmTkkiKzef/45P5NyeKiQhIiJyIhTOJHjblsL066GoAG6cB53OCmj+aP1u7pq2nPp1avH67UPp06ZhiDoqIiISuRTOJDir5sDc26FhGxj7esCKTIDpS7fyu3mr6NGyAZMnJNK6UVyIOioiIhLZFM7k+JyDT/4GHzwGHc6A66ZBvab+5qIix1/eXcdzH37HOT2a88zYAdSvo7eViIjIydJvUTm2giPeisyVr0LCNd6KzFole5Tl5hdy3+tf81ZKBmOGdODRy/tSS8XLRUREfhCFMylfzn6YOQ42fwLnPADnPhCwInPfoSNMejmZ5C37eeCSXtx2dhcVLxcREakACmdytH2b4LVrvf9e9TycOjqgedOeQ0ycspT0A7n8a0x/RvZT8XIREZGKonAmgbZ+BTOuB1cEN74Jnc4MaE7evI9bX04GYPqtQxjYUTUyRUREKpLCmZRInQ3z7oRGbWHsbGjaNaD5f1+n84vXv6ZtfBxTJgyiU7N6IeqoiIhI9aVwJr4VmX+FDx6HDkNh9DSo26RUs+PfH23kT2+vZVCnxrwwLpHG9WqHsMMiIiLVl8JZTVdwBP53D3z9GvS7Di5/OmBFZn5hEQ+9uYrpS7dx2alt+MuofipeLiIiUokUzmqyw/tg1o3eisxzfw3n/CpgRWZWbj53vbaCj9fv5q7zuvKLi1S8XEREpLIpnNVU+zbCtGsgcytc/R/od21Ac8aBHCZOSeLbXdk8cXUCowereLmIiEhVUDiribZ+6dXIBG9FZsehAc2r0w9w09QkDuUVMmXCIM7u0TwEnRQREamZFM5qmtTZMO8OaNTeq5FZZkXmknW7+Mm05TSMi2H2HWfQq5WKl4uIiFSlSq21Y2bDzWydmW0wswfKaT/bzJabWYGZjSrTVmhmK30f8yuznzWCc/DRn+GNm6HdILjl/aOC2atfbuGWl5Lp1Kwe8+46U8FMREQkBCpt5MzMooFngIuANCDJzOY759aUOm0rMAG4r5yXyHHOnVZZ/atRCvJ8KzKnw6nXw2X/DFiRWVTk+NPba3n+442c17M5/xozgHoqXi4iIhISlfkbeDCwwTm3EcDMZgBXAP5w5pzb7GsrqsR+1GyH98HMG2DLZ3De7+Ds+wJWZObmF/KLWV+zIDWDG07vwO8vU/FyERGRUKrMcNYW2FbqcRow5ASeH2tmyUAB8IRzbl5Fdq5G2PudtyLzQBr8+L+QMCqwOTuPW19OZsW2TH57aW9u+VFnFS8XEREJsXC+d9XRObfdzLoAH5hZqnPuu9InmNkkYBJAhw7a6iHAli9gxhhvlGz8fOhwekDzxt3ZTJiSxM6DuTw7ZgCXJLQOUUdFRESktMq8f7UdaF/qcTvfsaA457b7/rsR+BDoX845LzjnEp1zic2ba7sHv5RZ8PLlULepN/G/TDBbumkfVz/3OYfyCpg+6XQFMxERkTBSmeEsCehuZp3NrDYwGghq1aWZNTazOr7PmwFnUmqumhyDc/DhEzDnVmg/BG55D5p0CTjlzZXbueHFr2hSrzZz7zyTAR0ah6izIiIiUp5Ku63pnCsws58A7wDRwGTn3GozexRIds7NN7NBwFygMXCZmT3inOsL9Aae9y0UiMKbc6ZwdjwFeTD/p5AyE04bCyP/AbVKipM753j2w+/4yzvrGNy5CS+MG0h8XRUvFxERCTfmnAt1HypEYmKiS05ODnU3QuPwPpgxFrZ+Duc/CD/6RcCKzPzCIn43dxUzk7dxxWlt+POoftSppeLlIiIioWJmy5xzieW1hfOCAAlG6RWZoybDKT8OaD6Ym89d05bzybd7uPv8btx7UQ+tyBQREQljCmeRbPNnMHMsWBSM/x90CNypZHtmDjdNSeK73dn8eVQ/rk1sf4wXEhERkXChcBapvp4Jb94FTTrDmFnef0tZtd0rXp5zpJCpEwdzVvdmIeqoiIiInAiFs0hTvCLzoyeg89lw7csQF7jicvE3O/np9BU0rlubV+4YQs9WDULUWRERETlRCmeRpCAP3vwJpM6C/jfAiCcDVmQCvPLFZh6ev5o+bRoyefwgWjSMDU1fRURE5KQonEWKQ3u9+WVbv4ALHoKzfh6wIrOoyPGHhd/w4qebuLB3C/45ur+Kl4uIiEQg/faOBHs2wLRRcDAdRk2BU64OaM45Usi9M1fy9uodjD+jIw9d1pfoKK3IFBERiUQKZ+Fu86feHmZRtWDCAmg/KKB5T3Yet7yUzNdpmTw4sg83ndlJW2WIiIhEMIWzcLZyurfrf5MuMHYWNO4U0LxhVzYTpy5ld1Yez40dyPBTWoWmnyIiIlJhFM7CkXOw5A/w8Z+h8zm+FZnxAad8uXEvk15OpnatKGZMOoPT2scf48VEREQkkiichZv8XG//slWzof84GPkkRMcEnDJ3RRq/nJ1ChyZ1mTpxMO2b1A1RZ0VERKSiKZyFk0N7vPll276EC38PZ/4sYEWmc46nP9jA399bz+ldmvD8DYk0qhtzzJcTERGRyKNwFi72fOutyMzaAde8BH2vDGg+UlDEb+amMntZGlf3b8sTP+5H7VpRIeqsiIiIVBaFs3Cw6ROYeYN3+3LCAmgXWKT+QE4+d7y6jM+/28s9F3TnZxd214pMERGRakrhLNRWTIP/3QNNu3o1Mht3DGhO23+YiVOS2LTnEH+95lRGDWwXoo6KiIhIVVA4C5WiIljyOHzyN+hyrncrs8yKzJS0TG5+KZnc/EJevmkwQ7upeLmIiEh1p3AWCvk5MO9OWD0HBoyHEX87akXme2t2cvf0FTSpV5vXbhlC95YqXi4iIlITKJxVtUN7YPr1kLYULnoUht4dsCITYOpnm3jkrTUktG3Ei+MTadFAxctFRERqCoWzqrR7HUy7BrJ3ehvL9rkioLmwyPF/C75h8mebuKhPS/45+jTq1tb/IhERkZpEv/mrysaPYOY4qFUHJiyEdgMDmg8fKeCeGSt5b81ObjqzM78d0VvFy0VERGoghbOqsPwVeOtn0KwHjJkJ8R0Cmndn5XHLS0mkbD/Aw5f1YeKZnUPUUREREQk1hbPKVFQEHzwGn/4dup4P10yF2EYBp3y7M4uJU5PYm32EF8YlclGflqHpq4iIiIQFhbPKkp8Dc2+HNfNg4ES49K8QHXi5P/9uD7e9sow6taKZedvp9Gun4uUiIiI1ncJZZcjeDTOuh7RkuPhxOOMnR63IfGNZGg/MSaFT03pMmTiIdo1VvFxEREQUzirerrXw2jVeQLvuVeg9MqDZOcc/3v+Wfy7+lqFdm/LcDQNpFKfi5SIiIuJROKtI3y2BWeMhJhYmLoS2AwKajxQU8cAbKcxZsZ1RA9vxh6sSVLxcREREAiicVZRlL8GCn0Oznr4Vme0Dmg8czuf2V5fxxca9/PyiHvz0/G4qXi4iIiJHUTj7oYqKYPEj8Nk/oNuFMGoKxDYMOGXbvsNMmLKUrfsO8+R1p3JVfxUvFxERkfIpnP0Q+Tkw9zZY8yYk3gyX/PmoFZkrt2Vyy0tJHCko4pWbh3B6l6Yh6qyIiIhEAoWzk5W9C6aPhu3LYdgf4PQ7j1qR+c7qHdwzYwXNG9RhxqTT6dZCxctFRETk+BTOgpUyCxY/CgfSoH5LKDwCBbkwehr0GhFwqnOOyZ9t5vEFazi1XTwvjk+kWf06Ieq4iIiIRBKFs2CkzIL/3e3dxgTI3uH997zfHhXMCoscj721hqmfb2ZY35b847r+xNWOruIOi4iISKTSPg7BWPxoSTArbfnLAQ8PHyngtleSmfr5Zm45qzPPjh2oYCYiIiInpFLDmZkNN7N1ZrbBzB4op/1sM1tuZgVmNqpM23gz+9b3Mb4y+/m9DqR97/FdB3O57vkv+WDtLh69oi+/G9mH6ChtlSEiIiInptJua5pZNPAMcBGQBiSZ2Xzn3JpSp20FJgD3lXluE+BhIBFwwDLfc/dXVn+Pq1E7OLCt/OPA+p1ZTJySxL5DR/jPjYlc0FvFy0VEROTkVObI2WBgg3Nuo3PuCDADuKL0Cc65zc65FKCozHOHAe855/b5Atl7wPBK7OvxXfAQxMQFHouJgwse4tNv9/DjZz8nv7CI128/Q8FMREREfpDKDGdtgdLDTWm+Y5X93IrX71q47Clo1B4w77+XPcWsI2cwYcpS2sTHMfeuMzmlbaOQdVFERESqh4herWlmk4BJAB06dKjcL9bvWu8Db8y813QAAAefSURBVKuMv7+3nqc/SOGsbs149oYBNIxV8XIRERH54SoznG0HSheYbOc7Fuxzzy3z3A/LnuScewF4ASAxMdGdTCeDNW/Fdv7yzjrSM3OIjYkmJ7+Q6xLb8/hVpxATrUWvIiIiUjEqM1UkAd3NrLOZ1QZGA/ODfO47wMVm1tjMGgMX+46FxLwV2/n1nFS2Z+bggJz8QmpFGad3aaJgJiIiIhWq0pKFc64A+AleqPoGmOWcW21mj5rZ5QBmNsjM0oBrgOfNbLXvufuAx/ACXhLwqO9YSPzlnXXk5BcGHCsocvz13fUh6pGIiIhUV5U658w5txBYWObYQ6U+T8K7ZVnecycDkyuzf8FKzyxnA9rjHBcRERE5WbonF4Q28XEndFxERETkZCmcBeH+YT2JiwkswxQXE839w3qGqEciIiJSXUX0VhpV5cr+3hZrxas128THcf+wnv7jIiIiIhVF4SxIV/ZvqzAmIiIilU63NUVERETCiMKZiIiISBhROBMREREJIwpnIiIiImHEnKvUkpRVxsx2A1uq4Es1A/ZUwdepKXQ9K56uacXS9ax4uqYVT9e0YlXF9ezonGteXkO1CWdVxcySnXOJoe5HdaHrWfF0TSuWrmfF0zWteLqmFSvU11O3NUVERETCiMKZiIiISBhRODtxL4S6A9WMrmfF0zWtWLqeFU/XtOLpmlaskF5PzTkTERERCSMaORMREREJIwpnx2Bmw81snZltMLMHymmvY2Yzfe1fmVmnqu9l5Ajiek4ws91mttL3cUso+hkpzGyyme0ys1XHaDcze8p3vVPMbEBV9zHSBHFNzzWzA6Xeow9VdR8jiZm1N7MlZrbGzFab2T3lnKP3aZCCvJ56j54AM4s1s6Vm9rXvmj5Szjkh+V2vcFYOM4sGngEuAfoA15tZnzKn3Qzsd851A54E/lS1vYwcQV5PgJnOudN8Hy9WaScjz1Rg+HHaLwG6+z4mAc9VQZ8i3VSOf00BPin1Hn20CvoUyQqAXzjn+gCnA3eV832v92nwgrmeoPfoicgDznfOnQqcBgw3s9PLnBOS3/UKZ+UbDGxwzm10zh0BZgBXlDnnCuAl3+ezgQvMzKqwj5EkmOspJ8A59zGw7zinXAG87DxfAvFm1rpqeheZgrimcgKccxnOueW+z7OAb4C2ZU7T+zRIQV5POQG+912272GM76PsRPyQ/K5XOCtfW2BbqcdpHP1N4D/HOVcAHACaVknvIk8w1xPgx75bG7PNrH3VdK3aCvaay4k5w3cLZJGZ9Q11ZyKF71ZQf+CrMk16n56E41xP0Hv0hJhZtJmtBHYB7znnjvkercrf9QpnEi7+B3RyzvUD3qPkLxWRcLEcr9zKqcDTwLwQ9ycimFl94A3gZ865g6HuT6T7nuup9+gJcs4VOudOA9oBg83slFD3CRTOjmU7UHrkpp3vWLnnmFktoBGwt0p6F3m+93o65/Y65/J8D18EBlZR36qrYN7DcgKccweLb4E45xYCMWbWLMTdCmtmFoMXJKY55+aUc4repyfg+66n3qMnzzmXCSzh6HmnIfldr3BWviSgu5l1NrPawGhgfplz5gPjfZ+PAj5w2jTuWL73epaZZ3I53nwKOXnzgRt9q+FOBw445zJC3alIZmatiueamNlgvJ+f+oPsGHzX6r/AN865vx/jNL1PgxTM9dR79MSYWXMzi/d9HgdcBKwtc1pIftfXquwvEImccwVm9hPgHSAamOycW21mjwLJzrn5eN8kr5jZBrxJxKND1+PwFuT1vNvMLsdbkbQPmBCyDkcAM5sOnAs0M7M04GG8yaw45/4NLAQuBTYAh4GJoelp5Ajimo4C7jCzAiAHGK0/yI7rTGAckOqb0wPwG6AD6H16EoK5nnqPnpjWwEu+HQWigFnOubfC4Xe9KgSIiIiIhBHd1hQREREJIwpnIiIiImFE4UxEREQkjCiciYiIiIQRhTMRERGRMKJwJiLyA5nZuWb2Vqj7ISLVg8KZiIiISBhROBORGsPMbjCzpWa20sye9xU9zjazJ81stZktNrPmvnNPM7MvzSzFzOaaWWPf8W5m9r6vuPRyM+vqe/n6ZjbbzNaa2bTindpFRE6UwpmI1Ahm1hu4DjjTV+i4EBgL1MPbDbwv8BFeZQCAl4FfOef6Aamljk8DnvEVlx4KFJcb6g/8DOgDdMHb0V1E5ISpfJOI1BQXAAOBJN+gVhywCygCZvrOeRWYY2aNgHjn3Ee+4y8Br5tZA6Ctc24ugHMuF8D3ekudc2m+xyuBTsCnlf/PEpHqRuFMRGoKA15yzv064KDZg2XOO9madnmlPi9EP19F5CTptqaI1BSLgVFm1gLAzJqYWUe8n4OjfOeMAT51zh0A9pvZj3zHxwEfOeeygDQzu9L3GnXMrG6V/itEpNrTX3YiUiM459aY2e+Ad80sCsgH7gIOAYN9bbvw5qUBjAf+7QtfG4GJvuPjgOfN7FHfa1xThf8MEakBzLmTHcEXEYl8ZpbtnKsf6n6IiBTTbU0RERGRMKKRMxEREZEwopEzERERkTCicCYiIiISRhTORERERMKIwpmIiIhIGFE4ExEREQkjCmciIiIiYeT/AU/iTTZMXRaeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the loss, training accuracy, and validation accuracy should show clear overfitting:\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 1470) loss: 1.095365\n",
      "(Epoch 0 / 3) train acc: 0.276000; val_acc: 0.292000\n",
      "(Iteration 11 / 1470) loss: 1.282504\n",
      "(Iteration 21 / 1470) loss: 1.306024\n",
      "(Iteration 31 / 1470) loss: 1.320847\n",
      "(Iteration 41 / 1470) loss: 1.396638\n",
      "(Iteration 51 / 1470) loss: 1.408186\n",
      "(Iteration 61 / 1470) loss: 1.572116\n",
      "(Iteration 71 / 1470) loss: 1.194091\n",
      "(Iteration 81 / 1470) loss: 1.225935\n",
      "(Iteration 91 / 1470) loss: 0.951051\n",
      "(Iteration 101 / 1470) loss: 1.387936\n",
      "(Iteration 111 / 1470) loss: 1.299824\n",
      "(Iteration 121 / 1470) loss: 1.154365\n",
      "(Iteration 131 / 1470) loss: 1.264633\n",
      "(Iteration 141 / 1470) loss: 1.378582\n",
      "(Iteration 151 / 1470) loss: 1.314556\n",
      "(Iteration 161 / 1470) loss: 1.325566\n",
      "(Iteration 171 / 1470) loss: 1.523696\n",
      "(Iteration 181 / 1470) loss: 1.033561\n",
      "(Iteration 191 / 1470) loss: 1.125684\n",
      "(Iteration 201 / 1470) loss: 1.254518\n",
      "(Iteration 211 / 1470) loss: 1.120451\n",
      "(Iteration 221 / 1470) loss: 1.184285\n",
      "(Iteration 231 / 1470) loss: 1.134465\n",
      "(Iteration 241 / 1470) loss: 1.319351\n",
      "(Iteration 251 / 1470) loss: 1.037176\n",
      "(Iteration 261 / 1470) loss: 0.968336\n",
      "(Iteration 271 / 1470) loss: 1.045859\n",
      "(Iteration 281 / 1470) loss: 1.114611\n",
      "(Iteration 291 / 1470) loss: 1.111141\n",
      "(Iteration 301 / 1470) loss: 1.413887\n",
      "(Iteration 311 / 1470) loss: 1.076154\n",
      "(Iteration 321 / 1470) loss: 0.944622\n",
      "(Iteration 331 / 1470) loss: 1.214398\n",
      "(Iteration 341 / 1470) loss: 1.023624\n",
      "(Iteration 351 / 1470) loss: 0.988026\n",
      "(Iteration 361 / 1470) loss: 0.971499\n",
      "(Iteration 371 / 1470) loss: 1.037359\n",
      "(Iteration 381 / 1470) loss: 1.337230\n",
      "(Iteration 391 / 1470) loss: 1.154550\n",
      "(Iteration 401 / 1470) loss: 0.974340\n",
      "(Iteration 411 / 1470) loss: 0.963264\n",
      "(Iteration 421 / 1470) loss: 0.986961\n",
      "(Iteration 431 / 1470) loss: 1.052597\n",
      "(Iteration 441 / 1470) loss: 1.132902\n",
      "(Iteration 451 / 1470) loss: 1.093950\n",
      "(Iteration 461 / 1470) loss: 1.170762\n",
      "(Iteration 471 / 1470) loss: 0.895990\n",
      "(Iteration 481 / 1470) loss: 1.114609\n",
      "(Epoch 1 / 3) train acc: 0.374000; val_acc: 0.379000\n",
      "(Iteration 491 / 1470) loss: 1.109639\n",
      "(Iteration 501 / 1470) loss: 0.927677\n",
      "(Iteration 511 / 1470) loss: 0.936705\n",
      "(Iteration 521 / 1470) loss: 1.087557\n",
      "(Iteration 531 / 1470) loss: 1.002407\n",
      "(Iteration 541 / 1470) loss: 1.063493\n",
      "(Iteration 551 / 1470) loss: 1.044719\n",
      "(Iteration 561 / 1470) loss: 0.812146\n",
      "(Iteration 571 / 1470) loss: 0.952341\n",
      "(Iteration 581 / 1470) loss: 1.185393\n",
      "(Iteration 591 / 1470) loss: 0.937239\n",
      "(Iteration 601 / 1470) loss: 1.011858\n",
      "(Iteration 611 / 1470) loss: 0.875417\n",
      "(Iteration 621 / 1470) loss: 1.027296\n",
      "(Iteration 631 / 1470) loss: 0.988562\n",
      "(Iteration 641 / 1470) loss: 0.784027\n",
      "(Iteration 651 / 1470) loss: 1.037596\n",
      "(Iteration 661 / 1470) loss: 1.040689\n",
      "(Iteration 671 / 1470) loss: 0.958119\n",
      "(Iteration 681 / 1470) loss: 0.865766\n",
      "(Iteration 691 / 1470) loss: 0.786991\n",
      "(Iteration 701 / 1470) loss: 1.015481\n",
      "(Iteration 711 / 1470) loss: 0.875687\n",
      "(Iteration 721 / 1470) loss: 0.823368\n",
      "(Iteration 731 / 1470) loss: 0.895940\n",
      "(Iteration 741 / 1470) loss: 0.877333\n",
      "(Iteration 751 / 1470) loss: 0.763038\n",
      "(Iteration 761 / 1470) loss: 0.997726\n",
      "(Iteration 771 / 1470) loss: 0.974304\n",
      "(Iteration 781 / 1470) loss: 0.932509\n",
      "(Iteration 791 / 1470) loss: 0.931282\n",
      "(Iteration 801 / 1470) loss: 0.841047\n",
      "(Iteration 811 / 1470) loss: 0.903439\n",
      "(Iteration 821 / 1470) loss: 0.749620\n",
      "(Iteration 831 / 1470) loss: 0.960018\n",
      "(Iteration 841 / 1470) loss: 0.801346\n",
      "(Iteration 851 / 1470) loss: 1.000148\n",
      "(Iteration 861 / 1470) loss: 0.883451\n",
      "(Iteration 871 / 1470) loss: 0.787757\n",
      "(Iteration 881 / 1470) loss: 0.958755\n",
      "(Iteration 891 / 1470) loss: 1.019859\n",
      "(Iteration 901 / 1470) loss: 0.947192\n",
      "(Iteration 911 / 1470) loss: 0.803414\n",
      "(Iteration 921 / 1470) loss: 0.837292\n",
      "(Iteration 931 / 1470) loss: 0.898158\n",
      "(Iteration 941 / 1470) loss: 1.080557\n",
      "(Iteration 951 / 1470) loss: 0.824794\n",
      "(Iteration 961 / 1470) loss: 1.179421\n",
      "(Iteration 971 / 1470) loss: 0.735294\n",
      "(Epoch 2 / 3) train acc: 0.357000; val_acc: 0.347000\n",
      "(Iteration 981 / 1470) loss: 0.795599\n",
      "(Iteration 991 / 1470) loss: 0.941219\n",
      "(Iteration 1001 / 1470) loss: 0.947301\n",
      "(Iteration 1011 / 1470) loss: 0.859831\n",
      "(Iteration 1021 / 1470) loss: 0.700003\n",
      "(Iteration 1031 / 1470) loss: 1.016242\n",
      "(Iteration 1041 / 1470) loss: 0.808303\n",
      "(Iteration 1051 / 1470) loss: 1.073063\n",
      "(Iteration 1061 / 1470) loss: 0.885832\n",
      "(Iteration 1071 / 1470) loss: 0.908011\n",
      "(Iteration 1081 / 1470) loss: 0.666045\n",
      "(Iteration 1091 / 1470) loss: 0.709903\n",
      "(Iteration 1101 / 1470) loss: 0.920490\n",
      "(Iteration 1111 / 1470) loss: 0.749019\n",
      "(Iteration 1121 / 1470) loss: 0.889096\n",
      "(Iteration 1131 / 1470) loss: 0.839448\n",
      "(Iteration 1141 / 1470) loss: 0.911158\n",
      "(Iteration 1151 / 1470) loss: 0.933112\n",
      "(Iteration 1161 / 1470) loss: 0.737470\n",
      "(Iteration 1171 / 1470) loss: 0.856671\n",
      "(Iteration 1181 / 1470) loss: 0.817082\n",
      "(Iteration 1191 / 1470) loss: 0.875139\n",
      "(Iteration 1201 / 1470) loss: 0.800652\n",
      "(Iteration 1211 / 1470) loss: 0.835403\n",
      "(Iteration 1221 / 1470) loss: 0.788779\n",
      "(Iteration 1231 / 1470) loss: 0.908394\n",
      "(Iteration 1241 / 1470) loss: 0.771342\n",
      "(Iteration 1251 / 1470) loss: 0.813266\n",
      "(Iteration 1261 / 1470) loss: 0.777459\n",
      "(Iteration 1271 / 1470) loss: 0.783805\n",
      "(Iteration 1281 / 1470) loss: 0.670516\n",
      "(Iteration 1291 / 1470) loss: 0.636526\n",
      "(Iteration 1301 / 1470) loss: 0.681008\n",
      "(Iteration 1311 / 1470) loss: 0.715599\n",
      "(Iteration 1321 / 1470) loss: 0.764547\n",
      "(Iteration 1331 / 1470) loss: 0.654612\n",
      "(Iteration 1341 / 1470) loss: 0.598213\n",
      "(Iteration 1351 / 1470) loss: 0.763878\n",
      "(Iteration 1361 / 1470) loss: 0.807091\n",
      "(Iteration 1371 / 1470) loss: 0.732202\n",
      "(Iteration 1381 / 1470) loss: 0.849154\n",
      "(Iteration 1391 / 1470) loss: 0.799371\n",
      "(Iteration 1401 / 1470) loss: 0.755141\n",
      "(Iteration 1411 / 1470) loss: 0.753722\n",
      "(Iteration 1421 / 1470) loss: 0.973735\n",
      "(Iteration 1431 / 1470) loss: 0.720228\n",
      "(Iteration 1441 / 1470) loss: 0.882493\n",
      "(Iteration 1451 / 1470) loss: 0.723292\n",
      "(Iteration 1461 / 1470) loss: 0.903512\n",
      "(Epoch 3 / 3) train acc: 0.478000; val_acc: 0.398000\n",
      "Time: 2828.7054595947266\n",
      "(Iteration 1 / 1470) loss: 0.941950\n",
      "(Epoch 0 / 3) train acc: 0.426000; val_acc: 0.397000\n",
      "(Iteration 11 / 1470) loss: 0.801794\n",
      "(Iteration 21 / 1470) loss: 0.888586\n",
      "(Iteration 31 / 1470) loss: 0.780196\n",
      "(Iteration 41 / 1470) loss: 0.871571\n",
      "(Iteration 51 / 1470) loss: 0.786189\n",
      "(Iteration 61 / 1470) loss: 0.971057\n",
      "(Iteration 71 / 1470) loss: 1.002858\n",
      "(Iteration 81 / 1470) loss: 0.877245\n",
      "(Iteration 91 / 1470) loss: 1.044761\n",
      "(Iteration 101 / 1470) loss: 0.884491\n",
      "(Iteration 111 / 1470) loss: 0.794729\n",
      "(Iteration 121 / 1470) loss: 0.950146\n",
      "(Iteration 131 / 1470) loss: 0.786567\n",
      "(Iteration 141 / 1470) loss: 0.894373\n",
      "(Iteration 151 / 1470) loss: 1.009422\n",
      "(Iteration 161 / 1470) loss: 0.840290\n",
      "(Iteration 171 / 1470) loss: 0.891582\n",
      "(Iteration 181 / 1470) loss: 0.771801\n",
      "(Iteration 191 / 1470) loss: 0.870125\n",
      "(Iteration 201 / 1470) loss: 0.812624\n",
      "(Iteration 211 / 1470) loss: 0.897047\n",
      "(Iteration 221 / 1470) loss: 0.809334\n",
      "(Iteration 231 / 1470) loss: 0.699489\n",
      "(Iteration 241 / 1470) loss: 0.800368\n",
      "(Iteration 251 / 1470) loss: 0.897482\n",
      "(Iteration 261 / 1470) loss: 0.785606\n",
      "(Iteration 271 / 1470) loss: 0.693021\n",
      "(Iteration 281 / 1470) loss: 0.741440\n",
      "(Iteration 291 / 1470) loss: 0.610325\n",
      "(Iteration 301 / 1470) loss: 0.876888\n",
      "(Iteration 311 / 1470) loss: 0.852072\n",
      "(Iteration 321 / 1470) loss: 0.829948\n",
      "(Iteration 331 / 1470) loss: 0.833818\n",
      "(Iteration 341 / 1470) loss: 0.706397\n",
      "(Iteration 351 / 1470) loss: 0.843393\n",
      "(Iteration 361 / 1470) loss: 0.716432\n",
      "(Iteration 371 / 1470) loss: 0.645305\n",
      "(Iteration 381 / 1470) loss: 0.748148\n",
      "(Iteration 391 / 1470) loss: 0.760134\n",
      "(Iteration 401 / 1470) loss: 0.756265\n",
      "(Iteration 411 / 1470) loss: 0.764781\n",
      "(Iteration 421 / 1470) loss: 0.815270\n",
      "(Iteration 431 / 1470) loss: 0.797709\n",
      "(Iteration 441 / 1470) loss: 0.744799\n",
      "(Iteration 451 / 1470) loss: 0.681409\n",
      "(Iteration 461 / 1470) loss: 0.771169\n",
      "(Iteration 471 / 1470) loss: 0.755824\n",
      "(Iteration 481 / 1470) loss: 0.793676\n",
      "(Epoch 1 / 3) train acc: 0.414000; val_acc: 0.367000\n",
      "(Iteration 491 / 1470) loss: 0.506710\n",
      "(Iteration 501 / 1470) loss: 0.673975\n",
      "(Iteration 511 / 1470) loss: 0.670608\n",
      "(Iteration 521 / 1470) loss: 0.572437\n",
      "(Iteration 531 / 1470) loss: 0.618356\n",
      "(Iteration 541 / 1470) loss: 0.680244\n",
      "(Iteration 551 / 1470) loss: 0.657128\n",
      "(Iteration 561 / 1470) loss: 0.731668\n",
      "(Iteration 571 / 1470) loss: 0.791503\n",
      "(Iteration 581 / 1470) loss: 0.748769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 591 / 1470) loss: 0.626953\n",
      "(Iteration 601 / 1470) loss: 0.687872\n",
      "(Iteration 611 / 1470) loss: 0.782288\n",
      "(Iteration 621 / 1470) loss: 0.654528\n",
      "(Iteration 631 / 1470) loss: 0.720348\n",
      "(Iteration 641 / 1470) loss: 0.776398\n",
      "(Iteration 651 / 1470) loss: 0.650127\n",
      "(Iteration 661 / 1470) loss: 0.645728\n",
      "(Iteration 671 / 1470) loss: 0.806521\n",
      "(Iteration 681 / 1470) loss: 0.683346\n",
      "(Iteration 691 / 1470) loss: 0.895650\n",
      "(Iteration 701 / 1470) loss: 0.663352\n",
      "(Iteration 711 / 1470) loss: 0.736704\n",
      "(Iteration 721 / 1470) loss: 0.787176\n",
      "(Iteration 731 / 1470) loss: 0.644795\n",
      "(Iteration 741 / 1470) loss: 0.721793\n",
      "(Iteration 751 / 1470) loss: 0.952520\n",
      "(Iteration 761 / 1470) loss: 0.778653\n",
      "(Iteration 771 / 1470) loss: 0.705820\n",
      "(Iteration 781 / 1470) loss: 0.535438\n",
      "(Iteration 791 / 1470) loss: 0.747120\n",
      "(Iteration 801 / 1470) loss: 0.769331\n",
      "(Iteration 811 / 1470) loss: 0.646805\n",
      "(Iteration 821 / 1470) loss: 0.860039\n",
      "(Iteration 831 / 1470) loss: 0.813678\n",
      "(Iteration 841 / 1470) loss: 1.035192\n",
      "(Iteration 851 / 1470) loss: 0.721930\n",
      "(Iteration 861 / 1470) loss: 0.591097\n",
      "(Iteration 871 / 1470) loss: 0.605587\n",
      "(Iteration 881 / 1470) loss: 0.622272\n",
      "(Iteration 891 / 1470) loss: 0.550989\n",
      "(Iteration 901 / 1470) loss: 0.795557\n",
      "(Iteration 911 / 1470) loss: 0.619024\n",
      "(Iteration 921 / 1470) loss: 0.625531\n",
      "(Iteration 931 / 1470) loss: 0.669560\n",
      "(Iteration 941 / 1470) loss: 0.764551\n",
      "(Iteration 951 / 1470) loss: 0.629950\n",
      "(Iteration 961 / 1470) loss: 0.722066\n",
      "(Iteration 971 / 1470) loss: 0.656425\n",
      "(Epoch 2 / 3) train acc: 0.378000; val_acc: 0.367000\n",
      "(Iteration 981 / 1470) loss: 0.692381\n",
      "(Iteration 991 / 1470) loss: 0.754699\n",
      "(Iteration 1001 / 1470) loss: 0.778605\n",
      "(Iteration 1011 / 1470) loss: 0.640979\n",
      "(Iteration 1021 / 1470) loss: 0.741553\n",
      "(Iteration 1031 / 1470) loss: 0.608722\n",
      "(Iteration 1041 / 1470) loss: 0.701475\n",
      "(Iteration 1051 / 1470) loss: 0.647485\n",
      "(Iteration 1061 / 1470) loss: 0.870992\n",
      "(Iteration 1071 / 1470) loss: 0.722022\n",
      "(Iteration 1081 / 1470) loss: 0.629632\n",
      "(Iteration 1091 / 1470) loss: 0.639671\n",
      "(Iteration 1101 / 1470) loss: 0.737609\n",
      "(Iteration 1111 / 1470) loss: 0.809233\n",
      "(Iteration 1121 / 1470) loss: 0.618909\n",
      "(Iteration 1131 / 1470) loss: 0.755374\n",
      "(Iteration 1141 / 1470) loss: 0.507175\n",
      "(Iteration 1151 / 1470) loss: 0.798179\n",
      "(Iteration 1161 / 1470) loss: 0.673799\n",
      "(Iteration 1171 / 1470) loss: 0.775033\n",
      "(Iteration 1181 / 1470) loss: 0.671302\n",
      "(Iteration 1191 / 1470) loss: 0.661460\n",
      "(Iteration 1201 / 1470) loss: 0.759453\n",
      "(Iteration 1211 / 1470) loss: 0.754432\n",
      "(Iteration 1221 / 1470) loss: 0.801128\n",
      "(Iteration 1231 / 1470) loss: 0.537912\n",
      "(Iteration 1241 / 1470) loss: 0.680111\n",
      "(Iteration 1251 / 1470) loss: 0.695241\n",
      "(Iteration 1261 / 1470) loss: 0.731337\n",
      "(Iteration 1271 / 1470) loss: 0.465055\n",
      "(Iteration 1281 / 1470) loss: 0.518991\n",
      "(Iteration 1291 / 1470) loss: 0.498061\n",
      "(Iteration 1301 / 1470) loss: 0.595347\n",
      "(Iteration 1311 / 1470) loss: 0.692314\n",
      "(Iteration 1321 / 1470) loss: 0.651129\n",
      "(Iteration 1331 / 1470) loss: 0.643190\n",
      "(Iteration 1341 / 1470) loss: 0.579464\n",
      "(Iteration 1351 / 1470) loss: 0.479536\n",
      "(Iteration 1361 / 1470) loss: 0.594285\n",
      "(Iteration 1371 / 1470) loss: 0.556148\n",
      "(Iteration 1381 / 1470) loss: 0.525803\n",
      "(Iteration 1391 / 1470) loss: 0.511598\n",
      "(Iteration 1401 / 1470) loss: 0.489509\n",
      "(Iteration 1411 / 1470) loss: 0.734382\n",
      "(Iteration 1421 / 1470) loss: 0.482426\n",
      "(Iteration 1431 / 1470) loss: 0.643824\n",
      "(Iteration 1441 / 1470) loss: 0.588781\n",
      "(Iteration 1451 / 1470) loss: 0.721711\n",
      "(Iteration 1461 / 1470) loss: 0.479300\n",
      "(Epoch 3 / 3) train acc: 0.408000; val_acc: 0.399000\n",
      "Time: 2880.4732735157013\n",
      "(Iteration 1 / 1470) loss: 0.496508\n",
      "(Epoch 0 / 3) train acc: 0.459000; val_acc: 0.399000\n",
      "(Iteration 11 / 1470) loss: 0.410543\n",
      "(Iteration 21 / 1470) loss: 0.526402\n",
      "(Iteration 31 / 1470) loss: 0.497050\n",
      "(Iteration 41 / 1470) loss: 0.482721\n",
      "(Iteration 51 / 1470) loss: 0.594274\n",
      "(Iteration 61 / 1470) loss: 0.620263\n",
      "(Iteration 71 / 1470) loss: 0.415491\n",
      "(Iteration 81 / 1470) loss: 0.683572\n",
      "(Iteration 91 / 1470) loss: 0.460559\n",
      "(Iteration 101 / 1470) loss: 0.371766\n",
      "(Iteration 111 / 1470) loss: 0.507493\n",
      "(Iteration 121 / 1470) loss: 0.480649\n",
      "(Iteration 131 / 1470) loss: 0.553620\n",
      "(Iteration 141 / 1470) loss: 0.568535\n",
      "(Iteration 151 / 1470) loss: 0.706638\n",
      "(Iteration 161 / 1470) loss: 0.384820\n",
      "(Iteration 171 / 1470) loss: 0.414447\n",
      "(Iteration 181 / 1470) loss: 0.462151\n",
      "(Iteration 191 / 1470) loss: 0.439408\n",
      "(Iteration 201 / 1470) loss: 0.511771\n",
      "(Iteration 211 / 1470) loss: 0.482692\n",
      "(Iteration 221 / 1470) loss: 0.517257\n",
      "(Iteration 231 / 1470) loss: 0.334526\n",
      "(Iteration 241 / 1470) loss: 0.466123\n",
      "(Iteration 251 / 1470) loss: 0.492372\n",
      "(Iteration 261 / 1470) loss: 0.383600\n",
      "(Iteration 271 / 1470) loss: 0.503661\n",
      "(Iteration 281 / 1470) loss: 0.487575\n",
      "(Iteration 291 / 1470) loss: 0.497190\n",
      "(Iteration 301 / 1470) loss: 0.418216\n",
      "(Iteration 311 / 1470) loss: 0.460130\n",
      "(Iteration 321 / 1470) loss: 0.385608\n",
      "(Iteration 331 / 1470) loss: 0.438975\n",
      "(Iteration 341 / 1470) loss: 0.421353\n",
      "(Iteration 351 / 1470) loss: 0.350663\n",
      "(Iteration 361 / 1470) loss: 0.531766\n",
      "(Iteration 371 / 1470) loss: 0.390863\n",
      "(Iteration 381 / 1470) loss: 0.629697\n",
      "(Iteration 391 / 1470) loss: 0.399985\n",
      "(Iteration 401 / 1470) loss: 0.405699\n",
      "(Iteration 411 / 1470) loss: 0.384963\n",
      "(Iteration 421 / 1470) loss: 0.412243\n",
      "(Iteration 431 / 1470) loss: 0.484462\n",
      "(Iteration 441 / 1470) loss: 0.629234\n",
      "(Iteration 451 / 1470) loss: 0.409067\n",
      "(Iteration 461 / 1470) loss: 0.584120\n",
      "(Iteration 471 / 1470) loss: 0.516027\n",
      "(Iteration 481 / 1470) loss: 0.382469\n",
      "(Epoch 1 / 3) train acc: 0.400000; val_acc: 0.370000\n",
      "(Iteration 491 / 1470) loss: 0.504940\n",
      "(Iteration 501 / 1470) loss: 0.334103\n",
      "(Iteration 511 / 1470) loss: 0.312541\n",
      "(Iteration 521 / 1470) loss: 0.507497\n",
      "(Iteration 531 / 1470) loss: 0.294609\n",
      "(Iteration 541 / 1470) loss: 0.427541\n",
      "(Iteration 551 / 1470) loss: 0.531092\n",
      "(Iteration 561 / 1470) loss: 0.535185\n",
      "(Iteration 571 / 1470) loss: 0.722337\n",
      "(Iteration 581 / 1470) loss: 0.587048\n",
      "(Iteration 591 / 1470) loss: 0.441622\n",
      "(Iteration 601 / 1470) loss: 0.341783\n",
      "(Iteration 611 / 1470) loss: 0.266712\n",
      "(Iteration 621 / 1470) loss: 0.518371\n",
      "(Iteration 631 / 1470) loss: 0.432055\n",
      "(Iteration 641 / 1470) loss: 0.343458\n",
      "(Iteration 651 / 1470) loss: 0.596530\n",
      "(Iteration 661 / 1470) loss: 0.361162\n",
      "(Iteration 671 / 1470) loss: 0.471725\n",
      "(Iteration 681 / 1470) loss: 0.402525\n",
      "(Iteration 691 / 1470) loss: 0.422885\n",
      "(Iteration 701 / 1470) loss: 0.313367\n",
      "(Iteration 711 / 1470) loss: 0.391210\n",
      "(Iteration 721 / 1470) loss: 0.349346\n",
      "(Iteration 731 / 1470) loss: 0.516325\n",
      "(Iteration 741 / 1470) loss: 0.554272\n",
      "(Iteration 751 / 1470) loss: 0.519960\n",
      "(Iteration 761 / 1470) loss: 0.397835\n",
      "(Iteration 771 / 1470) loss: 0.445969\n",
      "(Iteration 781 / 1470) loss: 0.368310\n",
      "(Iteration 791 / 1470) loss: 0.323166\n",
      "(Iteration 801 / 1470) loss: 0.449341\n",
      "(Iteration 811 / 1470) loss: 0.651743\n",
      "(Iteration 821 / 1470) loss: 0.527967\n",
      "(Iteration 831 / 1470) loss: 0.421955\n",
      "(Iteration 841 / 1470) loss: 0.415927\n",
      "(Iteration 851 / 1470) loss: 0.457196\n",
      "(Iteration 861 / 1470) loss: 0.284242\n",
      "(Iteration 871 / 1470) loss: 0.365799\n",
      "(Iteration 881 / 1470) loss: 0.422896\n",
      "(Iteration 891 / 1470) loss: 0.441458\n",
      "(Iteration 901 / 1470) loss: 0.338371\n",
      "(Iteration 911 / 1470) loss: 0.376905\n",
      "(Iteration 921 / 1470) loss: 0.480746\n",
      "(Iteration 931 / 1470) loss: 0.587728\n",
      "(Iteration 941 / 1470) loss: 0.289901\n",
      "(Iteration 951 / 1470) loss: 0.393030\n",
      "(Iteration 961 / 1470) loss: 0.327394\n",
      "(Iteration 971 / 1470) loss: 0.269160\n",
      "(Epoch 2 / 3) train acc: 0.400000; val_acc: 0.379000\n",
      "(Iteration 981 / 1470) loss: 0.695814\n",
      "(Iteration 991 / 1470) loss: 0.315221\n",
      "(Iteration 1001 / 1470) loss: 0.533841\n",
      "(Iteration 1011 / 1470) loss: 0.401634\n",
      "(Iteration 1021 / 1470) loss: 0.320364\n",
      "(Iteration 1031 / 1470) loss: 0.349291\n",
      "(Iteration 1041 / 1470) loss: 0.263965\n",
      "(Iteration 1051 / 1470) loss: 0.551205\n",
      "(Iteration 1061 / 1470) loss: 0.565183\n",
      "(Iteration 1071 / 1470) loss: 0.415847\n",
      "(Iteration 1081 / 1470) loss: 0.404710\n",
      "(Iteration 1091 / 1470) loss: 0.458785\n",
      "(Iteration 1101 / 1470) loss: 0.471961\n",
      "(Iteration 1111 / 1470) loss: 0.332593\n",
      "(Iteration 1121 / 1470) loss: 0.328389\n",
      "(Iteration 1131 / 1470) loss: 0.414901\n",
      "(Iteration 1141 / 1470) loss: 0.396633\n",
      "(Iteration 1151 / 1470) loss: 0.392514\n",
      "(Iteration 1161 / 1470) loss: 0.517450\n",
      "(Iteration 1171 / 1470) loss: 0.334892\n",
      "(Iteration 1181 / 1470) loss: 0.326326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1191 / 1470) loss: 0.307325\n",
      "(Iteration 1201 / 1470) loss: 0.312693\n",
      "(Iteration 1211 / 1470) loss: 0.378931\n",
      "(Iteration 1221 / 1470) loss: 0.380673\n",
      "(Iteration 1231 / 1470) loss: 0.266468\n",
      "(Iteration 1241 / 1470) loss: 0.546113\n",
      "(Iteration 1251 / 1470) loss: 0.347014\n",
      "(Iteration 1261 / 1470) loss: 0.331328\n",
      "(Iteration 1271 / 1470) loss: 0.342020\n",
      "(Iteration 1281 / 1470) loss: 0.412101\n",
      "(Iteration 1291 / 1470) loss: 0.340458\n",
      "(Iteration 1301 / 1470) loss: 0.356247\n",
      "(Iteration 1311 / 1470) loss: 0.447984\n",
      "(Iteration 1321 / 1470) loss: 0.528638\n",
      "(Iteration 1331 / 1470) loss: 0.570444\n",
      "(Iteration 1341 / 1470) loss: 0.289786\n",
      "(Iteration 1351 / 1470) loss: 0.411087\n",
      "(Iteration 1361 / 1470) loss: 0.280303\n",
      "(Iteration 1371 / 1470) loss: 0.520131\n",
      "(Iteration 1381 / 1470) loss: 0.344460\n",
      "(Iteration 1391 / 1470) loss: 0.246551\n",
      "(Iteration 1401 / 1470) loss: 0.287486\n",
      "(Iteration 1411 / 1470) loss: 0.261848\n",
      "(Iteration 1421 / 1470) loss: 0.323540\n",
      "(Iteration 1431 / 1470) loss: 0.334613\n",
      "(Iteration 1441 / 1470) loss: 0.363259\n",
      "(Iteration 1451 / 1470) loss: 0.374313\n",
      "(Iteration 1461 / 1470) loss: 0.376960\n",
      "(Epoch 3 / 3) train acc: 0.434000; val_acc: 0.411000\n",
      "Time: 2836.6428496837616\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-a285a655019f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mval_acc_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mval_acc_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model3' is not defined"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best ConvNet that you can on CIFAR-10 with 3 epochs using    #\n",
    "# the sgd_momentum optimizer. Store your best model in the best_model variable.#\n",
    "################################################################################\n",
    "val_acc_max = 0\n",
    "\n",
    "for i in range(3):\n",
    "    if i==0:\n",
    "        model1 = ConvNet(\n",
    "            num_filters=[16, 32, 64, 128],\n",
    "            filter_sizes=[7, 5, 3, 3],\n",
    "            weight_scale=1e-3\n",
    "        )\n",
    "\n",
    "        solver = Solver(\n",
    "            model, data,\n",
    "            num_epochs=3, batch_size=100,\n",
    "            update_rule='sgd_momentum',\n",
    "            optim_config={\n",
    "              'learning_rate': 5e-2,\n",
    "            },\n",
    "            verbose=True, print_every=10\n",
    "        )\n",
    "        tic = time.time()\n",
    "        solver.train()\n",
    "        print(f\"Time: {time.time()-tic}\")\n",
    "        \n",
    "        val_acc = solver.check_accuracy(data['X_val'], data['y_val'])\n",
    "        if(val_acc>val_acc_max):\n",
    "            val_acc_max = val_acc\n",
    "            best_model = model1\n",
    "        \n",
    "    \n",
    "    if i==1:\n",
    "        model2 = ConvNet(\n",
    "            num_filters=[16, 32, 64, 128],\n",
    "            filter_sizes=[7, 5, 3, 3],\n",
    "            weight_scale=1e-3\n",
    "        )\n",
    "\n",
    "        solver = Solver(\n",
    "            model, data,\n",
    "            num_epochs=3, batch_size=100,\n",
    "            update_rule='sgd_momentum',\n",
    "            optim_config={\n",
    "              'learning_rate': 1e-1,\n",
    "            },\n",
    "            verbose=True, print_every=10\n",
    "        )\n",
    "        tic = time.time()\n",
    "        solver.train()\n",
    "        print(f\"Time: {time.time()-tic}\")\n",
    "        \n",
    "        val_acc = solver.check_accuracy(data['X_val'], data['y_val'])\n",
    "        if(val_acc>val_acc_max):\n",
    "            val_acc_max = val_acc\n",
    "            best_model = model2\n",
    "            \n",
    "      \n",
    "    if i==2:\n",
    "        model2 = ConvNet(\n",
    "            num_filters=[16, 32, 64, 128],\n",
    "            filter_sizes=[7, 5, 3, 3],\n",
    "            weight_scale=1e-1\n",
    "        )\n",
    "\n",
    "        solver = Solver(\n",
    "            model, data,\n",
    "            num_epochs=3, batch_size=100,\n",
    "            update_rule='sgd_momentum',\n",
    "            optim_config={\n",
    "              'learning_rate': 5e-2,\n",
    "            },\n",
    "            verbose=True, print_every=10\n",
    "        )\n",
    "        tic = time.time()\n",
    "        solver.train()\n",
    "        print(f\"Time: {time.time()-tic}\")\n",
    "        \n",
    "        val_acc = solver.check_accuracy(data['X_val'], data['y_val'])\n",
    "        if(val_acc>val_acc_max):\n",
    "            val_acc_max = val_acc\n",
    "            best_model = model3\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 735) loss: 0.332500\n",
      "(Epoch 0 / 3) train acc: 0.428000; val_acc: 0.409000\n",
      "(Iteration 11 / 735) loss: 0.330145\n",
      "(Iteration 21 / 735) loss: 0.358581\n",
      "(Iteration 31 / 735) loss: 0.475297\n",
      "(Iteration 41 / 735) loss: 0.301850\n",
      "(Iteration 51 / 735) loss: 0.211814\n",
      "(Iteration 61 / 735) loss: 0.334558\n",
      "(Iteration 71 / 735) loss: 0.305460\n",
      "(Iteration 81 / 735) loss: 0.286595\n",
      "(Iteration 91 / 735) loss: 0.316393\n",
      "(Iteration 101 / 735) loss: 0.438578\n",
      "(Iteration 111 / 735) loss: 0.240868\n",
      "(Iteration 121 / 735) loss: 0.207843\n",
      "(Iteration 131 / 735) loss: 0.298693\n",
      "(Iteration 141 / 735) loss: 0.326128\n",
      "(Iteration 151 / 735) loss: 0.286610\n",
      "(Iteration 161 / 735) loss: 0.245476\n",
      "(Iteration 171 / 735) loss: 0.282247\n",
      "(Iteration 181 / 735) loss: 0.284739\n",
      "(Iteration 191 / 735) loss: 0.246852\n",
      "(Iteration 201 / 735) loss: 0.318892\n",
      "(Iteration 211 / 735) loss: 0.482096\n",
      "(Iteration 221 / 735) loss: 0.315398\n",
      "(Iteration 231 / 735) loss: 0.339898\n",
      "(Iteration 241 / 735) loss: 0.211712\n",
      "(Epoch 1 / 3) train acc: 0.439000; val_acc: 0.402000\n",
      "(Iteration 251 / 735) loss: 0.256935\n",
      "(Iteration 261 / 735) loss: 0.241813\n",
      "(Iteration 271 / 735) loss: 0.354863\n",
      "(Iteration 281 / 735) loss: 0.286848\n",
      "(Iteration 291 / 735) loss: 0.188822\n",
      "(Iteration 301 / 735) loss: 0.362687\n",
      "(Iteration 311 / 735) loss: 0.199030\n",
      "(Iteration 321 / 735) loss: 0.260184\n",
      "(Iteration 331 / 735) loss: 0.273662\n",
      "(Iteration 341 / 735) loss: 0.332897\n",
      "(Iteration 351 / 735) loss: 0.300720\n",
      "(Iteration 361 / 735) loss: 0.218636\n",
      "(Iteration 371 / 735) loss: 0.283919\n",
      "(Iteration 381 / 735) loss: 0.274462\n",
      "(Iteration 391 / 735) loss: 0.303958\n",
      "(Iteration 401 / 735) loss: 0.227961\n",
      "(Iteration 411 / 735) loss: 0.263283\n",
      "(Iteration 421 / 735) loss: 0.208796\n",
      "(Iteration 431 / 735) loss: 0.188107\n",
      "(Iteration 441 / 735) loss: 0.254275\n",
      "(Iteration 451 / 735) loss: 0.272979\n",
      "(Iteration 461 / 735) loss: 0.264544\n",
      "(Iteration 471 / 735) loss: 0.258682\n",
      "(Iteration 481 / 735) loss: 0.231083\n",
      "(Epoch 2 / 3) train acc: 0.420000; val_acc: 0.379000\n",
      "(Iteration 491 / 735) loss: 0.310291\n",
      "(Iteration 501 / 735) loss: 0.337960\n",
      "(Iteration 511 / 735) loss: 0.319694\n",
      "(Iteration 521 / 735) loss: 0.189275\n",
      "(Iteration 531 / 735) loss: 0.206123\n",
      "(Iteration 541 / 735) loss: 0.222776\n",
      "(Iteration 551 / 735) loss: 0.198031\n",
      "(Iteration 561 / 735) loss: 0.214461\n",
      "(Iteration 571 / 735) loss: 0.266031\n",
      "(Iteration 581 / 735) loss: 0.226326\n",
      "(Iteration 591 / 735) loss: 0.288960\n",
      "(Iteration 601 / 735) loss: 0.303720\n",
      "(Iteration 611 / 735) loss: 0.213183\n",
      "(Iteration 621 / 735) loss: 0.206414\n",
      "(Iteration 631 / 735) loss: 0.221968\n",
      "(Iteration 641 / 735) loss: 0.310586\n",
      "(Iteration 651 / 735) loss: 0.267069\n",
      "(Iteration 661 / 735) loss: 0.172623\n",
      "(Iteration 671 / 735) loss: 0.281427\n",
      "(Iteration 681 / 735) loss: 0.268676\n",
      "(Iteration 691 / 735) loss: 0.220211\n",
      "(Iteration 701 / 735) loss: 0.230599\n",
      "(Iteration 711 / 735) loss: 0.157008\n",
      "(Iteration 721 / 735) loss: 0.224090\n",
      "(Iteration 731 / 735) loss: 0.199936\n",
      "(Epoch 3 / 3) train acc: 0.365000; val_acc: 0.381000\n",
      "Time: 2655.854017496109\n",
      "(Iteration 1 / 735) loss: 0.300547\n",
      "(Epoch 0 / 3) train acc: 0.436000; val_acc: 0.412000\n",
      "(Iteration 11 / 735) loss: 0.348787\n",
      "(Iteration 21 / 735) loss: 0.321729\n",
      "(Iteration 31 / 735) loss: 0.344426\n",
      "(Iteration 41 / 735) loss: 0.414864\n",
      "(Iteration 51 / 735) loss: 0.311783\n",
      "(Iteration 61 / 735) loss: 0.306124\n",
      "(Iteration 71 / 735) loss: 0.386082\n",
      "(Iteration 81 / 735) loss: 0.338295\n",
      "(Iteration 91 / 735) loss: 0.280219\n",
      "(Iteration 101 / 735) loss: 0.383464\n",
      "(Iteration 111 / 735) loss: 0.252378\n",
      "(Iteration 121 / 735) loss: 0.322776\n",
      "(Iteration 131 / 735) loss: 0.338404\n",
      "(Iteration 141 / 735) loss: 0.353651\n",
      "(Iteration 151 / 735) loss: 0.317016\n",
      "(Iteration 161 / 735) loss: 0.310680\n",
      "(Iteration 171 / 735) loss: 0.348504\n",
      "(Iteration 181 / 735) loss: 0.410283\n",
      "(Iteration 191 / 735) loss: 0.304134\n",
      "(Iteration 201 / 735) loss: 0.330793\n",
      "(Iteration 211 / 735) loss: 0.369784\n",
      "(Iteration 221 / 735) loss: 0.332329\n",
      "(Iteration 231 / 735) loss: 0.298562\n",
      "(Iteration 241 / 735) loss: 0.310482\n",
      "(Epoch 1 / 3) train acc: 0.468000; val_acc: 0.404000\n",
      "(Iteration 251 / 735) loss: 0.261450\n",
      "(Iteration 261 / 735) loss: 0.423103\n",
      "(Iteration 271 / 735) loss: 0.389577\n",
      "(Iteration 281 / 735) loss: 0.293157\n",
      "(Iteration 291 / 735) loss: 0.316315\n",
      "(Iteration 301 / 735) loss: 0.329102\n",
      "(Iteration 311 / 735) loss: 0.386629\n",
      "(Iteration 321 / 735) loss: 0.290266\n",
      "(Iteration 331 / 735) loss: 0.241755\n",
      "(Iteration 341 / 735) loss: 0.340319\n",
      "(Iteration 351 / 735) loss: 0.230565\n",
      "(Iteration 361 / 735) loss: 0.327874\n",
      "(Iteration 371 / 735) loss: 0.308343\n",
      "(Iteration 381 / 735) loss: 0.269122\n",
      "(Iteration 391 / 735) loss: 0.283008\n",
      "(Iteration 401 / 735) loss: 0.314650\n",
      "(Iteration 411 / 735) loss: 0.316200\n",
      "(Iteration 421 / 735) loss: 0.313920\n",
      "(Iteration 431 / 735) loss: 0.273482\n",
      "(Iteration 441 / 735) loss: 0.214441\n",
      "(Iteration 451 / 735) loss: 0.248728\n",
      "(Iteration 461 / 735) loss: 0.256044\n",
      "(Iteration 471 / 735) loss: 0.299994\n",
      "(Iteration 481 / 735) loss: 0.300815\n",
      "(Epoch 2 / 3) train acc: 0.387000; val_acc: 0.345000\n",
      "(Iteration 491 / 735) loss: 0.209056\n",
      "(Iteration 501 / 735) loss: 0.272457\n",
      "(Iteration 511 / 735) loss: 0.269603\n",
      "(Iteration 521 / 735) loss: 0.279479\n",
      "(Iteration 531 / 735) loss: 0.331247\n",
      "(Iteration 541 / 735) loss: 0.289293\n",
      "(Iteration 551 / 735) loss: 0.292543\n",
      "(Iteration 561 / 735) loss: 0.261316\n",
      "(Iteration 571 / 735) loss: 0.277153\n",
      "(Iteration 581 / 735) loss: 0.326000\n",
      "(Iteration 591 / 735) loss: 0.289999\n",
      "(Iteration 601 / 735) loss: 0.348317\n",
      "(Iteration 611 / 735) loss: 0.298999\n",
      "(Iteration 621 / 735) loss: 0.225576\n",
      "(Iteration 631 / 735) loss: 0.270622\n",
      "(Iteration 641 / 735) loss: 0.388783\n",
      "(Iteration 651 / 735) loss: 0.300249\n",
      "(Iteration 661 / 735) loss: 0.239278\n",
      "(Iteration 671 / 735) loss: 0.339708\n",
      "(Iteration 681 / 735) loss: 0.208419\n",
      "(Iteration 691 / 735) loss: 0.238491\n",
      "(Iteration 701 / 735) loss: 0.296289\n",
      "(Iteration 711 / 735) loss: 0.341509\n",
      "(Iteration 721 / 735) loss: 0.259178\n",
      "(Iteration 731 / 735) loss: 0.299318\n",
      "(Epoch 3 / 3) train acc: 0.362000; val_acc: 0.372000\n",
      "Time: 2616.6510944366455\n",
      "(Iteration 1 / 735) loss: 0.368618\n",
      "(Epoch 0 / 3) train acc: 0.457000; val_acc: 0.406000\n",
      "(Iteration 11 / 735) loss: 0.327631\n",
      "(Iteration 21 / 735) loss: 0.317278\n",
      "(Iteration 31 / 735) loss: 0.264250\n",
      "(Iteration 41 / 735) loss: 0.363603\n",
      "(Iteration 51 / 735) loss: 0.279311\n",
      "(Iteration 61 / 735) loss: 0.351313\n",
      "(Iteration 71 / 735) loss: 0.380025\n",
      "(Iteration 81 / 735) loss: 0.364038\n",
      "(Iteration 91 / 735) loss: 0.353721\n",
      "(Iteration 101 / 735) loss: 0.328348\n",
      "(Iteration 111 / 735) loss: 0.284747\n",
      "(Iteration 121 / 735) loss: 0.294891\n",
      "(Iteration 131 / 735) loss: 0.267969\n",
      "(Iteration 141 / 735) loss: 0.334277\n",
      "(Iteration 151 / 735) loss: 0.308535\n",
      "(Iteration 161 / 735) loss: 0.291178\n",
      "(Iteration 171 / 735) loss: 0.263797\n",
      "(Iteration 181 / 735) loss: 0.252468\n",
      "(Iteration 191 / 735) loss: 0.283394\n",
      "(Iteration 201 / 735) loss: 0.227725\n",
      "(Iteration 211 / 735) loss: 0.290848\n",
      "(Iteration 221 / 735) loss: 0.353229\n",
      "(Iteration 231 / 735) loss: 0.344852\n",
      "(Iteration 241 / 735) loss: 0.311543\n",
      "(Epoch 1 / 3) train acc: 0.323000; val_acc: 0.343000\n",
      "(Iteration 251 / 735) loss: 0.361346\n",
      "(Iteration 261 / 735) loss: 0.343052\n",
      "(Iteration 271 / 735) loss: 0.453190\n",
      "(Iteration 281 / 735) loss: 0.269929\n",
      "(Iteration 291 / 735) loss: 0.326508\n",
      "(Iteration 301 / 735) loss: 0.298226\n",
      "(Iteration 311 / 735) loss: 0.235692\n",
      "(Iteration 321 / 735) loss: 0.284475\n",
      "(Iteration 331 / 735) loss: 0.263435\n",
      "(Iteration 341 / 735) loss: 0.266273\n",
      "(Iteration 351 / 735) loss: 0.235716\n",
      "(Iteration 361 / 735) loss: 0.288000\n",
      "(Iteration 371 / 735) loss: 0.238759\n",
      "(Iteration 381 / 735) loss: 0.276358\n",
      "(Iteration 391 / 735) loss: 0.268384\n",
      "(Iteration 401 / 735) loss: 0.315992\n",
      "(Iteration 411 / 735) loss: 0.248376\n",
      "(Iteration 421 / 735) loss: 0.273162\n",
      "(Iteration 431 / 735) loss: 0.213947\n",
      "(Iteration 441 / 735) loss: 0.330570\n",
      "(Iteration 451 / 735) loss: 0.241467\n",
      "(Iteration 461 / 735) loss: 0.247271\n",
      "(Iteration 471 / 735) loss: 0.212619\n",
      "(Iteration 481 / 735) loss: 0.232998\n",
      "(Epoch 2 / 3) train acc: 0.393000; val_acc: 0.368000\n",
      "(Iteration 491 / 735) loss: 0.278942\n",
      "(Iteration 501 / 735) loss: 0.237889\n",
      "(Iteration 511 / 735) loss: 0.252183\n",
      "(Iteration 521 / 735) loss: 0.266594\n",
      "(Iteration 531 / 735) loss: 0.268553\n",
      "(Iteration 541 / 735) loss: 0.305520\n",
      "(Iteration 551 / 735) loss: 0.259425\n",
      "(Iteration 561 / 735) loss: 0.280437\n",
      "(Iteration 571 / 735) loss: 0.245398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 581 / 735) loss: 0.204480\n",
      "(Iteration 591 / 735) loss: 0.198390\n",
      "(Iteration 601 / 735) loss: 0.230003\n",
      "(Iteration 611 / 735) loss: 0.188091\n",
      "(Iteration 621 / 735) loss: 0.251576\n",
      "(Iteration 631 / 735) loss: 0.190420\n",
      "(Iteration 641 / 735) loss: 0.179581\n",
      "(Iteration 651 / 735) loss: 0.253287\n",
      "(Iteration 661 / 735) loss: 0.290082\n",
      "(Iteration 671 / 735) loss: 0.270356\n",
      "(Iteration 681 / 735) loss: 0.218565\n",
      "(Iteration 691 / 735) loss: 0.311535\n",
      "(Iteration 701 / 735) loss: 0.256591\n",
      "(Iteration 711 / 735) loss: 0.214783\n",
      "(Iteration 721 / 735) loss: 0.220750\n",
      "(Iteration 731 / 735) loss: 0.248363\n",
      "(Epoch 3 / 3) train acc: 0.458000; val_acc: 0.395000\n",
      "Time: 2608.591860294342\n",
      "(Iteration 1 / 735) loss: 0.368910\n",
      "(Epoch 0 / 3) train acc: 0.449000; val_acc: 0.412000\n",
      "(Iteration 11 / 735) loss: 0.357118\n",
      "(Iteration 21 / 735) loss: 0.253027\n",
      "(Iteration 31 / 735) loss: 0.371299\n",
      "(Iteration 41 / 735) loss: 0.357019\n",
      "(Iteration 51 / 735) loss: 0.298068\n",
      "(Iteration 61 / 735) loss: 0.397375\n",
      "(Iteration 71 / 735) loss: 0.309912\n",
      "(Iteration 81 / 735) loss: 0.365870\n",
      "(Iteration 91 / 735) loss: 0.501758\n",
      "(Iteration 101 / 735) loss: 0.330019\n",
      "(Iteration 111 / 735) loss: 0.411386\n",
      "(Iteration 121 / 735) loss: 0.377482\n",
      "(Iteration 131 / 735) loss: 0.259335\n",
      "(Iteration 141 / 735) loss: 0.271966\n",
      "(Iteration 151 / 735) loss: 0.435935\n",
      "(Iteration 161 / 735) loss: 0.289724\n",
      "(Iteration 171 / 735) loss: 0.436403\n",
      "(Iteration 181 / 735) loss: 0.382271\n",
      "(Iteration 191 / 735) loss: 0.259941\n",
      "(Iteration 201 / 735) loss: 0.308113\n",
      "(Iteration 211 / 735) loss: 0.273427\n",
      "(Iteration 221 / 735) loss: 0.265159\n",
      "(Iteration 231 / 735) loss: 0.226066\n",
      "(Iteration 241 / 735) loss: 0.348820\n",
      "(Epoch 1 / 3) train acc: 0.451000; val_acc: 0.405000\n",
      "(Iteration 251 / 735) loss: 0.370925\n",
      "(Iteration 261 / 735) loss: 0.357615\n",
      "(Iteration 271 / 735) loss: 0.405086\n",
      "(Iteration 281 / 735) loss: 0.317748\n",
      "(Iteration 291 / 735) loss: 0.266719\n",
      "(Iteration 301 / 735) loss: 0.291862\n",
      "(Iteration 311 / 735) loss: 0.275640\n",
      "(Iteration 321 / 735) loss: 0.280649\n",
      "(Iteration 331 / 735) loss: 0.335161\n",
      "(Iteration 341 / 735) loss: 0.227438\n",
      "(Iteration 351 / 735) loss: 0.291378\n",
      "(Iteration 361 / 735) loss: 0.283882\n",
      "(Iteration 371 / 735) loss: 0.259323\n",
      "(Iteration 381 / 735) loss: 0.328455\n",
      "(Iteration 391 / 735) loss: 0.309735\n",
      "(Iteration 401 / 735) loss: 0.375668\n",
      "(Iteration 411 / 735) loss: 0.286387\n",
      "(Iteration 421 / 735) loss: 0.280378\n",
      "(Iteration 431 / 735) loss: 0.298781\n",
      "(Iteration 441 / 735) loss: 0.259564\n",
      "(Iteration 451 / 735) loss: 0.376578\n",
      "(Iteration 461 / 735) loss: 0.332972\n",
      "(Iteration 471 / 735) loss: 0.263481\n",
      "(Iteration 481 / 735) loss: 0.493762\n",
      "(Epoch 2 / 3) train acc: 0.415000; val_acc: 0.374000\n",
      "(Iteration 491 / 735) loss: 0.254744\n",
      "(Iteration 501 / 735) loss: 0.315716\n",
      "(Iteration 511 / 735) loss: 0.240715\n",
      "(Iteration 521 / 735) loss: 0.177478\n",
      "(Iteration 531 / 735) loss: 0.252437\n",
      "(Iteration 541 / 735) loss: 0.260169\n",
      "(Iteration 551 / 735) loss: 0.273328\n",
      "(Iteration 561 / 735) loss: 0.373819\n",
      "(Iteration 571 / 735) loss: 0.291357\n",
      "(Iteration 581 / 735) loss: 0.312391\n",
      "(Iteration 591 / 735) loss: 0.315347\n",
      "(Iteration 601 / 735) loss: 0.291068\n",
      "(Iteration 611 / 735) loss: 0.288198\n",
      "(Iteration 621 / 735) loss: 0.280588\n",
      "(Iteration 631 / 735) loss: 0.260993\n",
      "(Iteration 641 / 735) loss: 0.246647\n",
      "(Iteration 651 / 735) loss: 0.194006\n",
      "(Iteration 661 / 735) loss: 0.230950\n",
      "(Iteration 671 / 735) loss: 0.215231\n",
      "(Iteration 681 / 735) loss: 0.305910\n",
      "(Iteration 691 / 735) loss: 0.374156\n",
      "(Iteration 701 / 735) loss: 0.274141\n",
      "(Iteration 711 / 735) loss: 0.287939\n",
      "(Iteration 721 / 735) loss: 0.351682\n",
      "(Iteration 731 / 735) loss: 0.354565\n",
      "(Epoch 3 / 3) train acc: 0.438000; val_acc: 0.384000\n",
      "Time: 2618.6441843509674\n",
      "(Iteration 1 / 735) loss: 0.339711\n",
      "(Epoch 0 / 3) train acc: 0.459000; val_acc: 0.410000\n",
      "(Iteration 11 / 735) loss: 0.249641\n",
      "(Iteration 21 / 735) loss: 0.270790\n",
      "(Iteration 31 / 735) loss: 0.289638\n",
      "(Iteration 41 / 735) loss: 0.327423\n",
      "(Iteration 51 / 735) loss: 0.358295\n",
      "(Iteration 61 / 735) loss: 0.297269\n",
      "(Iteration 71 / 735) loss: 0.347420\n",
      "(Iteration 81 / 735) loss: 0.343460\n",
      "(Iteration 91 / 735) loss: 0.304787\n",
      "(Iteration 101 / 735) loss: 0.225835\n",
      "(Iteration 111 / 735) loss: 0.264979\n",
      "(Iteration 121 / 735) loss: 0.283711\n",
      "(Iteration 131 / 735) loss: 0.344911\n",
      "(Iteration 141 / 735) loss: 0.309067\n",
      "(Iteration 151 / 735) loss: 0.279002\n",
      "(Iteration 161 / 735) loss: 0.290201\n",
      "(Iteration 171 / 735) loss: 0.259775\n",
      "(Iteration 181 / 735) loss: 0.325623\n",
      "(Iteration 191 / 735) loss: 0.307307\n",
      "(Iteration 201 / 735) loss: 0.266782\n",
      "(Iteration 211 / 735) loss: 0.325344\n",
      "(Iteration 221 / 735) loss: 0.260279\n",
      "(Iteration 231 / 735) loss: 0.302248\n",
      "(Iteration 241 / 735) loss: 0.268053\n",
      "(Epoch 1 / 3) train acc: 0.413000; val_acc: 0.388000\n",
      "(Iteration 251 / 735) loss: 0.275313\n",
      "(Iteration 261 / 735) loss: 0.291135\n",
      "(Iteration 271 / 735) loss: 0.347198\n",
      "(Iteration 281 / 735) loss: 0.200404\n",
      "(Iteration 291 / 735) loss: 0.241524\n",
      "(Iteration 301 / 735) loss: 0.193176\n",
      "(Iteration 311 / 735) loss: 0.299002\n",
      "(Iteration 321 / 735) loss: 0.237246\n",
      "(Iteration 331 / 735) loss: 0.302028\n",
      "(Iteration 341 / 735) loss: 0.287504\n",
      "(Iteration 351 / 735) loss: 0.291256\n",
      "(Iteration 361 / 735) loss: 0.246518\n",
      "(Iteration 371 / 735) loss: 0.223443\n",
      "(Iteration 381 / 735) loss: 0.272923\n",
      "(Iteration 391 / 735) loss: 0.238299\n",
      "(Iteration 401 / 735) loss: 0.220687\n",
      "(Iteration 411 / 735) loss: 0.287718\n",
      "(Iteration 421 / 735) loss: 0.258449\n",
      "(Iteration 431 / 735) loss: 0.222666\n",
      "(Iteration 441 / 735) loss: 0.217921\n",
      "(Iteration 451 / 735) loss: 0.206900\n",
      "(Iteration 461 / 735) loss: 0.234079\n",
      "(Iteration 471 / 735) loss: 0.220321\n",
      "(Iteration 481 / 735) loss: 0.190958\n",
      "(Epoch 2 / 3) train acc: 0.427000; val_acc: 0.389000\n",
      "(Iteration 491 / 735) loss: 0.215275\n",
      "(Iteration 501 / 735) loss: 0.236338\n",
      "(Iteration 511 / 735) loss: 0.244416\n",
      "(Iteration 521 / 735) loss: 0.289725\n",
      "(Iteration 531 / 735) loss: 0.203336\n",
      "(Iteration 541 / 735) loss: 0.280579\n",
      "(Iteration 551 / 735) loss: 0.276901\n",
      "(Iteration 561 / 735) loss: 0.202766\n",
      "(Iteration 571 / 735) loss: 0.238122\n",
      "(Iteration 581 / 735) loss: 0.211288\n",
      "(Iteration 591 / 735) loss: 0.188821\n",
      "(Iteration 601 / 735) loss: 0.236655\n",
      "(Iteration 611 / 735) loss: 0.158911\n",
      "(Iteration 621 / 735) loss: 0.277348\n",
      "(Iteration 631 / 735) loss: 0.183805\n",
      "(Iteration 641 / 735) loss: 0.227064\n",
      "(Iteration 651 / 735) loss: 0.247886\n",
      "(Iteration 661 / 735) loss: 0.223296\n",
      "(Iteration 671 / 735) loss: 0.165593\n",
      "(Iteration 681 / 735) loss: 0.369292\n",
      "(Iteration 691 / 735) loss: 0.181366\n",
      "(Iteration 701 / 735) loss: 0.205135\n",
      "(Iteration 711 / 735) loss: 0.188207\n",
      "(Iteration 721 / 735) loss: 0.218951\n",
      "(Iteration 731 / 735) loss: 0.190950\n",
      "(Epoch 3 / 3) train acc: 0.429000; val_acc: 0.387000\n",
      "Time: 2630.6528735160828\n"
     ]
    }
   ],
   "source": [
    "best_model2 = None\n",
    "################################################################################\n",
    "# TODO: Train the best ConvNet that you can on CIFAR-10 with 3 epochs using    #\n",
    "# the sgd_momentum optimizer. Store your best model in the best_model variable.#\n",
    "################################################################################\n",
    "val_acc_max = 0\n",
    "\n",
    "for i in range(6):\n",
    "    if i==0:\n",
    "        model1 = ConvNet(\n",
    "            num_filters=[16, 32, 64, 128],\n",
    "            filter_sizes=[7, 5, 3, 3],\n",
    "            weight_scale=1e-1\n",
    "        )\n",
    "\n",
    "        solver = Solver(\n",
    "            model, data,\n",
    "            num_epochs=3, batch_size=200,\n",
    "            update_rule='sgd_momentum',\n",
    "            optim_config={\n",
    "              'learning_rate': 7e-2,\n",
    "            },\n",
    "            verbose=True, print_every=10\n",
    "        )\n",
    "        tic = time.time()\n",
    "        solver.train()\n",
    "        print(f\"Time: {time.time()-tic}\")\n",
    "        \n",
    "        val_acc = solver.check_accuracy(data['X_val'], data['y_val'])\n",
    "        if(val_acc>val_acc_max):\n",
    "            val_acc_max = val_acc\n",
    "            best_model2 = model1\n",
    "        \n",
    "    \n",
    "    if i==1:\n",
    "        model2 = ConvNet(\n",
    "            num_filters=[16, 32, 64, 128],\n",
    "            filter_sizes=[7, 5, 3, 3],\n",
    "            weight_scale=1e-1\n",
    "        )\n",
    "\n",
    "        solver = Solver(\n",
    "            model, data,\n",
    "            num_epochs=3, batch_size=200,\n",
    "            update_rule='sgd_momentum',\n",
    "            optim_config={\n",
    "              'learning_rate': 1e-1,\n",
    "            },\n",
    "            verbose=True, print_every=10\n",
    "        )\n",
    "        tic = time.time()\n",
    "        solver.train()\n",
    "        print(f\"Time: {time.time()-tic}\")\n",
    "        \n",
    "        val_acc = solver.check_accuracy(data['X_val'], data['y_val'])\n",
    "        if(val_acc>val_acc_max):\n",
    "            val_acc_max = val_acc\n",
    "            best_model2 = model2\n",
    "            \n",
    "      \n",
    "    if i==2:\n",
    "        model3 = ConvNet(\n",
    "            num_filters=[16, 32, 64, 128],\n",
    "            filter_sizes=[7, 5, 3, 3],\n",
    "            weight_scale=0.5e-1\n",
    "        )\n",
    "\n",
    "        solver = Solver(\n",
    "            model, data,\n",
    "            num_epochs=3, batch_size=200,\n",
    "            update_rule='sgd_momentum',\n",
    "            optim_config={\n",
    "              'learning_rate': 7e-2,\n",
    "            },\n",
    "            verbose=True, print_every=10\n",
    "        )\n",
    "        tic = time.time()\n",
    "        solver.train()\n",
    "        print(f\"Time: {time.time()-tic}\")\n",
    "        \n",
    "        val_acc = solver.check_accuracy(data['X_val'], data['y_val'])\n",
    "        if(val_acc>val_acc_max):\n",
    "            val_acc_max = val_acc\n",
    "            best_model2 = model3\n",
    "    \n",
    "    if i==3:\n",
    "        model4 = ConvNet(\n",
    "            num_filters=[16, 32, 64, 128],\n",
    "            filter_sizes=[7, 5, 3, 3],\n",
    "            weight_scale=0.5e-1\n",
    "        )\n",
    "\n",
    "        solver = Solver(\n",
    "            model, data,\n",
    "            num_epochs=3, batch_size=200,\n",
    "            update_rule='sgd_momentum',\n",
    "            optim_config={\n",
    "              'learning_rate': 1e-1,\n",
    "            },\n",
    "            verbose=True, print_every=10\n",
    "        )\n",
    "        tic = time.time()\n",
    "        solver.train()\n",
    "        print(f\"Time: {time.time()-tic}\")\n",
    "        \n",
    "        val_acc = solver.check_accuracy(data['X_val'], data['y_val'])\n",
    "        if(val_acc>val_acc_max):\n",
    "            val_acc_max = val_acc\n",
    "            best_model2 = model4\n",
    "    \n",
    "    if i==4:\n",
    "        model5 = ConvNet(\n",
    "            num_filters=[16, 32, 64, 128],\n",
    "            filter_sizes=[7, 5, 3, 3],\n",
    "            weight_scale=0.5e-1\n",
    "        )\n",
    "\n",
    "        solver = Solver(\n",
    "            model, data,\n",
    "            num_epochs=3, batch_size=200,\n",
    "            update_rule='sgd_momentum',\n",
    "            optim_config={\n",
    "              'learning_rate': 0.5e-1,\n",
    "            },\n",
    "            verbose=True, print_every=10\n",
    "        )\n",
    "        tic = time.time()\n",
    "        solver.train()\n",
    "        print(f\"Time: {time.time()-tic}\")\n",
    "        \n",
    "        val_acc = solver.check_accuracy(data['X_val'], data['y_val'])\n",
    "        if(val_acc>val_acc_max):\n",
    "            val_acc_max = val_acc\n",
    "            best_model2 = model5\n",
    "\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy:  0.117\n",
      "Test set accuracy:  0.109\n"
     ]
    }
   ],
   "source": [
    "# Run your best model on the validation and test sets. You should achieve above 62% accuracy on the validation set.\n",
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy: ', (y_test_pred == data['y_test']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can visualize the first-layer convolutional filters from the trained network by running the following:\n",
    "from utils.vis_utils import visualize_grid\n",
    "\n",
    "grid = visualize_grid(model.params['W1'].transpose(0, 2, 3, 1))\n",
    "plt.imshow(grid.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='red'>**Task 4.3: report validation accuray for other hyper parameters you have tried (2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate=7e-2\n",
    "num_filters=[16, 32, 64, 128]\n",
    "filter_sizes=[7, 5, 3, 3]\n",
    "weight_scale=1e-1\n",
    "val_accuracy=\n",
    "\n",
    "learning_rate = 1e-1\n",
    "layers = \n",
    "filters = \n",
    "val_accuracy = \n",
    "\n",
    "learning_rate = 7e-2\n",
    "layers = \n",
    "filters = \n",
    "val_accuracy = \n",
    "\n",
    "learning_rate = 1e-1\n",
    "layers = \n",
    "filters = \n",
    "val_accuracy = \n",
    "\n",
    "learning_rate = 0.5e-1\n",
    "layers = \n",
    "filters = \n",
    "val_accuracy = \n",
    "\n",
    "learning_rate = 5e-2\n",
    "layers = \n",
    "filters = \n",
    "val_accuracy = \n",
    "\n",
    "learning_rate = 1e-1\n",
    "layers = \n",
    "filters = \n",
    "val_accuracy = \n",
    "\n",
    "learning_rate = 5e-2\n",
    "layers = \n",
    "filters = \n",
    "val_accuracy = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='4' color='red'>**Task 4.4: train a ConvNet without using batch normalization layers (3 points).**\n",
    "    \n",
    "<font size='4'>Report the best validation accuracy you can get and discuss how it is different from the version with batch normalization layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Fill your answer here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5' color='red'>**Extra credits: Implement a simple ResNet (10 points).**\n",
    "    \n",
    "<font size='4'>We now have all the needed layers for a ResNet model [2]. Implement a simple ResNet and eport its best validation accuracy.\n",
    "    \n",
    "[2] He et al., Deep Residual Learning for Image Recognition. CVPR 2016. https://arxiv.org/abs/1512.03385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(object):\n",
    "    \"\"\"\n",
    "    A simple residual network with the following architecture:\n",
    "\n",
    "    [conv-bn-relu][conv - bn - relu] x M - adaptive_average_pooling - affine - softmax\n",
    "    \n",
    "    \"[conv - bn - relu] x M\" means the \"conv-bn-relu\" architecture is repeated for\n",
    "    M times, where M is implicitly defined by the convolution layers' parameters.\n",
    "    \n",
    "    For each convolution layer, we do downsampling of factor 2 by setting the stride\n",
    "    to be 2. So we can have a large receptive field size.\n",
    "\n",
    "    The network operates on minibatches of data that have shape (N, C, H, W)\n",
    "    consisting of N images, each with height H and width W and with C input\n",
    "    channels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_filters=[32], filter_sizes=[7],\n",
    "            num_classes=10, weight_scale=1e-3, reg=0.0, use_batch_norm=True, \n",
    "            dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Initialize a new network.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: Tuple (C, H, W) giving size of input data\n",
    "        - num_filters: Number of filters to use in the convolutional layer. It is a\n",
    "          list whose length defines the number of convolution layers\n",
    "        - filter_sizes: Width/height of filters to use in the convolutional layer. It\n",
    "          is a list with the same length with num_filters\n",
    "        - num_classes: Number of output classes\n",
    "        - weight_scale: Scalar giving standard deviation for random initialization\n",
    "          of weights.\n",
    "        - reg: Scalar giving L2 regularization strength\n",
    "        - use_batch_norm: A boolean variable indicating whether to use batch normalization\n",
    "        - dtype: numpy datatype to use for computation.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        self.reg = reg\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        assert len(num_filters) == len(filter_sizes)\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Initialize weights and biases for the simple convolutional         #\n",
    "        # network. Weights should be initialized from a Gaussian centered at 0.0   #\n",
    "        # with standard deviation equal to weight_scale; biases should be          #\n",
    "        # initialized to zero. All weights and biases should be stored in the      #\n",
    "        #  dictionary self.params.                                                 #\n",
    "        #                                                                          #\n",
    "        # IMPORTANT:                                                               #\n",
    "        # 1. For this assignment, you can assume that the padding                  #\n",
    "        # and stride of the first convolutional layer are chosen so that           #\n",
    "        # **the width and height of the input are preserved**. You need to         #\n",
    "        # carefully set the `pad` parameter for the convolution.                   #\n",
    "        #                                                                          #\n",
    "        # 2. For each convolution layer, we use stride of 2 to do downsampling.    #\n",
    "        ############################################################################\n",
    "        C,H,W = input_dim\n",
    "        self.M = len(num_filters)\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "\n",
    "        self.params['W1'] = np.random.normal(0, weight_scale, (self.num_filters[0], C, self.filter_sizes[0], \n",
    "                                                               self.filter_sizes[0]))\n",
    "        self.params['b1'] = np.zeros(self.num_filters[0])\n",
    "        \n",
    "        self.params['gamma1'] = np.ones(self.num_filters[0])\n",
    "        self.params['beta1'] = np.zeros(self.num_filters[0])\n",
    "        \n",
    "#         print(\"X - \",X.shape)\n",
    "#         print(\"W1 - \",self.params[f'W1'].shape)\n",
    "#         print(\"b1 - \",self.params[f'b1'].shape)\n",
    "        \n",
    "        for i in range(self.M-1):\n",
    "            self.params[f'W{i+2}'] = np.random.normal(0, weight_scale, (self.num_filters[i+1],\n",
    "                          self.num_filters[i],self.filter_sizes[i+1], self.filter_sizes[i+1]))\n",
    "            \n",
    "#             print(f\"W{i+2} - {self.params[f'W{i+2}'].shape}\")\n",
    "            \n",
    "            self.params[f'b{i+2}'] = np.zeros(self.num_filters[i+1])\n",
    "#             print(f\"b{i+2} - {self.params[f'b{i+2}'].shape}\")\n",
    "            \n",
    "            self.params[f'gamma{i+2}'] = np.ones(self.num_filters[i+1])\n",
    "            self.params[f'beta{i+2}'] = np.zeros(self.num_filters[i+1])\n",
    "            \n",
    "        self.params['W_affine1'] = np.random.normal(0, weight_scale, (self.num_filters[-1], num_classes))\n",
    "        self.params['b_affine1'] = np.zeros(num_classes)\n",
    "        \n",
    "        # raise NotImplementedError        \n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Evaluate loss and gradient for the three-layer convolutional network.\n",
    "\n",
    "        Input / output: Same API as TwoLayerNet in fc_net.py.\n",
    "        \"\"\"\n",
    "\n",
    "        scores = None\n",
    "        mode = 'test' if y is None else 'train'\n",
    "        ############################################################################\n",
    "        # TODO: Implement the forward pass for the simple convolutional net,       #\n",
    "        # computing the class scores for X and storing them in the scores          #\n",
    "        # variable.                                                                #\n",
    "        ############################################################################\n",
    "        cache = {}\n",
    "        outs = {}\n",
    "        \n",
    "#         tic = time.time()\n",
    "        outs['conv_l1_out'], cache['conv_l1_cache'] = conv_forward_naive(X,self.params['W1'],\n",
    "                                self.params['b1'], {'stride':2, 'pad':(self.filter_sizes[0]-1)//2})\n",
    "        \n",
    "#         print(f\"Time taken for conv1 layer: {time.time()-tic}\")\n",
    "#         tic = time.time()\n",
    "        \n",
    "        outs['bn_l1_out'], cache['bn_l1_cache'] = spatial_batchnorm_forward(outs['conv_l1_out'],\n",
    "                                    self.params['gamma1'], self.params['beta1'], {\"mode\":mode})\n",
    "        \n",
    "#         print(f\"Time taken for bn1 layer: {time.time()-tic}\")\n",
    "#         tic = time.time()\n",
    "        \n",
    "        outs['relu_l1_out'], cache['relu_l1_cache'] = relu_forward(outs['bn_l1_out'])\n",
    "        \n",
    "#         print(f\"Time taken for relu1 layer: {time.time()-tic}\")\n",
    "        \n",
    "        \n",
    "#         print(f'Conv1', outs[f'conv_l1_out'].shape)\n",
    "#         print(f'BN1', outs[f'bn_l1_out'].shape)\n",
    "#         print(f'Relu1',  outs[f'relu_l1_out'].shape)\n",
    "#         tic = time.time()\n",
    "        for i in range(self.M-1):\n",
    "            outs[f'conv_l{i+2}_out'], cache[f'conv_l{i+2}_cache'] = conv_forward_naive(\n",
    "                                         outs[f'relu_l{i+1}_out'], self.params[f'W{i+2}'], \n",
    "                self.params[f'b{i+2}'], {'stride':2, 'pad':(self.filter_sizes[i+1]-1)//2})\n",
    "            \n",
    "            outs[f'bn_l{i+2}_out'], cache[f'bn_l{i+2}_cache'] = spatial_batchnorm_forward(\n",
    "                                     outs[f'conv_l{i+2}_out'], self.params[f'gamma{i+2}'], \n",
    "                                              self.params[f'beta{i+2}'], {\"mode\":mode})\n",
    "            \n",
    "            outs[f'relu_l{i+2}_out'], cache[f'relu_l{i+2}_cache'] = relu_forward(outs[f'bn_l{i+2}_out'])\n",
    "            \n",
    "#             print(f'Conv{i+2} - ', outs[f'conv_l{i+2}_out'].shape)\n",
    "#             print(f'BN{i+2}- ', outs[f'bn_l{i+2}_out'].shape)\n",
    "#             print(f'Relu{i+2}- ', outs[f'relu_l{i+2}_out'].shape)\n",
    "        \n",
    "#         print(f\"Time taken for all other layers: {time.time()-tic}\")\n",
    "#         tic = time.time()\n",
    "        \n",
    "        outs[f'avg_pooling1_out'], cache[f'avg_pooling1_cache'] = adaptive_avg_pool_forward(\n",
    "                                                                outs[f'relu_l{self.M}_out'])\n",
    "        \n",
    "#         print(f\"Time taken for pooling layer: {time.time()-tic}\")\n",
    "#         tic = time.time()\n",
    "        \n",
    "        # def affine_forward(x, w, b): return out, cache\n",
    "        outs['scores'], cache['ln_cache'] = affine_forward(outs[f'avg_pooling1_out'],\n",
    "                                self.params[f'W_affine1'], self.params[f'b_affine1'])\n",
    "        \n",
    "        scores = outs['scores']\n",
    "        \n",
    "#         print(f\"Time taken for affine layer: {time.time()-tic}\")\n",
    "        # raise NotImplementedError\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0, {}\n",
    "        ############################################################################\n",
    "        # TODO: Implement the backward pass for the simple convolutional net,      #\n",
    "        # storing the loss and gradients in the loss and grads variables. Compute  #\n",
    "        # data loss using softmax, and make sure that grads[k] holds the gradients #\n",
    "        # for self.params[k]. Don't forget to add L2 regularization!               #\n",
    "        #                                                                          #\n",
    "        # NOTE: To ensure that your implementation matches ours and you pass the   #\n",
    "        # automated tests, make sure that your L2 regularization includes a factor #\n",
    "        # of 0.5 to simplify the expression for the gradient.                      #\n",
    "        ############################################################################\n",
    "#         tic = time.time()\n",
    "        loss, dscores = softmax_loss(scores, y)\n",
    "        \n",
    "        for i in range(self.M):\n",
    "            loss += 0.5*self.reg*np.sum(self.params[f'W{i+1}']**2)\n",
    "        \n",
    "        loss += 0.5*self.reg*np.sum(self.params[f'W_affine1']**2)\n",
    "        \n",
    "        dx, grads[f'W_affine1'], grads[f'b_affine1'] = affine_backward(dscores, cache['ln_cache'])\n",
    "        dx = adaptive_avg_pool_backward(dx, cache[f'avg_pooling1_cache']) \n",
    "        \n",
    "        for i in range(self.M, 0, -1):\n",
    "#             print(dx.shape)\n",
    "            dx = relu_backward(dx, cache[f'relu_l{i}_cache'])\n",
    "#             print(dx.shape)\n",
    "            \n",
    "            # def spatial_batchnorm_backward(dout, cache): return dx, dgamma, dbeta\n",
    "            dx, grads[f'gamma{i}'],grads[f'beta{i}'] = spatial_batchnorm_backward(dx, cache[f'bn_l{i}_cache'])\n",
    "#             print(dx.shape)\n",
    "#             print(cache[f'conv_l{i}_cache'][0].shape)\n",
    "#             print(cache[f'conv_l{i}_cache'][1].shape)\n",
    "#             print(cache[f'conv_l{i}_cache'][2].shape)\n",
    "            dx, grads[f'W{i}'], grads[f'b{i}'] = conv_backward_naive(dx, cache[f'conv_l{i}_cache'])\n",
    "#             print(dx.shape)\n",
    "            \n",
    "            \n",
    "        \n",
    "        grads[f'W_affine1'] += self.reg*self.params[f'W_affine1']\n",
    "        for i in range(self.M, 0, -1):\n",
    "            grads[f'W{i}'] += self.reg*self.params[f'W{i}']\n",
    "\n",
    "#         print(f\"Time taken for backward pass- {time.time()-tic}\")\n",
    "        # raise NotImplementedError\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "        return loss, grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
